{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6c5bf2a9-480b-448f-bc9f-84ff3ed37c21",
    "deepnote_cell_height": 729.421875,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# Chapter 3: Graphical Causal Models\n",
    "\n",
    "## Thinking About Causality with Graphs\n",
    "\n",
    "Directed Acyclic Graphs (DAGs) provide a systematic and visual framework for representing and analyzing causal relationships. In addition, DAGs assist in identifying confounding variables, selecting appropriate statistical identification strategies, and making causal inferences based on observed data. \n",
    "\n",
    "Here's how we can think about causality using DAGs:\n",
    "\n",
    "\n",
    "**Directionality**: Causality is about the direction of influence between variables. In graphs, arrows indicate the direction of causality. If there is an arrow from Variable $X$ to Variable $Y$, it suggests that $X$ causally influences $Y$.\n",
    "\n",
    "\n",
    "**Temporal Order**: Causal relationships are typically characterized by a temporal order, where the cause precedes the effect. In DAGs, causes are depicted as nodes with arrows pointing toward the effect nodes. \n",
    "\n",
    "\n",
    "**Conditional Independence**: Graphs provide a way to assess conditional independence, which is a key concept in causal inference. If two variables are conditionally independent given a set of observed variables, they are considered to be **d-separated** in the graph. This implies that there is no direct causal relationship between them once the conditioning variables are taken into account.\n",
    "\n",
    "\n",
    "**Confounding and Mediation**: DAGs help in understanding the concepts of confounding and mediation. \n",
    "* **Confounding** occurs when an unobserved variable influences both the cause and the effect, creating a spurious association. DAGs make it explicit by including confounding variables in the graph. \n",
    "* **Mediation** is the concept of an intermediate variable that lies on the causal path between the cause and the effect. DAGs represent mediation by showing arrows from the cause to the mediator and from the mediator to the effect.\n",
    "\n",
    "\n",
    "**Alternative Paths and Backdoor Paths**: Graphs help in identifying alternative paths that can convey indirect causal relationships. \n",
    "* An **alternative path** is a route from the cause to the effect without a direct causal path. Understanding alternative paths helps recognize potential confounders and the importance of controlling them. \n",
    "* **Backdoor paths** represent non-causal associations between variables that can introduce bias when estimating causal effects.\n",
    "\n",
    "\n",
    "**Counterfactuals and interventions**: Graphs facilitate thinking about counterfactuals and interventions. \n",
    "* **Counterfactuals** involve comparing the outcome under two different conditions: the observed condition and a hypothetical condition where a specific intervention or treatment is applied. \n",
    "* Graphs provide a framework to represent **interventions** by removing or modifying arrows in the graph, allowing for reasoning about the potential effects of interventions and counterfactual scenarios.\n",
    "\n",
    "\n",
    "**Guiding Statistical Analysis**: DAGs provide guidance on selecting appropriate statistical methods for estimating causal effects. Based on the graphical structure, DAGs help identify appropriate identification strategies, such as *Randomized Control Trials (RCN), Structural Equation Modeling (SEM), Instrumental Variable Analysis (IV), Propensity Score Matching (PSM), Regression Discontinuity Design (RDD), etc. \n",
    "\n",
    "* A good refrence on appropriate statistical methods for estimating causal effects is the [Causal Inference: What If](https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/) book by Miguel Hernan.\n",
    "\n",
    "**Addressing Endogeneity and Selection Bias**: DAGs help address endogeneity and selection bias issues in observational studies. By explicitly representing the causal relationships, DAGs can guide the identification of instrumental variables, treatment assignment mechanisms, or appropriate matching strategies to account for bias.\n",
    "\n",
    "\n",
    "It is important to note that DAGs are simplifications of complex causal systems and should be used in conjunction with rigorous statistical analysis and domain knowledge to draw reliable causal conclusions. DAGs should be seen as a part of a broader toolkit for causal inference rather than a standalone solution.\n",
    "\n",
    "\n",
    "**<font color='blue'> What are the possible disadvantages of DAGs for Causal inference?</font>**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-5eed9118-789c-4e7e-a339-28ea8453019c",
    "deepnote_cell_height": 234.34375,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "\n",
    "\n",
    "## Basic Terminologies for Graphs\n",
    "\n",
    "\n",
    "For our purpose, it is (very) important that we understand what kind of independence and conditional independence assumptions a graphical model entails. To understand this, let's explain some common graphical definition and structures. They will be quite simple, but they are the sufficient building blocks to understand everything about graphical models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "1d93add9-278d-4503-a633-2c0b39c78686",
    "deepnote_cell_height": 74.78125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "**Graph:** A graph $G = (V, E)$ is a set V of vertices (nodes) and a set E of edges, which can be graphically illustrated, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "cell_id": "700e0795-16b7-4991-9c8d-33feb26b930e",
    "deepnote_cell_height": 381,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     122
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 27,
    "execution_start": 1647607230350,
    "source_hash": "ec6e9c07",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m g \u001b[38;5;241m=\u001b[39m \u001b[43mgr\u001b[49m\u001b[38;5;241m.\u001b[39mDigraph(graph_attr\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrankdir\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLR\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA graph with nodes L , U , A  and Y​\u001b[39m\u001b[38;5;124m\"\u001b[39m}, \n\u001b[1;32m      2\u001b[0m                node_attr\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplaintext\u001b[39m\u001b[38;5;124m'\u001b[39m}, \n\u001b[1;32m      3\u001b[0m                edge_attr\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marrowhead\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvee\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marrowsize\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m      4\u001b[0m g\u001b[38;5;241m.\u001b[39medge(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mU\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mdir\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m g\u001b[38;5;241m.\u001b[39medge(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gr' is not defined"
     ]
    }
   ],
   "source": [
    "g = gr.Digraph(graph_attr={'rankdir':'LR', 'label': \"A graph with nodes L , U , A  and Y​\"}, \n",
    "               node_attr={'shape': 'plaintext'}, \n",
    "               edge_attr={'arrowhead':'vee', 'arrowsize':'1'})\n",
    "g.edge(\"L\", \"U\", dir = \"none\")\n",
    "g.edge(\"L\", \"A\")\n",
    "g.edge(\"U\", \"A\")\n",
    "g.edge(\"A\", \"Y\")\n",
    "g\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b8d1e253-e1dc-46bf-b48c-c126e0f2b6ca",
    "deepnote_cell_height": 382.6875,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "**Nodes:** typically represent random variables.\n",
    "\n",
    "**Edges (arrows):** can be undirected, directed or bi-directed and typically indicate a certain relationship between nodes or possible direct causal effects.\n",
    "\n",
    "**Path:** A trail of edges going from one node to another, not necessarily following the direction of arrows. a path cannot cross a node more than once.\n",
    "\n",
    "**Cyclic Graph:** A cyclic graph has at least one path that can be followed through directed edges back to the original node​\n",
    "\n",
    "**Acyclic Graph:** An acyclic graph is a graph that contains no such cycles.\n",
    "\n",
    "<br/><br/>\n",
    "**Example: Paths in a Graph**\n",
    "\n",
    "What are paths from $X$ to $C$ in the following graph?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "6043cb7b-c4fd-4bd0-b069-35b9bc3aff93",
    "deepnote_cell_height": 518,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     187
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 42,
    "execution_start": 1647607230380,
    "source_hash": "1abe9b0d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = gr.Digraph(graph_attr={'rankdir':'LR'}, \n",
    "               edge_attr={'arrowhead':'vee', 'arrowsize':'1'})\n",
    "g.node(\"X\", \"X\", color=\"blue\", fontcolor=\"blue\")\n",
    "g.node(\"C\", \"C\", color=\"blue\", fontcolor=\"blue\")\n",
    "g.edge(\"U1\", \"Y\")\n",
    "g.edge(\"U1\", \"X\")\n",
    "g.edge(\"U2\", \"X\")\n",
    "g.edge(\"U2\", \"Y\")\n",
    "g.edge(\"X\", \"T\")\n",
    "g.edge(\"T\", \"C\")\n",
    "g.edge(\"C\", \"Y\")\n",
    "g.edge(\"X\", \"C\")\n",
    "g.edge(\"X\", \"Y\")\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "54c7d2fe-eaf9-400c-bedb-8e7dcf68b30c",
    "deepnote_cell_height": 153.5625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Paths from $X$ to $C$ are:\n",
    "\n",
    "- $ X \\rightarrow C$\n",
    "- $ X \\rightarrow T \\rightarrow C$\n",
    "- $ X \\rightarrow U_2 \\rightarrow Y \\rightarrow C$\n",
    "- $ X \\rightarrow U_1 \\rightarrow Y \\rightarrow C$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c2281eb1-f83b-4845-8672-1a0b63f181f0",
    "deepnote_cell_height": 242.734375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### More Terminologies:\n",
    "\n",
    "**Directed Acyclic Graph (DAG):** A DAG is a graph that is both directed and acyclic.​\n",
    "\n",
    "**Children and Parents:** Nodes directly affected by and affecting other nodes respectively.​\n",
    "\n",
    "**Ancestors and Descendants:** Nodes directly or indirectly affected by and affecting other nodes\n",
    "\n",
    "**Exogenous and Endogenous Nodes:** Nodes without and with parents respectively​\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "**Example: Parentship in a Graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "5fb1a843-4c68-4d0b-82a2-ff6b019f20c0",
    "deepnote_cell_height": 496,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     183
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 31,
    "execution_start": 1647607230430,
    "source_hash": "4a339dca",
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = gr.Digraph(graph_attr={'rankdir':'LR'}, \n",
    "               edge_attr={'arrowhead':'vee', 'arrowsize':'1'})\n",
    "g.node(\"T\", \"T\", color=\"blue\", fontcolor=\"blue\")\n",
    "g.edge(\"U1\", \"Y\")\n",
    "g.edge(\"U1\", \"X\")\n",
    "g.edge(\"U2\", \"X\")\n",
    "g.edge(\"U2\", \"Y\")\n",
    "g.edge(\"X\", \"T\")\n",
    "g.edge(\"T\", \"C\")\n",
    "g.edge(\"C\", \"Y\")\n",
    "g.edge(\"X\", \"C\")\n",
    "g.edge(\"X\", \"Y\")\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "7d209a97-7341-41a3-b9c6-7120508fa8a4",
    "deepnote_cell_height": 220.34375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Here are parental relations in the graph:\n",
    "\n",
    "**Parents:** $pa(T) = \\{ X \\}$​\n",
    "\n",
    "**Children:** $ch(T) = \\{ C \\}$​\n",
    "\n",
    "**Ancestors:** $anc(T) = \\{ X, U_1, U_2 \\}$​\n",
    "\n",
    "**Descendants:** $desc(T) = \\{ C, Y \\}$​\n",
    "\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6d80e7a1-5653-4ab6-91bf-cd1087b94431",
    "deepnote_cell_height": 366.515625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## DAGs for Causal Inference \n",
    "\n",
    "DAGs as models are mathematical objects, part of a larger class of graphical models such as Bayesian networks or Markov networks.​ DAGs graphically represent *non-parametric structural equation models*.​\n",
    "\n",
    "- DAGs **advantages** for causal models:​\n",
    "\n",
    "    - All pictures, no algebra​\n",
    "\n",
    "    - Focus on causal links\n",
    "\n",
    "    - easy for deriving nonparametric analysis\n",
    "\n",
    "\n",
    "- DAGs **disadvantages** for causal models:​:\n",
    "\n",
    "    - Don’t display the parametric assumptions that are often necessary for estimation in practice.​\n",
    "\n",
    "    - Generality can obscure important distinctions between estimands.​"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "8df2870a-bc06-4b43-a1b2-ff68cf40ac5c",
    "deepnote_cell_height": 569.640625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Notes on Causal DAGs\n",
    "\n",
    "- Causal DAGs encode the qualitative causal assumptions of the data-generating model. It is the *model of how the world works*. \n",
    "\n",
    "<br/>\n",
    "\n",
    "- Causal DAGs are **non-parametric**, i.e. they make no assumption about\n",
    "  - The distribution of the variables (nodes) in the DAG\n",
    "  - The functional form of the direct causal effects (arcs)\n",
    "\n",
    "<br/>\n",
    "\n",
    "- When we build a causal model (= drawing a DAG), we must consider all factors/variables that play a role in data generation, regardless of whether they are observed or unobserved.\n",
    "\n",
    "  - Causal assumptions are encoded by the *direction and absence of arrows*. \n",
    "  - *Directed arrows* or arcs represent possible direct causal effects.\n",
    "  - *Absence of arrows* or *missing arcs* represent sharp nulls of no-effect.\n",
    "\n",
    " <br/>\n",
    "  \n",
    "- Causal DAGs are **acyclic** because:\n",
    "\n",
    "    - One cannot trace a sequence of arcs in the direction of the arrows and arrive where one started.\n",
    "    - We impose acyclicness since a variable can’t cause itself.\n",
    "    - The future cannot directly or indirectly cause the past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "8c0aa1f9-9918-4115-a986-a6eb2b2dfab2",
    "deepnote_cell_height": 568,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     327
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 29,
    "execution_start": 1647607230471,
    "source_hash": "6a9af239",
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = gr.Digraph( graph_attr={'label': \"The arrow from A to B means that A may affect B, but not the other way around. \\n \" +\n",
    "                                    \"The absence of an arrow from D to B means that D does not affect B \\n\" + \n",
    "                                    \"The presence of C means that A and B may or may not have common causes.\"}, \n",
    "            edge_attr={'arrowhead':'vee', 'arrowsize':'1'})\n",
    "g.edge(\"C\", \"A\")\n",
    "g.edge(\"C\", \"B\")\n",
    "g.edge(\"A\", \"B\")\n",
    "g.node(\"D\", \"D\")\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a7b21d81-a9db-45be-96c0-1dd413e7a132",
    "deepnote_cell_height": 220.953125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### DAG Major Structure 1: Collider (V-Structure)\n",
    "\n",
    "- A collider (also known as v-structure or head-to-head meeting) has two incoming arrows along a chosen path.\n",
    "\n",
    "- A collider is when two arrows collide on a single variable. We can say that in this case both variables share a common effect.\n",
    "\n",
    "  $A \\!\\perp\\!\\!\\!\\perp B$   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "6e492bf8-59d9-4411-a4af-7eb818a663ef",
    "deepnote_cell_height": 338,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     187
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 27,
    "execution_start": 1647607230505,
    "source_hash": "4654047c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = gr.Digraph(graph_attr={'label': \"A and B are independent in general\"}, edge_attr={'arrowhead':'vee', 'arrowsize':'1'})\n",
    "g.edge(\"A\", \"C\")\n",
    "g.edge(\"B\", \"C\")\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a8c0ed7c-3726-4c72-87ed-085e314a1219",
    "deepnote_cell_height": 175.953125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "- As a general rule, **conditioning (adjusting or controlling)** on a collider opens the causal path. Not conditioning on it leaves it closed.\n",
    "- This phenomenon is sometimes called **explaining away**, because one cause already explains the effect, making the other cause less likely.\n",
    "\n",
    "  $A \\not\\!\\perp\\!\\!\\!\\perp B | C$ \n",
    "  \n",
    "  \n",
    "- In other words, **conditioning on** or **observing** a collider can lead to spurious associations between the variables that connect it. This phenomenon is known as **collider bias** or **Berkson's paradox**. It occurs because conditioning on a collider variable can induce a correlation between the variables that influence it, even if they are not causally related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "7ba2e1f6-6b0b-4a64-84b0-706633364443",
    "deepnote_cell_height": 356.328125,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     187.328125
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 88,
    "execution_start": 1647607230533,
    "source_hash": "653ea178",
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = gr.Digraph(graph_attr={'label': \"A and B are not independent anymore if we condition on C (collider)\"}, edge_attr={'arrowhead':'vee', 'arrowsize':'1'})\n",
    "g.node(\"C\", color=\"red\")\n",
    "g.edge(\"A\", \"C\")\n",
    "g.edge(\"B\", \"C\")\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "675df4cf-be04-42e2-b34f-fda20bc35764",
    "deepnote_cell_height": 128.171875,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Collider Example: A 6 Nodes Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "0f6926ba-7b76-4045-8f0e-a4403ae659a1",
    "deepnote_cell_height": 467,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     154
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 59,
    "execution_start": 1647607230661,
    "source_hash": "296f7886",
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = gr.Digraph(graph_attr={'rankdir':'LR'}, \n",
    "               edge_attr={'arrowhead':'vee', 'arrowsize':'1'})\n",
    "g.node(\"X\", \"X\", color = \"blue\")\n",
    "g.edge(\"U1\", \"Y\")\n",
    "g.edge(\"U1\", \"X\")\n",
    "g.edge(\"U2\", \"X\")\n",
    "g.edge(\"U2\", \"Y\")\n",
    "g.edge(\"X\", \"T\")\n",
    "g.edge(\"T\", \"C\")\n",
    "g.edge(\"C\", \"Y\")\n",
    "g.edge(\"X\", \"C\")\n",
    "g.edge(\"X\", \"Y\")\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above graph, we have: \n",
    "\n",
    "- $X$ is a collider on the path $U_1 \\rightarrow X \\leftarrow U_2$\n",
    "\n",
    "- $X$ is not a collider on the path $U_1 \\rightarrow X \\rightarrow T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "555a0d36-6590-411c-b75a-7db6c1091fd2",
    "deepnote_cell_height": 295.90625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Collider Example: School Admission\n",
    "\n",
    "We can assume there are two ways to be admitted to school. You can either be good at math or be good in arts (this is just an example, dont wory there are other ways too). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "1357fe37-c41d-415a-98ee-1092969c9ceb",
    "deepnote_cell_height": 361.65625,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     156.65625
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 12,
    "execution_start": 1647607230724,
    "source_hash": "7cadcc3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = gr.Digraph(edge_attr={'arrowhead':'vee', 'arrowsize':'1'})\n",
    "g.node(\"A\", label=\"Talented in Arts\")\n",
    "g.node(\"B\", label=\"Talented in Math\")\n",
    "g.node(\"C\", label=\"Admitted to School\")\n",
    "g.edge(\"A\", \"C\")\n",
    "g.edge(\"B\", \"C\")\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't condition on the admission to school (i.e. you don't know if a student has been admitted to school), then being good in arts or maths are independent conditions. \n",
    "\n",
    "* In other words, knowing that a student is good in math doens't tell anything about how good he is in arts (and viceversa). \n",
    "\n",
    "However, if you condition on the admission to school (i.e. you know the outcome of the admission), then being good in arts or maths become dependent. \n",
    "\n",
    "* If you know that a student has been admitted to school and he is not talented in arts, then it is more likely that he is talented in math. Conversely, if he is bad at math but he has been admitted to school, then he has to be good in arts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collider Example: Fire and Smoke Machine\n",
    "\n",
    "Intuitively we say that two variables are independent if knowing one variable doesn't provide any information on the other variable, i.e., it doesn't change our belief. \n",
    "\n",
    "Let's consider the following numerical example of a collider (similar to the previous example) where we we have a smoke machine in a club:\n",
    "\n",
    "- $P(F=1) = 0.2$\n",
    "- $P(M=1) = 0.1$\n",
    "- $P(S=1|F=0; M=0) = 0.1$\n",
    "- $P(S=1|F=1; M=0) = 0.8$\n",
    "- $P(S=1|F=0; M=1) = 0.8$\n",
    "- $P(S=1|F=1; M=1) = 0.9$\n",
    "\n",
    "Following is the DAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = gr.Digraph(edge_attr={'arrowhead':'vee', 'arrowsize':'1'})\n",
    "g.node(\"F\", label=\"Fire\")\n",
    "g.node(\"M\", label=\"Fog machine\")\n",
    "g.node(\"S\", label=\"Smoke\")\n",
    "g.edge(\"F\", \"S\")\n",
    "g.edge(\"M\", \"S\")\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analytical Solution:**\n",
    "\n",
    "\n",
    "The probability of fire is:\n",
    "$$P(F=1) = 0.2$$\n",
    "\n",
    "\n",
    "If we observe smoke ($S=1$), then the probability of fire increased to:\n",
    "\n",
    "$$P(F=1|S=1) = \\frac{P(S=1|F=1)P(F=1)}{P(S=1)} $$ \n",
    "where \n",
    "\\begin{align*}\n",
    "P(S=1) = P(S=1|F=0;M=0)P(F=0)P(M=0) +  \\\\\n",
    "            P(S=1|F=1;M=0)P(F=1)P(M=0) + \\\\           \n",
    "                P(S=1|F=0;M=1)P(F=0)P(M=1) + \\\\\n",
    "                    P(S=1|F=1;M=1)P(F=1)P(M=1)  \n",
    "\\end{align*}\n",
    "\n",
    "and \n",
    "\n",
    "\\begin{align*}\n",
    "P(S=1|F=1) = P(S=1|F=1;M=0)P(M=0) +  \\\\\n",
    "            P(S=1|F=1;M=1)P(M=1)   \n",
    "\\end{align*}\n",
    "\n",
    "Put the numbers in:\n",
    "\n",
    "$$P(F=1|S=1) = 0.54$$\n",
    "\n",
    "However, if we observe that the fog machine is on, $M = 1$, then the probability of fire decreased to \n",
    "\n",
    "$$P(F=1|S=1;M=1) = 0.22$$\n",
    "\n",
    "Here, $F$ and $M$ are not conditionally independent given $S$. When the probability of one explanation increases, the alternative explanations become less probable (they are explained away).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python Solution:**\n",
    "\n",
    "To solve this problem or similar problems, we can alsio use the **bnlearn** library in Python, [here](https://pypi.org/project/bnlearn/). \n",
    "The orginal library was first developemd for R by Marco Scutari. The package comes with a lot of examples. [Here](https://www.bnlearn.com) is the link.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bnlearn as bn\n",
    "from pgmpy.factors.discrete import TabularCPD\n",
    "\n",
    "# Define the network structure\n",
    "edges = [('Fire', 'Smoke'),\n",
    "         ('Machine', 'Smoke')]\n",
    "\n",
    "# Make the DAG\n",
    "DAG = bn.make_DAG(edges)\n",
    "\n",
    "# Input Probability Funtions Data\n",
    "cpt_fire = TabularCPD(variable='Fire', variable_card=2, values=[[0.8], [0.2]])\n",
    "cpt_machine = TabularCPD(variable='Machine', variable_card=2, values=[[0.9], [0.1]])\n",
    "\n",
    "\n",
    "cpt_smoke = TabularCPD(variable='Smoke', variable_card=2,\n",
    "                           values=[[0.9, 0.2, 0.2, 0.1],\n",
    "                                   [0.1, 0.8, 0.8, 0.9]],\n",
    "                           evidence=['Fire', 'Machine'],\n",
    "                           evidence_card=[2, 2])\n",
    "\n",
    "\n",
    "DAG = bn.make_DAG(DAG, CPD=[cpt_fire, cpt_machine, cpt_smoke])\n",
    "\n",
    "bn.print_CPD(DAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate $P(F=1|S=1)$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = bn.inference.fit(DAG, variables=['Fire'], evidence={'Smoke':1} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate $P(F=1|S=1;M=1)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2 = bn.inference.fit(DAG, variables=['Fire'], evidence={'Smoke':1, 'Machine':1} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "fbcc72d4-3cab-4585-9ddd-38f1d053a5ad",
    "deepnote_cell_height": 246.34375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### DAG Major Structure 2: Fork (Confounder)\n",
    "\n",
    "- A fork (Confounder or common cause) is a node $C$ in a graph that has outgoing edges to two (or more) other variables $A$ and $B$.\n",
    "- Fork $C$ causes cofounding. Confounding means that $A$ and $B$ have a common cause (direct or indirect). \n",
    "- $A$ and $B$ are often referred to as \"children\" or \"descendants\" of the common cause.\n",
    "- Fork $C$ is a common cause of $A$ and $B$. \n",
    "\n",
    "    $A \\not\\!\\perp\\!\\!\\!\\perp B$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "82dfd427-182a-4e5f-8609-64d898c9897c",
    "deepnote_cell_height": 338,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     187
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 112,
    "execution_start": 1647607230735,
    "source_hash": "7de0a144",
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = gr.Digraph(graph_attr={'label': \"A and B are not independent in general\"}, edge_attr={'arrowhead':'vee', 'arrowsize':'1'})\n",
    "g.edge(\"C\", \"A\")\n",
    "g.edge(\"C\", \"B\")\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "72d378b5-9d71-4ac0-9415-434d447b35d9",
    "deepnote_cell_height": 153.5625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "- In fork structure, the dependence flows backward through the arrows and we have what is called a **backdoor path**. \n",
    "\n",
    "- We can close the backdoor path and shut down dependence by conditioning on the common cause.\n",
    "\n",
    "    $A \\!\\perp\\!\\!\\!\\perp B | C$ \n",
    "    \n",
    "    \n",
    "- Forks represent situations where two variables appear to be associated or correlated, but the association is not due to a direct causal relationship between them. Instead, the association is induced by the shared influence of the common cause.\n",
    "\n",
    "- Failure to address a fork confounder can lead to biased estimates of the causal effect between the treatment and outcome. Neglecting the confounder can result in mistakenly attributing the observed association to a direct causal effect, when it is actually due, at least in part, to the unmeasured confounder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "cell_id": "2524faa8-029b-4ae0-93c6-dfeb1a6e5a75",
    "deepnote_cell_height": 356,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     187
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 29,
    "execution_start": 1647607230819,
    "source_hash": "ecbbe66d",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m g \u001b[38;5;241m=\u001b[39m \u001b[43mgr\u001b[49m\u001b[38;5;241m.\u001b[39mDigraph(graph_attr\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA and B are independent if we condition on C (fork)\u001b[39m\u001b[38;5;124m\"\u001b[39m}, edge_attr\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marrowhead\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvee\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marrowsize\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m      2\u001b[0m g\u001b[38;5;241m.\u001b[39mnode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m g\u001b[38;5;241m.\u001b[39medge(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gr' is not defined"
     ]
    }
   ],
   "source": [
    "g = gr.Digraph(graph_attr={'label': \"A and B are independent if we condition on C (fork)\"}, edge_attr={'arrowhead':'vee', 'arrowsize':'1'})\n",
    "g.node(\"C\", color=\"red\")\n",
    "g.edge(\"C\", \"A\")\n",
    "g.edge(\"C\", \"B\")\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "31bb0eba-6231-4887-a2a5-8ce5c0e720c6",
    "deepnote_cell_height": 214.734375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Fork Example: Ice Cream and Hot Summers\n",
    "\n",
    "We can consider the high temperature to influence both ice-cream consumption and number of solarburns in a city. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "c028f19f-1383-498f-b35b-81983a5bbd35",
    "deepnote_cell_height": 361.65625,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     156.65625
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 7,
    "execution_start": 1647607230849,
    "source_hash": "ba398afe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = gr.Digraph(edge_attr={'arrowhead':'vee', 'arrowsize':'1'})\n",
    "g.node(\"A\", label=\"Ice-cream consumption\")\n",
    "g.node(\"B\", label=\"Number of solarburns\")\n",
    "g.node(\"C\", label=\"Hot temperature\")\n",
    "g.edge(\"C\", \"A\")\n",
    "g.edge(\"C\", \"B\")\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't know whether it is summer (hot) or winter (cold), ice-cream consumption and number of solarburns are dependent (i.e., if we plot one w.r.t. the other we see a correlation). \n",
    "\n",
    "On the other hand, if we condition on the temperature (i.e. we know the temperature and if it is summer or winter), ice-cream consumption and number of solarburns become independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = [('Hot', 'Ice-cream'),\n",
    "         ('Hot', 'Solarburn')]\n",
    "\n",
    "# Make the actual Bayesian DAG\n",
    "DAG = bn.make_DAG(edges)\n",
    "\n",
    "# Input Probability Funtions Data\n",
    "cpt_icecream= TabularCPD(variable='Ice-cream', variable_card=2, values=[[0.7, 0.4],[0.3, 0.6]], evidence = ['Hot'], evidence_card=[2])\n",
    "cpt_solarburn = TabularCPD(variable='Solarburn', variable_card=2, values=[[0.9, 0.4],[0.1, 0.6]], evidence = ['Hot'], evidence_card=[2])\n",
    "\n",
    "cpt_hot = TabularCPD(variable='Hot', variable_card=2, values=[[0.5], [0.5]])\n",
    "\n",
    "DAG = bn.make_DAG(DAG, CPD=[cpt_icecream, cpt_solarburn, cpt_hot])\n",
    "bn.print_CPD(DAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate $P(IceCream | Hot)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = bn.inference.fit(DAG, variables=['Ice-cream'], evidence={'Hot':1} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate $P(IceCream | Solarburn, Hot)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2 = bn.inference.fit(DAG, variables=['Ice-cream'], evidence={'Solarburn':1, 'Hot':1} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing the hot summer, the probality of Ice Cream and Sunburn are independent.\n",
    "\n",
    " $$P(IceCream |Hot) = P(IceCream | Solarburn, Hot)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9b2906aa-7876-4dda-ac45-22969d03dca5",
    "deepnote_cell_height": 223.953125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### DAG Major Structure 3: Mediator (Chain)\n",
    "\n",
    "- A node $C$ is a mediator if it lies on a directed path from $A$ to $B$.\n",
    "\n",
    "- It helps explain the pathways of causation and understand the specific variables or processes that operate between the cause and effect.\n",
    "\n",
    "- As a general rule, the dependence flow in the direct path from $A$ to $B$ is blocked when we condition on an intermediary variable $C$.\n",
    "\n",
    "  - $A \\not\\!\\perp\\!\\!\\!\\perp B$   \n",
    "\n",
    "  - $A \\!\\perp\\!\\!\\!\\perp B | C$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "cell_id": "85729589-2bf3-442c-9df9-281715064b9b",
    "deepnote_cell_height": 242,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     91
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 25,
    "execution_start": 1647607230861,
    "source_hash": "5c2b13cd",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m g \u001b[38;5;241m=\u001b[39m \u001b[43mgr\u001b[49m\u001b[38;5;241m.\u001b[39mDigraph(graph_attr\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrankdir\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLR\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA and B are not independent in general\u001b[39m\u001b[38;5;124m\"\u001b[39m}, edge_attr\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marrowhead\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvee\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marrowsize\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m      2\u001b[0m g\u001b[38;5;241m.\u001b[39medge(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m g\u001b[38;5;241m.\u001b[39medge(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gr' is not defined"
     ]
    }
   ],
   "source": [
    "g = gr.Digraph(graph_attr={'rankdir':'LR', 'label': \"A and B are not independent in general\"}, edge_attr={'arrowhead':'vee', 'arrowsize':'1'})\n",
    "g.edge(\"A\", \"C\")\n",
    "g.edge(\"C\", \"B\")\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "cell_id": "6787d866-8a8d-45f9-a95a-e942277d8ff4",
    "deepnote_cell_height": 260,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     91
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2,
    "execution_start": 1647607230936,
    "source_hash": "7ed516a",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m g \u001b[38;5;241m=\u001b[39m \u001b[43mgr\u001b[49m\u001b[38;5;241m.\u001b[39mDigraph(graph_attr\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrankdir\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLR\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA and B are independent if we condition on C (mediator)\u001b[39m\u001b[38;5;124m\"\u001b[39m}, edge_attr\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marrowhead\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvee\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marrowsize\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m      2\u001b[0m g\u001b[38;5;241m.\u001b[39mnode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m g\u001b[38;5;241m.\u001b[39medge(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gr' is not defined"
     ]
    }
   ],
   "source": [
    "g = gr.Digraph(graph_attr={'rankdir':'LR', 'label': \"A and B are independent if we condition on C (mediator)\"}, edge_attr={'arrowhead':'vee', 'arrowsize':'1'})\n",
    "g.node(\"C\", color=\"red\")\n",
    "g.edge(\"A\", \"C\")\n",
    "g.edge(\"C\", \"B\")\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "276e7301-6b39-4b91-a204-73f6f5fa354b",
    "deepnote_cell_height": 192.34375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Chain Example: Throwing Dices\n",
    "\n",
    "Let's assume we are throwing dices. If we don't know (i.e. we don't condition) the sum in the first $n$ throws, then knowing the sum we got in the first $n-1$ throws helps us in better estimating the sum in the $n+1$ throws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "10a98bed-eb72-473c-9926-58a11e04fe99",
    "deepnote_cell_height": 265.65625,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     60.65625
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 44,
    "execution_start": 1647607230937,
    "source_hash": "bd07b184",
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = gr.Digraph(graph_attr={'rankdir':'LR'}, edge_attr={'arrowhead':'vee', 'arrowsize':'1'})\n",
    "g.node(\"A\", label=\"Sum in n-1 throws\")\n",
    "g.node(\"B\", label=\"Sum in n throws\")\n",
    "g.node(\"C\", label=\"Sum in n+1 throws\")\n",
    "g.edge(\"A\", \"B\")\n",
    "g.edge(\"B\", \"C\")\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we condition (i.e. we observe) on the sum in the first $n$ throws, then knowing the sum in $n-1$ throws doesn't provide any extra information in better estimating the sum in $n+1$ thorws. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ef010c60-f187-4986-9e67-39064f583177",
    "deepnote_cell_height": 167.171875,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## DAG Major Structure 4: Causal Paths\n",
    "\n",
    "A causal path describes the flow of influence or the causal mechanism from an initial variable (\"cause\" or \"exposure\") to a final variable (\"effect\" or \"outcome\") through a series of intermediate variables.\n",
    "\n",
    "The causal path from $X$ to $C$ mediate the causal effect of $X$ on $C$, the non-causal path do not.\n",
    "\n",
    "For example in the graphs below, the causal path between $X$ and $C$ are highlighted in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "44b45d40-182e-4198-973b-7713b32a6a5b",
    "deepnote_cell_height": 518.328125,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     187.328125
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 46,
    "execution_start": 1647607230980,
    "source_hash": "4714b6b4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = gr.Digraph(graph_attr={'rankdir':'LR'}, \n",
    "               edge_attr={'arrowhead':'vee', 'arrowsize':'1'})\n",
    "g.node(\"X\", \"X\", color = \"blue\")\n",
    "g.node(\"C\", \"C\", color = \"blue\")\n",
    "g.edge(\"U1\", \"Y\")\n",
    "g.edge(\"U1\", \"X\")\n",
    "g.edge(\"U2\", \"X\")\n",
    "g.edge(\"U2\", \"Y\")\n",
    "g.edge(\"X\", \"T\", color=\"red\")\n",
    "g.edge(\"T\", \"C\", color=\"red\")\n",
    "g.edge(\"C\", \"Y\")\n",
    "g.edge(\"X\", \"C\", color=\"red\")\n",
    "g.edge(\"X\", \"Y\")\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "da415130-cf58-4472-ba1c-6178dafb4e2a",
    "deepnote_cell_height": 220.953125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Path Blocking Rules\n",
    "\n",
    "Path blocking in Directed Acyclic Graphs (DAGs) refers to the process of identifying variables or conditions that can close or block a specific causal path between two variables. It involves determining which variables need to be controlled for or conditioned on to prevent spurious associations or biases when estimating causal effects.\n",
    "\n",
    "An **active path** is *open* and allows for potential causal influence between the variables, while an **inactive path** is *blocked* or closed, preventing any direct causal influence.\n",
    "\n",
    "Path are either open or blocked, according to two rules:\n",
    "\n",
    "- **Rule 1:** A path is blocked if somewhere along the path, there is a variable $C$ that sits in a *chain*, or sits in a *fork* and we have conditioned for $C$. \n",
    "\n",
    "- **Rule 2:** A path is blocked if somewhere along the path, there is a variable $C$ that sits in a *collider* and we have not conditioned for $C$, or any of its descendents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path Blocking Example: a 4 Nodes Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "23f2fe50-f8ac-45aa-b139-4dfe56232230",
    "deepnote_cell_height": 325.65625,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     156.65625
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1647607231025,
    "source_hash": "5d0cf649",
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = gr.Digraph(edge_attr={'arrowhead':'vee', 'arrowsize':'1'})\n",
    "g.edge(\"V\", \"A\")\n",
    "g.edge(\"V\", \"W\")\n",
    "g.edge(\"Y\", \"W\")\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "1c2618ee-d6ae-4548-8f00-edb9129d6b10",
    "deepnote_cell_height": 178.953125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "From the causal graph above we notice that:\n",
    "- Conditioning for **V** blocks the path from **A** to **W** (rule 1) \n",
    "- Conditioning for **W** leaves the path open (rule 2) from **A** to **Y**. \n",
    "- Conditioning for both **V** and **W** blocks the path from **A** to **Y**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ee7eed8a-0f2d-4a90-854e-e41f889d7eff",
    "deepnote_cell_height": 919.96875,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Path Blocking and D-Separation \n",
    "\n",
    "D-separation, short for *directed separation*, is a criterion used to determine whether a specific set of variables blocks or renders inactive a path between two variables in a DAG. \n",
    "\n",
    "D-separation relies on a set of rules, often referred to as path blocking rules or **d-separation rules**, to determine when a path is blocked. These rules identify the necessary variables to condition on to close a specific path and prevent spurious associations or biases in estimating causal effects.\n",
    "\n",
    "\n",
    "**d-separation in Sets:** Sets of variables $A$ and $B$ are d-separated (or blocked) by $C$ if all paths between $A$ and $B$ are blocked by $C$. d-separation implies: \n",
    "\n",
    "$A \\perp \\!\\!\\! \\perp B | C$.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**d-separation in Paths:** D-separation determines which paths transmit association, and which ones don’t.\n",
    "Formally, a path **P** is said to be d-separated (or blocked) by a conditioning set of nodes $\\{Z\\}$ if:\n",
    "\n",
    "1. **P** contains a chain $A \\rightarrow M \\rightarrow B$ or a fork $A \\leftarrow M \\rightarrow B$ such that the middle node $M$ is in $\\{Z\\}$, or\n",
    "2. **P** contains a collider $A \\rightarrow M \\leftarrow B$ such that neither the middle node $M$, nor any descendant of $M$, is in $\\{Z\\}$.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**d-connected in Paths:** A path **P** is said to be d-connected (or unblocked or open) by a conditioning set of nodes $\\{Z\\}$ if it is not d-separated.\n",
    "\n",
    "\n",
    "In other words:\n",
    "\n",
    "- *Blocked* (d-separated) paths don’t transmit association (information). \n",
    "\n",
    "- *Unblocked* (d-connected) paths may transmit association (information).\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "The three aforementioned blocking criteria can be rephased as: \n",
    "\n",
    "- Conditioning on a non-collider blocks a path, \n",
    "\n",
    "- Conditioning on a collider, or a descendent of a collider, unblocks a path, \n",
    "\n",
    "- Not conditioning on a collider leaves a path “naturally” blocked.\n",
    "\n",
    "<br/>\n",
    "\n",
    "### Path Blocking and Independence \n",
    "\n",
    "- Two variables that are d-separated along all paths given $\\{Z\\}$ are <font color='red'>conditionally independent given $\\{Z\\}$.</font>\n",
    "\n",
    "- Two variables that are *NOT* d-separated along all paths given $\\{Z\\}$ are <font color='red'>potentially dependent given $\\{Z\\}$.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "3fac0589-db96-4209-94ab-d40c54166ecc",
    "deepnote_cell_height": 692.4375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### D-Separation Example, a 10 Nodes Graph\n",
    "\n",
    "We use the following DAG from [Bardy Neal course](https://www.bradyneal.com/causal-inference-course) to see blocked and un-blocked pathes between $T$ and $Y$ for different controlling (conditioning) cases.\n",
    "\n",
    "![img](img/ch3/graph_Dsep_example_case0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "bc3a6faa-d562-40c2-ab40-fe7d13c26e47",
    "deepnote_cell_height": 658.828125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<br/><br/>\n",
    "**Case 1:** In the following graph, is  $T \\perp \\!\\!\\! \\perp Y | M_1$  valid?\n",
    "\n",
    "![img](img/ch3/graph_Dsep_example_case1.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "29d387ba-5c77-441b-97e3-0689ad6f9c7b",
    "deepnote_cell_height": 153.5625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "**NO!**, \n",
    "- the chain path $T-M_1-M_2-Y$ is blocked ($M_1$ is in the conditioning set) \n",
    "- the fork path $T-W_1-W_2-W_3-Y$ is not blocked.\n",
    "- the collider path $T-X_1-X_2-X_3-Y$ is blocked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "863ba0fb-b574-4e95-af15-dd183d2b14bc",
    "deepnote_cell_height": 658.828125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<br/><br/>\n",
    "**Case 2:** In the following graph, is  $T \\perp \\!\\!\\! \\perp Y | M_1, W_2 $  valid?  \n",
    "\n",
    "![img](img/ch3/graph_Dsep_example_case2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "8e4cb192-65a1-4bd4-ad8b-19ccf11332e9",
    "deepnote_cell_height": 153.5625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "**YES!**, \n",
    "- the chain path $T-M_1-M_2-Y$ is blocked ($M_1$ is in the conditioning set) \n",
    "- the fork path $T-W_1-W_2-W_3-Y$ is blocked ($W_2$ is in the conditioning set).\n",
    "- the collider path $T-X_1-X_2-X_3-Y$ is blocked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00046-4464fcaa-1ecc-4156-8f88-d03800a3e4e1",
    "deepnote_cell_height": 658.828125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<br/><br/>\n",
    "**Case 3:** In the following graph, is  $T \\perp \\!\\!\\! \\perp Y | M_1, W_3 $  valid?  \n",
    "\n",
    "![img](img/ch3/graph_Dsep_example_case3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "fe2c3d7b-98da-4d90-9d22-f628615f267f",
    "deepnote_cell_height": 153.5625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "**YES!**,\n",
    "- the chain path $T-M_1-M_2-Y$ is blocked ($M_1$ is in the conditioning set). \n",
    "- the fork path $T-W_1-W_2-W_3-Y$ is blocked ($W_3$ is in the conditioning set).\n",
    "- the collider path $T-X_1-X_2-X_3-Y$ is blocked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00048-33acc8f8-091f-4f6e-b0d5-ca95a6517273",
    "deepnote_cell_height": 658.828125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<br/><br/>\n",
    "**Case 4:** In the following graph, is  $T \\perp \\!\\!\\! \\perp Y | M_1, W_1, W_2, X_2$  valid?  \n",
    "\n",
    "![img](img/ch3/graph_Dsep_example_case4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6a65db51-6e07-478d-a661-1e222610cfb8",
    "deepnote_cell_height": 153.5625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "**NO!**,\n",
    "- the chain path $T-M_1-M_2-Y$ is blocked ($M_1$ is in the conditioning set). \n",
    "- the fork path $T-W_1-W_2-W_3-Y$ is blocked ($W_2, W_3$ are in the conditioning set).\n",
    "- the collider path $T-X_1-X_2-X_3-Y$ is NOT blocked ($X_2$ is in the conditioning set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4030a3d2-522a-4a33-ac83-e11de02542c7",
    "deepnote_cell_height": 658.828125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<br/><br/>\n",
    "**Case 5:** In the following graph, is  $T \\perp \\!\\!\\! \\perp Y | M_1, W_2, W_3, X_1, X_2?$  valid?\n",
    "\n",
    "![img](img/ch3/graph_Dsep_example_case5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00054-160f1856-1d83-45c9-8fd1-90fe514afd87",
    "deepnote_cell_height": 153.5625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "**YES!**,\n",
    "- the chain path $T-M_1-M_2-Y$ is blocked ($M_1$ is in the conditioning set). \n",
    "- the fork path $T-W_1-W_2-W_3-Y$ is blocked ($W_2, W_3$ are in the conditioning set).\n",
    "- the collider path $T-X_1-X_2-X_3-Y$ is blocked ($X_1$ is in the conditioning set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic d-separation test with Python\n",
    "\n",
    "Testing the d-separation manually is not feasible for large graphs.\n",
    "We can use the [networkx](https://networkx.org) python library instead to test for conditional independence.\n",
    "\n",
    "NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from(\n",
    "    [\n",
    "        (\"T\", \"M1\"),\n",
    "        (\"M1\", \"M2\"),\n",
    "        (\"M2\", \"Y\"),\n",
    "        #\n",
    "        (\"T\", \"X1\"),\n",
    "        (\"X1\", \"X2\"),\n",
    "        (\"Y\", \"X3\"),\n",
    "        (\"X3\", \"X2\"),\n",
    "        #\n",
    "        (\"W1\", \"T\"),\n",
    "        (\"W2\", \"W1\"),\n",
    "        (\"W2\", \"W3\"),\n",
    "        (\"W3\", \"Y\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Use an alternative layout such as spring_layout\n",
    "pos = nx.spring_layout(G)\n",
    "\n",
    "# Drawing options\n",
    "options = {\n",
    "    'node_color': 'gray',\n",
    "    'node_size': 500,\n",
    "    'width': 1,\n",
    "    'arrowstyle': '-|>',\n",
    "    'arrowsize': 12,\n",
    "}\n",
    "\n",
    "# Draw the graph\n",
    "nx.draw(G, pos, with_labels=True, **options)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is  $T \\perp \\!\\!\\! \\perp Y | M_1$  valid?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is T and Y d-separated given M1? \n",
    "nx.d_separated(G, {\"T\"}, {\"Y\"}, {\"M1\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is  $T \\perp \\!\\!\\! \\perp Y | M_1, W_2 $  valid? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is T and Y d-separated given M1 and W2? \n",
    "nx.d_separated(G, {\"T\"}, {\"Y\"}, {\"M1\", \"W2\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Assumptions\n",
    "\n",
    "Causal DAGs require additional assumptions to make meaningful inferences about causal structure. These assumptions narrow down the possible solutions and provide a framework for drawing causal conclusions based on observed data. The goal is not to magically discover causal relationships but to understand what causality can be learned given the causal assumptions.\n",
    "\n",
    "There are four common assumptions made across causal discovery algorithms. \n",
    "\n",
    " 1) **Acyclicity** — causal structure can be represented by a DAG $\\mathcal{G}$. We already seen that.\n",
    " \n",
    "* The Acyclicity assumption, also known as the Directed Acyclic Graph (DAG) assumption or the no-feedback assumption\n",
    "* It states that the causal relationships among variables can be represented by a directed acyclic graph, where there are no cycles or feedback loops.\n",
    "* It ensures that the causal relationships are well-defined and can be represented in a graph structure.\n",
    "\n",
    "\n",
    " 2) **Markov Property** — all nodes are independent of their non-descendants when conditioned on their parents. \n",
    " \n",
    "* The Markov Property assumption, also known as the Markov condition or the local independence assumption.\n",
    "* It states that a variable is conditionally independent of its non-descendants given its direct causes or parents in a directed acyclic graph (DAG).\n",
    "* It facilitates the identification of causal effects because it provides a way to isolate the effects of specific variables from the influence of other variables in the system. \n",
    "\n",
    "\n",
    " 3) **Faithfulness** — all conditional independences in true underlying distribution $p$ are represented in $\\mathcal{G}$ \n",
    "\n",
    "* It states that a causal model should be faithful to the observed data, meaning that all the conditional independence relationships present in the data are reflected in the causal model.\n",
    "* If two variables are statistically independent in the observed data, they should be independent in the underlying causal model. Similarly, if two variables are dependent in the observed data, there should be a corresponding causal relationship between them in the causal model.\n",
    "\n",
    "\n",
    " 4) **Sufficiency** — any pair of nodes in $\\mathcal{G}$ has no common external cause.\n",
    "\n",
    "* It implies that once we condition on the observed variables, there are no additional unobserved variables that provide further information or influence the relationships between the observed variables. \n",
    "* It ensures that the observed associations between variables are not confounded by unobserved factors.\n",
    "* The Sufficiency assumption is an assumption and not a guarantee. It relies on the notion that all relevant confounding variables are observed and appropriately accounted for in the analysis. \n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "In this lecture, we are more focus on Markovian assumption. However, there is a comprehensive discussion of these four causal assumptions in [Kalainathan et al., 2018]( https://arxiv.org/abs/1803.04929). \n",
    "Also check this nice paper, [An introduction to causal inference](https://www.cmu.edu/dietrich/philosophy/docs/scheines/introtocausalinference.pdf), by Richard Scheines, CMU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9abab6e8-564d-408b-a29d-aa0e1b64f25c",
    "deepnote_cell_height": 1409.8125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Markov Property\n",
    "### Definition of Markov Property\n",
    "\n",
    "The Markov Property assumption states that the causal relationships among variables can be represented by a Directed Acyclic Graph (DAG), where each variable is independent of its descendants given its parents. This assumption helps identify the minimal set of variables needed to estimate causal effects. \n",
    "\n",
    "\n",
    "- When a distribution $p$ is Markovian with respect to a graph $\\mathcal{G}$, this graph encodes certain independence in the distribution.\n",
    "\n",
    "- We will see how Markov property links causal DAGs to conditional probabilities (from data).\n",
    "\n",
    "We use two basic definitions before further explaining Markov property:\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Chain Rule:** We know from the definition of conditional probability that: \n",
    "\n",
    "$$P(X_1,X_2) = P(X_1|X_2) \\cdot P(X_2) = P(X_2|X_1) \\cdot P(X_1)$$\n",
    "\n",
    "This can be generalied to multiple events (random variables): \n",
    "\n",
    "$$\n",
    "\\mathrm{P}\\left(X_n, \\ldots, X_1\\right) = \\mathrm{P}\\left(X_n \\mid X_{n-1}, \\ldots, X_1\\right) \\cdot \\mathrm{P}\\left(X_{n-1}, \\ldots, X_1\\right)\n",
    "$$\n",
    "\n",
    "Repeating this process with each final term creates the product form:\n",
    "\n",
    "$$\n",
    "\\mathrm{P}\\left(X_n, \\ldots, X_1\\right) = \\mathrm{P}\\left(\\bigcap_{k=1}^n X_k\\right) = \\prod_{k-1}^n \\mathrm{P}\\left(X_k \\mid \\bigcap_{j=1}^{k-1} X_j\\right)\n",
    "$$\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Conditional Independence:** Now let's $X,Y,Z$ be three random variables.\n",
    "\n",
    "- $X$ and $Y$ *are (marginally) independent* if: \n",
    "\n",
    "$$X \\perp Y \\Leftrightarrow  P(X,Y) = P(X) \\cdot P(Y)$$\n",
    "\n",
    "<br/>\n",
    "\n",
    "- $X$ and $Y$ *are conditionally independent* if: \n",
    "\n",
    "$$X \\perp Y|Z \\Leftrightarrow  P(X, Y| Z) = P(X|Z) \\cdot P(Y|Z)$$ \n",
    "\n",
    "<br/>\n",
    "\n",
    "- Conditional Independence mathematically is equivalent to the statement that: *the joint distribution of the variables $X = \\{ X_1 , X_2 , ..., X_n \\}$ in a DAG $\\mathcal{G}$ can be factorized using the Markov factorization or Bayesian Network Factorization given parents $pa$ of ecah variable*:\n",
    "\n",
    "$$P(X) = \\prod_{i=1}^n P(X_i|pa(X_i))$$\n",
    "\n",
    "- In other words:\n",
    "    - Conditional on its parents, a variable $X_i$ is independent of its predecessors variables (conditional independence). \n",
    "    - Parents of $X_i$  or $pa(X_i)$ are independent aspects of the mechanism that generated $X_i$ values (data).  \n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Definition 3.1 (Markov Property):** Given a DAG $\\mathcal{G}$ and a joint distribution $P_X$, this distribution is said to satisfy:\n",
    "\n",
    "<br/>\n",
    "\n",
    "(a) the **global Markov property** with respect to the DAG $\\mathcal{G}$ if:\n",
    "\n",
    "$$\\mathbf{A} \\!\\perp\\!\\!\\!\\perp_{\\mathcal{G}} \\mathbf{B}|\\mathbf{C} \\Rightarrow \\mathbf{A} \\!\\perp\\!\\!\\!\\perp \\mathbf{B}|\\mathbf{C}$$\n",
    "\n",
    "\n",
    "for all disjoint vertex sets $A,B,C$, the symbol $\\!\\perp\\!\\!\\!\\perp_{\\mathcal{G}}$ denotes d-separation.\n",
    "\n",
    "<br/>\n",
    "\n",
    "(b) the **local Markov property** with respect to the DAG $\\mathcal{G}$ if each variable is independent of its predecessors given its parents, and\n",
    "\n",
    "<br/>\n",
    "\n",
    "(c) the **Markov factorization property** with respect to the DAG $\\mathcal{G}$ if\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x})=p\\left(x_{1}, \\ldots, x_{n}\\right)=\\prod_{i=1}^{n} p\\left(x_{i} \\mid \\mathbf{p} \\mathbf{a}_{i}^{\\mathcal{G}}\\right)\n",
    "$$\n",
    "\n",
    "For this last property, we have to assume that $P_X$ has a density $p$; the factors in the product are referred to as causal Markov kernels describing the conditional distributions $P_{X_{i} \\mid \\mathbf{PA}_{i}^{\\mathcal{G}}}$\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "It's worth noting that: \n",
    "\n",
    "* The Markov Property assumption relies on the acyclicity assumption (no cycles or feedback loops in the graph) and assumes that all relevant variables are included in the analysis. \n",
    "* The Markov Property is an assumption and may not hold in all real-world scenarios. \n",
    "* Violation of the Markov Property can result in biased causal estimates. Careful consideration of the causal structure and potential confounding factors is necessary for valid causal inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "54f765eb-c4eb-493b-b350-612ac81f4622",
    "deepnote_cell_height": 462.703125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Truncated Factorization\n",
    "\n",
    "An extension to Markov factorization is that the probability distribution generated by an *intervention do(x)* operation. \n",
    "\n",
    "For example, $do( X_1 = x_1 )$ is given by a Truncated Factorization:\n",
    "\n",
    "$$P(X_2, \\dots, X_n |do(X_1 = x_1)) = \\prod_{i \\neq 1} P(X_i | pa(X_i))$$\n",
    "\n",
    "**Truncated Factorization** suggested by (Pearl, 1993) is also known as the **G-formula** (Robins, 1986), or **manipulation theorem** (Spirtes, 2000), or **intervention formula** (Lauritzen, 2002).\n",
    "\n",
    "We have $X = \\{ X_1, X_2 , X_3 , ..., X_n \\}$, the causal effect of $X_1$ on $X_2$ can now be derived by marginalizing (summing) the truncated factorization over $X' = \\{ X_3 , ..., X_n \\}$:\n",
    "\n",
    "$$P(X_2|do(X_1=x_1)) = \\sum_{x'} P(X_2, X'|do(X_1=x_1))$$\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "See an example with three nodes for Truncated Factorization in this short [video](https://www.youtube.com/watch?v=_gcmY5ukbWM) made by [Brady Neal](https://www.bradyneal.com/causal-inference-course).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b9976685-062a-4fb5-b76d-1b6947285f40",
    "deepnote_cell_height": 100.390625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Local Markov Example: a 4 Nodes Graph\n",
    "\n",
    "Given its parents in the DAG, a node $X$ is independent of all its predecessors. For example, let consider the graph below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "f4624bf1-16eb-4d02-8152-4e8b4cc0287c",
    "deepnote_cell_height": 386.328125,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     199.328125
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 6,
    "execution_start": 1647607468720,
    "source_hash": "974d7484",
    "tags": []
   },
   "outputs": [],
   "source": [
    "g=gr.Digraph(edge_attr={'arrowhead':'vee', 'arrowsize':'1'}, graph_attr={'rankdir': 'LR', 'layout':'circo'})\n",
    "g.edge(\"X1\", \"X2\")\n",
    "g.edge(\"X1\", \"X3\")\n",
    "g.edge(\"X2\", \"X3\")\n",
    "g.edge(\"X3\", \"X4\")\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "47b98605-f268-4c0d-ac7d-3560d21b445d",
    "deepnote_cell_height": 146.265625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "In the DAG above, we have:\n",
    "\n",
    "$$\n",
    "P(x_1, x_2, x_3, x_4) = P(x_1) P(x_2|x_1) P(x_3| x_2,x_1) P(x_4 | x_3, x_2, x_1)\n",
    "$$\n",
    "\n",
    "What happens with the *Local Markov Assumption*?\n",
    "\n",
    "$$\n",
    "P(x_1, x_2, x_3, x_4) = P(x_1) P(x_2|x_1) P(x_3| x_2,x_1) \\underbrace{P(x_4 | x_3, x_2, x_1)}_{P(x_4 | x_3)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "064b5dc8-e8c5-4c7b-849f-c265910030aa",
    "deepnote_cell_height": 1565.796875,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### D-separation in Graphs vs. Conditional independence in Distributions\n",
    "\n",
    "We saw that DAGs offer an efficient (and visually easier) way to factorize the joint probability between random variables.\n",
    "We present here a summary of the cases we have seen:\n",
    "\n",
    "- $A$ and $B$ are **marginally dependent**  ($A \\not \\perp B$)\n",
    "\n",
    " $$P(A,B) \\neq P(A) \\cdot P(B)$$\n",
    "![img](img/ch3/DAGs_PDFs_marginallyDep.png)\n",
    "\n",
    "- $A$ and $B$ are **marginally independent**  ($A \\perp B$)\n",
    "\n",
    " $$P(A,B) = P(A) \\cdot P(B)$$\n",
    "![img](img/ch3/DAGs_PDFs_marginallyIndep.png)\n",
    "\n",
    "\n",
    "- $A$ and $B$ are **conditionally independent given $C$**  ($A \\perp B | C$)\n",
    "\n",
    " $$P(A,B|C) = P(A|C) \\cdot P(B|C)$$\n",
    "![img](img/ch3/DAGs_PDFs_conditionallyIndep.png)\n",
    "\n",
    "\n",
    "- $A$ and $B$ are **conditionally dependent given $C$**  ($A \\not \\perp B | C$)\n",
    "\n",
    " $$P(A,B|C) \\neq P(A|C) \\cdot P(B|C)$$\n",
    "![img](img/ch3/DAGs_PDFs_conditionallyDep.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5af83390-7f1e-4134-a627-da1c3865e09d",
    "deepnote_cell_height": 265.125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "\n",
    "### Global Markov Assumption in a Graph\n",
    "\n",
    "Two (sets of) nodes $X$ and $Y$ are d-separated by a set of nodes $\\{Z\\}$ if all the paths between (any node in) $X$ and (any node in) $Y$ are blocked by $\\{Z\\}$. \n",
    "d-separation implies independence. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Theorem 3.2. (Global Markov Assumption):** Given that distribution $P$ is Markov with respect to graph $\\mathcal{G}$, d-separation in a graph $\\mathcal{G}$ is equivalent of conditional independent in distribution $P$. This is also called *Global Markov assumption*.\n",
    "\n",
    "$X \\!\\perp\\!\\!\\!\\perp_G Y |\\{Z\\} \\Rightarrow X \\!\\perp\\!\\!\\!\\perp_P Y |\\{Z\\}$\n",
    "\n",
    "Because of d-separation, we can read it $P$ is Markov with respect to $\\mathcal{G}$ or $P$ satisfy Markov assumption in respect to $\\mathcal{G}$.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d0ccc148-b2b7-4728-934e-793d36cc2831",
    "deepnote_cell_height": 1144.9375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Markov Equivalence\n",
    "\n",
    "Markov equivalence is a concept that refers to a set of DAGs that encode the same set of conditional independence relationships among variables. In other words, Markov equivalent DAGs exhibit the same pattern of statistical dependencies among variables, despite potentially having different graphical structures.\n",
    "\n",
    "We formalize here this concept in two steps:\n",
    "\n",
    "First, we introduce two important graph qualities that we can use to distinguish equivalent graphs: \n",
    "\n",
    "- **Skeleton:** an undirected graph obtained by removing directions \n",
    "\n",
    "- **Immorality (v-structure or collider):** a collider structure A → C ← B, such that there is no direct edge between A and B \n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Definition 3.3. (Markov Equivalence of Graphs)):** We denote by $\\mathcal{M}(\\mathcal{G})$ the set of distributions that are Markovian with respect to $\\mathcal{G}$ :\n",
    "\n",
    "$\\mathcal{M}(\\mathcal{G})$ = {$P$ : $P$ satisfies the global (or local) Markov property with respect to $\\mathcal{G}$}.\n",
    "\n",
    "Two DAGs $\\mathcal{G_1}$ and $\\mathcal{G_2}$ are **Markov equivalent** if $\\mathcal{M}(\\mathcal{G_1})$ = $\\mathcal{M}(\\mathcal{G_2})$. \n",
    "\n",
    "* This is the case if and only if $\\mathcal{G_1}$ and $\\mathcal{G_2}$ satisfy the same set of d-separations, which means the Markov condition entails the same set of (conditional) independence conditions.\n",
    "* The set of all DAGs that are Markov equivalent to the same distribution is called **Markov equivalence class of $\\mathcal{G}$**. \n",
    "\n",
    "</div>\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Lemma 3.4. (Graphical criteria for Markov equivalence):** Two DAGs $\\mathcal{G_1}$ and $\\mathcal{G_2}$ are Markov equivalent if and only if they have the same skeleton and the same immoralities (colliders).\n",
    "\n",
    "   - Two graphs are Markov equivalent, if they entail the same conditional independencies. \n",
    "   - Two Markov equivalent graphs can be used for representing the same set of probability distributions.\n",
    "\n",
    "</div>\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "Following figure shows an example of two Markov equivalent graphs.\n",
    "\n",
    "![img](img/ch3/Markov_equivalent.png)\n",
    "\n",
    "* Notice that chain and fork structures are treated the same in Markov equivalent DAGs.\n",
    "\n",
    "<br/>\n",
    "\n",
    "\n",
    "Markov equivalence has important implications for causal inference because: \n",
    "\n",
    "* Different causal graphs can lead to the same set of observed statistical dependencies. This means that two or more DAGs that are Markov equivalent cannot be distinguished based solely on observed data and statistical tests.\n",
    "\n",
    "* Given observational data, it is generally not possible to uniquely determine the true causal structure among variables when multiple DAGs are Markov equivalent. \n",
    "\n",
    "* Additional interventions or domain knowledge can sometimes help resolve the ambiguity and identify the true causal structure.\n",
    "\n",
    "\n",
    "\n",
    "See a short video on [Markov Equivalence](https://www.youtube.com/watch?v=nnjKCtdORwY&list=PLoazKTcS0Rzb6bb9L508cyJ1z-U9iWkA0&index=64) made by Brady Neal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Equivalent Example, a 5 Node Graph using Python\n",
    "\n",
    "You can use the [pgmpy](https://pgmpy.org/index.html) python library to test the Markov equivalence in DAGs.\n",
    "\n",
    "The pgmpy is a python library for Bayesian Networks with a focus on Structure Learning, Parameter Estimation, and Causal Inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.base import DAG\n",
    "import graphviz as gr\n",
    "\n",
    "#Ceate the DAGs\n",
    "G1 = DAG()\n",
    "G1.add_edges_from([('X', 'Y'), ('X', 'Z'),\n",
    "                  ('Z', 'Y'), \n",
    "                  ('V', 'Z'),\n",
    "                  ('U','V')])\n",
    "\n",
    "G2 = DAG()\n",
    "G2.add_edges_from([('X', 'Y'), ('X', 'Z'),\n",
    "                  ('Z', 'Y'), \n",
    "                  ('V', 'Z'),\n",
    "                  ('V','U')])\n",
    "\n",
    "\n",
    "def plot_from_model_pgmpy(edges):\n",
    "    # plot\n",
    "    edges = [el for el in edges] #unpack\n",
    "    g = gr.Digraph()\n",
    "    \n",
    "    for i in range(0, len(edges)):\n",
    "        g.edge(edges[i][0],edges[i][1])\n",
    "    return g\n",
    "\n",
    "# Plot the DAGs\n",
    "g1 = plot_from_model_pgmpy(G1.edges())\n",
    "g2 = plot_from_model_pgmpy(G2.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are $G_1$ and $G_2$ markov equivalent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G1.is_iequivalent(G2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the immoralities in $G_1$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G1.get_immoralities()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the immoralities in $G_2$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G2.get_immoralities()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d662b0f6-db9a-4355-be3f-3f234ddf3355",
    "deepnote_cell_height": 905.859375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Markov Blanket\n",
    "\n",
    "The **Markov Blanket** of a variable $X$ consists of three sets of variables:\n",
    "\n",
    "* Parents of $X$: The parents of $X$ in the DAG are the variables that directly influence X. In terms of causality, the parents represent the direct causes of $X$.\n",
    "\n",
    "* Children of $X$: The children of $X$ in the DAG are the variables that are directly influenced by $X$. In terms of causality, the children represent the direct effects of $X$.\n",
    "\n",
    "* Parents of the children (spouses) of $X$: These are the variables that are parents of the children of $X$ but are not themselves parents of $X$. These variables provide information about the relationship between $X$ and its children, conditioning on other variables.\n",
    "\n",
    "The **Markov blanket** of a node $X$, denoted by $MB(X)$ is the union of the children, parents and spouses (parents of children) of $X$. \n",
    "\n",
    "- $MB(X)$ is a subset that contains all the useful information about $X$. \n",
    "- The Markov blanket of a variable represents the **minimal set of variables** that need to estimate the causal effect of $X$ on any other variable in the graph. \n",
    "- $X$ is conditionally independent of all nodes outside its Markov blanket given its Markov blanket:\n",
    "\n",
    "$$X \\!\\perp\\!\\!\\!\\perp N \\setminus MB(X) | MB(X)$$\n",
    "\n",
    "\n",
    "![img](img/ch3/Markov_blanket.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Blanket Example, a 5 Node Graph using Python\n",
    "\n",
    "We use the same pgmpy python package.\n",
    "\n",
    "Let's find the Markov blanket for node $Z$ in the previous graph $G_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G1.get_markov_blanket('Z')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4a00d9983b664ea1a253163e6fb4991d",
    "deepnote_cell_height": 645.421875,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Common Cause Principle\n",
    "\n",
    "According to the **Common Cause Principle**, when variables $X$ and $Y$ are found to be associated or correlated, there must be a common cause or confounding variable that influences both $X$ and $Y$. In other words, the observed relationship between variable $X$ and variable $Y$ is not only indicative of a direct causal relationship but also can be the result of their shared association with a third variable.\n",
    "\n",
    "\n",
    "We can use Markov property to justify **Common Cause Principle**. It states that when the random variables $X$ and $Y$ are dependent, there must be a *causal explanation* for this dependence as follows:\n",
    "\n",
    "- $X$ is (possibly indirectly) causing $Y$, or \n",
    "- $Y$ is (possibly indirectly) causing $X$, or\n",
    "- there is a (possibly unobserved) common cause $Z$ that (possibly indirectly) causes both $X$ and $Y$.\n",
    "\n",
    "The following proposition justifies Reichenbach’s principle with respect to a notion of “causing,” namely the existence of a directed path.\n",
    "\n",
    "<br/>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Proposition 3.5. (Common Cause Principle):** Assume that any pair of variables $X$ and $Y$ can be embedded into a larger system in the following sense. There exists a **causal model** over the collection $\\mathbf{X}$ of random variables that contains $X$ and $Y$ within graph $\\mathcal{G}$. Then common cause principle follows from the Markov property. If $X$ and $Y$ are dependent, then there is:\n",
    "\n",
    "<br/>\n",
    "\n",
    "- either a directed path from $X$ to $Y$ , or \n",
    "\n",
    "<br/>\n",
    "\n",
    "- a directed path from $Y$ to $X$,or \n",
    "\n",
    "<br/>\n",
    "\n",
    "- there is a node $Z$ with a directed path from $Z$ to $X$ and from $Z$ to $Y$.\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "The Common Cause Principle highlights the importance of identifying and accounting for confounding variables when drawing causal conclusions from observational data. By controlling for confounders through study design, statistical adjustment, or randomized controlled experiments, researchers can reduce the likelihood of drawing incorrect causal inferences and gain a better understanding of the true causal relationships between variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4851617257fe4ddd9d90faa06023646f",
    "deepnote_cell_height": 931.171875,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Causal Graphical Models\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Proposition 3.6. (Causal Model vs. Markov Property):** [Source](https://mitpress.mit.edu/books/elements-causal-inference) Assume that $P_X$ is induced by an Structural causal Model (SCM) with graph $\\mathcal{G}$. Then, $P_X$ is Markovian with respect to $\\mathcal{G}$. The assumption that says a distribution is Markovian with respect to the causal graph is sometimes called the **causal Markov condition**. \n",
    "\n",
    "</div>\n",
    "\n",
    "<br/>\n",
    "\n",
    "We will see in the next chapter that it is sufficient to know the observational data distribution and the related graph structure for defining intervention distributions for a process. Therefore, we define a causal graphical model as a pair consisting of a *graph* and an *observational distribution* such that the distribution is Markovian with respect to the graph (causal Markov condition).\n",
    "\n",
    "<br/>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Definition 3.7 (Causal graphical model):** A causal graphical model over random variables $\\mathbf{X}=\\left(X_{1}, \\ldots, X_{d}\\right)$ contains a graph $\\mathcal{G}$ and a collection of functions (structure) $f_{j}\\left(x_{j}, x_{\\mathbf{P A}_{j}^{\\mathcal{G}}}\\right)$ that integrate to $1:$\n",
    "\n",
    "$$\n",
    "\\int f_{j}\\left(x_{j}, x_{\\mathbf{P A}_{j}^{\\mathcal{G}}}\\right) d x_{j}=1\n",
    "$$\n",
    "\n",
    "These collection of functions (structure) induces a distribution $P_{\\mathbf{X}}$ over $\\mathbf{X}$ via\n",
    "\n",
    "$$\n",
    "p\\left(x_{1}, \\ldots, x_{d}\\right)=\\prod_{j=1}^{d} f_{j}\\left(x_{j}, x_{\\mathbf{PA_j}}^{\\mathcal{G}}\\right)\n",
    "$$\n",
    "\n",
    "and thus play the role of conditionals: \n",
    "\n",
    "$$\n",
    "f_{j}\\left(x_{j}, x_{\\mathbf{PA_j^\\mathcal{G}}}\\right) = p\\left(x_{j} \\mid x_{\\mathbf{PA_j}^\\mathcal{G}}\\right)\n",
    "$$\n",
    "\n",
    "Refer to [Causal Elements Book, Chapter 6](https://mitpress.mit.edu/books/elements-causal-inference) for proof.\n",
    "</div>\n",
    "\n",
    "<br/>\n",
    "\n",
    "If a distribution $P_X$ over $X$ is Markovian with respect to a graph $\\mathcal{G}$ and allows for a strictly positive, continuous density $p$, the pair $(P_X,\\mathcal{G})$ defines a **causal graphical model** by:\n",
    "\n",
    "$$\n",
    "f_j\\left(x_{j} \\mid x_{\\mathbf{PA_j}^\\mathcal{G}}\\right) = p\\left(x_{j} \\mid x_{\\mathbf{PA_j}^\\mathcal{G}}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "See a short video on [Causal Graphs](https://www.youtube.com/watch?v=vjvP9oRgZyM&list=PLoazKTcS0Rzb6bb9L508cyJ1z-U9iWkA0&index=21) made by Brady Neal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faithfulness and Causal Minimality\n",
    "\n",
    "\n",
    "Faithfulness assumption suggests that a causal model is faithful if and only if it satisfies the following condition:\n",
    "\n",
    "*For every conditional independence relationship that holds in the probability distribution induced by the causal model, there exists a corresponding d-separation relationship in the causal graph.*\n",
    "\n",
    "In simpler terms, faithfulness asserts that if two variables, $X$ and $Y$, are conditionally independent given a set of other variables $Z$ in a causal model, then there should be no direct causal link or path between $X$ and $Y$ in the underlying DAG $\\mathcal{G}$ when conditioning on $Z$.\n",
    "\n",
    "\n",
    "As we seen, the **Markov** assumption enables us to undrestand **independences** from a graph structure. **Faithfulness** allows us to infer **dependences** from the graph structure.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Definition 3.8 (Faithfulness and causal minimality):** Consider a distribution $P_\\mathbf{X}$ and a DAG $\\mathcal{G}$.\n",
    "    \n",
    "**(a)** $P_\\mathbf{X}$ is faithfulls to DAG $\\mathcal{G}$ if\n",
    "\n",
    "$$\n",
    "\\mathbf{A} \\!\\perp\\!\\!\\!\\perp \\mathbf{B}\\left|\\mathbf{C} \\Rightarrow \\mathbf{A} \\!\\perp\\!\\!\\!\\perp_{\\mathcal{G}} \\mathbf{B}\\right| \\mathbf{C}\n",
    "$$\n",
    "\n",
    "for all  all disjoint vertex sets $\\mathbf{A}, \\mathbf{B}, \\mathbf{C}$.\n",
    "\n",
    "**(b)** A distribution satisfies causal minimality with respect to $\\mathcal{G}$ if it is Markovian with respect to $\\mathcal{G}$, but not to any proper subgraph of $\\mathcal{G}$.\n",
    "</div>\n",
    "\n",
    "<br/>\n",
    "\n",
    "Part (a) posits an implication that is the opposite of the global Markov condition:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} \\!\\perp\\!\\!\\!\\perp_{\\mathcal{G}} \\mathbf{B}|\\mathbf{C} \\Rightarrow \\mathbf{A} \\!\\perp\\!\\!\\!\\perp \\mathbf{B}| \\mathbf{C},\n",
    "$$\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Proposition 3.9 (Faithfulness implies causal minimality):** If $P_\\mathbf{X}$ is faithful and Markovian with respect to $\\mathcal{G}$, then causal minimality is satisfied.\n",
    "    \n",
    "</div>\n",
    "\n",
    "A distribution is minimal with respect to $\\mathcal{G}$ if and only if there is no node that is conditionally independent of any of its parents, given the remaining parents.\n",
    "\n",
    "\n",
    "However, it's important to note that the **Faithfulness assumption** does not guarantee that all causal relationships can be identified or estimated. There may be cases where causal relationships are not reflected in the observed data due to various reasons such as unobserved confounding or measurement error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Causal Sufficiency\n",
    "\n",
    "A set of variables $P_\\mathbf{X}$ is usually said to be **causally sufficient** if there is no hidden common cause $C \\notin \\mathbf{X}$ that is causing more than one variable in $\\mathbf{X}$.\n",
    "\n",
    "- The definition **causally sufficient** matches the intuitive meaning of the set of “relevant” variables. However, it uses the concept of a **common cause**.\n",
    "\n",
    "- a variable $C$ is a **common cause** of $X$ and $Y$ if there is a directed path from $C$ to $X$ and $Y$ that does not include $Y$ and $X$, respectively.\n",
    "\n",
    "- **Common causes** are also called **confounders** and we use these terms interchangeably.\n",
    "- **Causal Sufficiency** describes when a set of variables is large enough to perform causal reasoning, in the sense of computing observational and intervention distributions.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Definition 3.10 (Causal Sufficiency):** We call a set $\\mathbf{X}$ of variables interventionally sufficient if there exists an SCM over $\\mathbf{X}$ that cannot be falsified as an interventional model. It also induces (create) observational and intervention distributions that coincide with what we observe in practice.\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "For more information on Causal or Interventional sufficiency, see Chapter 9. Hidden Variables from [Elements of Causal Inference](https://mitpress.mit.edu/books/elements-causal-inference) book.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal DAGs in Action \n",
    "\n",
    "<font color='blue'>What we saw is part of the math under the hook! In this course, we are focused on using causal DAGs. However, DAGs can still be of great practical use without detailed knowledge of this mathematical background. If you want to know more in-depth math, see Judea Pearl's paper [Foundations of Causal Inference, 2010](https://ftp.cs.ucla.edu/pub/stat_ser/r355-reprint.pdf) and books we suggested in the syllabus.</font>\n",
    "\n",
    "\n",
    "Using causal assumptions, the associated properties, and the python libraries we presented in this chapter, researchers are making causal models to answer different causal questions in various domains.\n",
    "\n",
    "For example, researchers in this [paper](https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-017-0448-y#citeas) created a causal disease network that implements disease causality through text mining on biomedical literature.\n",
    "\n",
    "![img](img/ch3/Causal_Model_Disease_Categories.png)\n",
    "\n",
    "\n",
    "As anothe exmaple, authores in this [paper](https://www.nature.com/articles/s41467-020-15195-y) proposed a method that integrates observational data and causal inference techniques to identify causal relationships between climate variables. By constructing causal networks, they provide insights into the mechanisms driving climate change and enable more accurate predictions. \n",
    "\n",
    "![img](img/ch3/Causal_Model_Climate.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c9e7f5186c4545eca8adbf149c8ad029",
    "deepnote_cell_height": 167.171875,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## References\n",
    "\n",
    "This chapter contents are highly inspired by the [Elements of Causal Inference (Open Access) book](https://mitpress.mit.edu/books/elements-causal-inference) by By Jonas Peters, Dominik Janzing and [Bernhard Schölkopf](https://www.is.mpg.de/~bs) especially for definitions and propositions.\n",
    "\n",
    "We also used a graph example from the [Introduction to Causal Inference](https://www.bradyneal.com/causal-inference-course) course by Brady Neal. He made a very nice open-access online course accompanied by videos on [YouTube](https://youtube.com/playlist?list=PLoazKTcS0Rzb6bb9L508cyJ1z-U9iWkA0).\n",
    "\n",
    "More proofs and theores can ber find in Judea Pearl's [Causal Inference in Statistics: A Primer](https://www.wiley.com/en-us/Causal+Inference+in+Statistics%3A+A+Primer-p-9781119186847).\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "09b16480-942c-492c-be08-da138720dcc2",
  "interpreter": {
   "hash": "f82454df3ab4669350e470cecfad51160e6fdff8e76eafd19d8880dd92d922a3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
