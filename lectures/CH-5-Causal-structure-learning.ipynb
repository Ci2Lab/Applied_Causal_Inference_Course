{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Causal Structure Learning from Data\n",
    "\n",
    "In the previous chapter, we build a causal model knowing the equation structure of the problem from our domain knowledge. However, we do not have the privilege of knowing causal structure in the real world.\n",
    "In this chapter, our goal is to create a causal model without knowing the causal structure of the problem. In other words, our problem is **causal structure identification or discovery**.\n",
    "\n",
    "- **With structure learning, we want to determine the structure of the graph that best captures the causal dependencies between the variables in a data set.** \n",
    "- In other words, given a dataset, we derive a causal model that describes it.\n",
    "\n",
    "\n",
    "## How can we estimate the causal structure from a dataset?\n",
    "\n",
    "Unfortunately, there is no standard recipe for that, and that's why the causal inference is generally challenging. Causal discovery is an example of an inverse problem. \n",
    "\n",
    "The usual approach to solving inverse problems is to **make assumptions** about what we are trying to investigate. This narrows down the possible solutions and hopefully makes the problem solvable. \n",
    "\n",
    "Remember that there are four common assumptions made across causal discovery algorithms. \n",
    "\n",
    "- **Acyclicity** — causal structure can be represented by a DAG $G$ (mentioned in [Chapter 3](/lectures/CH-3-Graphical-Causal-Models.ipynb))\n",
    "- **Markov Property** — all nodes are independent of their non-descendants when conditioned on their parents (mentioned in [Chapter 3](/lectures/CH-3-Graphical-Causal-Models.ipynb))\n",
    "- **Faithfulness** — all conditional independences in true underlying distribution $p$ are represented in $G$ \n",
    "- **Sufficiency** — any pair of nodes in $G$ has no common external cause\n",
    "\n",
    "\n",
    "A comprehensive discussion of these causal assumptions is availabe  in [Kalainathan et al., 2018]( https://arxiv.org/abs/1803.04929)\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there is a large pool of different methods available (and it is still an active field of ongoing research), all these different structure learning techniques can be generally divided into two main groups:  **Independence-Based Methods** and **Score-Based Methods**\n",
    "\n",
    "### A. Independence-Based Methods\n",
    "\n",
    "Independence-based methods assume that the distribution is faithful to the underlying DAG. In orher workds, there is a one-to-one correspondence between d-separationsin the graph and conditional independences in the distribution of the data.  Any query of a d-separation statement can therefore be answered by checking the corresponding conditional independence test.  Inductive causation (IC) algorithm, the\n",
    "SGS algorithm, and the PC algorithm (named after Peter Spirtes and Clark Glymour) are famous independence-test based algorithms. \n",
    "\n",
    "The figure below summarizes the approach for the identification of causal structures. Independence-based methods test for conditional independences in the data;\n",
    "these properties are related to the graph structure by the Markov condition and faithfulness. However,often the graph is not uniquely identifiable; the method may therefore output different\n",
    "graphs $G$ and $G'$.\n",
    "\n",
    "![img](img/ch6/independence_tests.png)\n",
    "\n",
    "Most independence-based methods first estimate the skeleton, that is, the undirected edges, and orient as many edges as possible afterward.\n",
    "\n",
    "#### Estimation of the skeleton\n",
    "\n",
    "The following two statements hold:\n",
    "\n",
    "- Two nodes $X,Y$ in a DAG $(\\mathbf{X},\\mathcal{E})$ are adjacent if and only if they cannot be d-separated by any subset $S \\in \\mathbf{V} \\ {X,Y}$.\n",
    "- If two nodes $X,Y$ in a DAG $(\\mathbf{X},\\mathcal{E})$ are not adjacent, then they are d-separated by either $\\mathbf{Pa}_X$ or $\\mathbf{Pa}_Y$.\n",
    "\n",
    "It follows that if two variables are always dependent, no matter what other variables one conditions on, these two variables must be adjacent (i.e., an edge must connect them).\n",
    "\n",
    "#### Orientation of Edges\n",
    "\n",
    "After the skeleton is built, if two nodes are not directly connected in the obtained skeleton, there is a set that d-separates these nodes. \n",
    "Suppose that the skeleton contains the structure $X - Z - Y$ with no direct edge between $X$ and $$. Further, let $A$ be a set that d-separates $X$ and $Y$. The structure $X - Z - Y$ is\n",
    "an immorality and can therefore be oriented as $X → Z ← Y$ if and only if $Z \\notin A$.\n",
    "After the orientation of immoralities, further edges are oriented in order to avoid cycles.\n",
    "\n",
    "<br>\n",
    "\n",
    "An outline of the PC algorithm is illustrated in the figure below (image taken from https://towardsdatascience.com/causal-discovery-6858f9af6dcb). \n",
    "\n",
    "![img](img/ch6/Trick1.png)\n",
    "\n",
    "\n",
    "**Step 1:** form a fully connected, undirected graph using every variable in the dataset. \n",
    "\n",
    "**Step 2:**  edges are deleted if the corresponding variables are independent. \n",
    "\n",
    "**Step 3:**  connected edges undergo conditional independence testing, e.g., independence test of bottom and far-right node conditioned on the middle node (see step 2). If conditioning on a variable kills the dependence, that variable is added to the Separation set for those two variables. Depending on the size of the graph, conditional independence testing will continue (i.e. condition on more variables) until there are no more candidates for testing.\n",
    "\n",
    "**Step 4:**  colliders (i.e. $X \\rightarrow Y \\leftarrow Z$) are oriented based on the Separation set of node pairs. \n",
    "\n",
    "**Step 5:**  remaining edges are directed based on two constraints, 1) no new v-structures and 2) no directed cycles can be formed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Score-based Methods\n",
    "\n",
    "In the preceding section we have directly used the independence statements to infer the graph. Alternatively, we can test different graph structures in their ability to\n",
    "fit the data. The idea is that graph structures encoding the wrong conditional independences, for example, will yield bad model fits.\n",
    "\n",
    "#### Best Scoring Graph  \n",
    "\n",
    "Given data $\\mathcal{D} = (X^1,\\dots,X^n)$ from a vector $X$ of variables (e.g., a sample containing $n$ i.i.d. observations), the idea is to assign a score $S(\\mathcal{D},\\mathcal{G})$ to each graph $G$ and search over the space of DAGs to find the graph with the highest score:\n",
    "\n",
    "$$\\hat{ \\mathcal{D} } = argmax_{\\mathcal{G} DAG over \\mathbf{X}} S(\\mathcal{D},\\mathcal{G})$$\n",
    "\n",
    "There are several possibilities to define such a scoring function $S$. Often a parametric model is assumed, for example linear Gaussian equations or multinomial distributions which introduces a set of parameters $\\theta$.\n",
    "\n",
    "Here listed are some common functions or approaches used.\n",
    "\n",
    "**(Penalized) Likelihood:**  \n",
    "\n",
    "For each graph we may consider the maximum likelihood estimator $\\hat{\\theta}$ for $\\theta$ and then define a score function by the BIC: \n",
    "\n",
    "$$ S(\\mathcal{D},\\mathcal{G}) = \\log p(\\mathcal{D}|\\hat{\\theta}, \\mathcal{g}) - \\frac{\\# parameters}{2} \\log n $$ \n",
    "\n",
    "where $\\log p(\\mathcal{D}|\\hat{\\theta}, \\mathcal{G})$ is the log likelihood and n is the sample size.\n",
    "\n",
    "**Bayesian Scoring Functions:** \n",
    "\n",
    "We define priors $p_{pr}(G)$ and $p_{pr}(\\theta)$ over DAGs and parameters, respectively, and consider the log posterior as a score function (note that $p(\\mathcal{D})$ is constant over all DAGs):\n",
    "\n",
    "$$ S(\\mathcal{D},\\mathcal{G}) := \\log p (\\mathcal{D},\\mathcal{G}) \\propto \\log p_{pr}(\\mathcal{G}) + \\log p(\\mathcal{D}|\\mathcal{G})$$ \n",
    "\n",
    "where $\\log p(\\mathcal{D}|\\mathcal{G})$ is the marginal likelihood.\n",
    "\n",
    "In the case of parametric models, we call two graphs $G_1$ and $G_2$ distribution equivalent if for each parameter $\\theta_1$ there is a corresponding parameter $\\theta_2$, such that the distribution obtained from $G_1$ in combination with θ1 is the same as the distribution obtained from graph $G_1$ with $\\theta_2$, and vice versa. \n",
    "It can be shown that in the linear Gaussian case, for example, two graphs are distribution equivalent if and only if they are Markov equivalent. \n",
    "\n",
    "\n",
    "#### Greedy Search Techniques\n",
    "\n",
    "The search space of all DAGs is growing superexponentially in the number of variables. In general, finding the optimal scoring DAG is a NP-hard problem. \n",
    "Just to have an idea, , the numbers of DAGs for 2, 3, 4, and 10 variables are 3, 25, 543, and 4175098976430598143, respectively.\n",
    "Therefore, it is evident that computing an exact solution that maximize the score $S(\\mathcal{D},\\mathcal{G})$ by searching over all graphs is often infeasible. \n",
    "\n",
    "Instead, greedy search algorithms can be applied.\n",
    "\n",
    "A greedy algorithm is an approach for solving a problem by selecting the best option available at the moment. \n",
    "\n",
    "- greedy algorithms doesn't worry whether the current best result will bring the globally optimal result. The algorithm never reverses the earlier decision, even if the choice is wrong. \n",
    "\n",
    "- Usually, greedy algorithms are easier to describe and can perform quite well than other algorithms. However, greedy searches cannot guarantee an optimal solution. \n",
    "\n",
    "- For most problems, the space of possible DAGs is so big that finding a true optimal solution is challenging.\n",
    "\n",
    "- The **Greedy Equivalence Search (GES)** algorithm uses this trick. GES starts with an empty graph and iteratively adds directed edges such that the improvement in a model fitness measure (i.e., score) is maximized.\n",
    "\n",
    "\n",
    "\n",
    "### C. Other tricks\n",
    "\n",
    "Often, some additional tricks are used.  \n",
    "A fundamental property of causality is **asymmetry**. $A$ could cause $B$, but $B$ may not cause $A$. Thus, some algorithms leverage this idea to select between causal model candidates concerning time.\n",
    "Time asymmetry is quite natural since causes happen before effects. This is used in the **Granger causality test** too. Granger causality test says that a variable $X$ that evolves over time causes another evolving variable $Y$ if predictions of the value of $Y$ based on its own past values and based on the past values of $X$ are better than predictions of $Y$ based only on $ Y$'s own past values.\n",
    "\n",
    "\n",
    "**Neural networks** have been used to explore causal relationships. Following are two examples:\n",
    "\n",
    "- Causal Generative Neural Networks (CGNN), where the algorithm learns functional causal models from observational data based on generative neural networks. [CGNN](https://arxiv.org/pdf/1711.08936.pdf)\n",
    "\n",
    "- Casual Recurrent Neural Networks (CRNN), where we developed a framework to explore causal structure in a multivariate time-series problem. [CRNN](https://ieeexplore.ieee.org/abstract/document/8437162)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A broad overview of different causal structure search methods is available at:\n",
    "[Review of Causal Discovery Methods Based on Graphical Models](https://www.frontiersin.org/articles/10.3389/fgene.2019.00524/full#:~:text=Causal%20discovery%20aims%20to%20find,process%20or%20the%20sampling%20process)\n",
    "\n",
    "We also recommend you to check Chapter 4 and 7, Learning Cause-Effect Models from the [Elements of Causal Inference](https://mitpress.mit.edu/books/elements-causal-inference) book.\n",
    "\n",
    "Finally, **structure learning is an active field of research.** These algorithms don't guarantee to retrieve the true causal relationship for all the variables. However, they can help better understand the problems and some relationships between variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example for Causal Structure Learning : Shortness of breath disease\n",
    "\n",
    "We will make use of the **bnlearn** library, which is built on top of the extensive *pgmpy* library. *pgmpy* is a python implementation for Bayesian Networks with various algorithms for Structure Learning, Parameter Estimation, Approximate (Sampling Based), and Causal Inference.\n",
    "\n",
    "In this example we will try to analyse patients treatment regarding shortness-of-breath (dyspnoea). The dataset has few variables and is siumulated by [Lauritzen and Spiegelhalter,1988](https://www.jstor.org/stable/2345762?seq=1). The data is about relationship between lung diseases (tuberculosis, lung cancer or bronchitis) and visits to infection areas for 20000 patintes.\n",
    "\n",
    "**Background:** Shortness-of-breath (dyspnoea) may be due to **tuberculosis, lung cancer, bronchitis**, or none of them, or more than one of them. A recent visit to infectious areas increases the chances of tuberculosis, while smoking is known to be a risk factor for both lung cancer and bronchitis. The results of a single chest X-ray do not discriminate between lung cancer and tuberculosis, as neither does the presence or absence of dyspnoea.\n",
    "\n",
    "![img](img/ch6/dyspnoea.jpeg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tub_area</th>\n",
       "      <th>tub</th>\n",
       "      <th>smoke</th>\n",
       "      <th>lung</th>\n",
       "      <th>bronc</th>\n",
       "      <th>either</th>\n",
       "      <th>xray</th>\n",
       "      <th>dysp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tub_area  tub  smoke  lung  bronc  either  xray  dysp\n",
       "0             1    1      1     1      1       1     1     1\n",
       "1             1    1      0     0      1       0     0     0\n",
       "2             1    1      1     1      1       1     1     1\n",
       "3             1    1      1     1      1       1     1     1\n",
       "4             1    1      1     1      1       1     1     1\n",
       "...         ...  ...    ...   ...    ...     ...   ...   ...\n",
       "19995         1    1      0     1      1       1     1     1\n",
       "19996         1    1      1     1      1       1     0     1\n",
       "19997         1    1      0     0      0       0     0     0\n",
       "19998         1    1      0     1      1       1     1     1\n",
       "19999         1    1      0     1      1       1     1     1\n",
       "\n",
       "[20000 rows x 8 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "\n",
    "import bnlearn as bn\n",
    "import pickle5\n",
    "with open(\"data/smoke_dataset.pkl\", \"rb\") as fh:\n",
    "  df = pickle5.load(fh)\n",
    "\n",
    "#df = bn.load(filepath='smoke_dataset_2.pkl')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a causal model when we have data and domain knowledge\n",
    "\n",
    "As we saw in the lectures, expert knowledge can be included in causal models by using graphs in the form of a Directed Acyclic Graphs. Let's assume that our knowledge about dyspnoea is limited to: smoking is related to lung cancer, smoking is related to bronchitis, and if you have lung or bronchitus we may need an xray examination. \n",
    "\n",
    "Therefore, we create a DAG based on this knowledge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bnlearn] >bayes DAG created.\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.50.0 (0)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"142pt\" height=\"188pt\"\n",
       " viewBox=\"0.00 0.00 141.55 188.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 184)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-184 137.55,-184 137.55,4 -4,4\"/>\n",
       "<!-- smoke -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>smoke</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"65\" cy=\"-162\" rx=\"34.39\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"65\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">smoke</text>\n",
       "</g>\n",
       "<!-- lung -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>lung</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">lung</text>\n",
       "</g>\n",
       "<!-- smoke&#45;&gt;lung -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>smoke&#45;&gt;lung</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M55.99,-144.41C51.45,-136.04 45.84,-125.71 40.77,-116.37\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"43.79,-114.59 35.94,-107.47 37.64,-117.93 43.79,-114.59\"/>\n",
       "</g>\n",
       "<!-- bronc -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>bronc</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"103\" cy=\"-90\" rx=\"30.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"103\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">bronc</text>\n",
       "</g>\n",
       "<!-- smoke&#45;&gt;bronc -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>smoke&#45;&gt;bronc</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M74.01,-144.41C78.55,-136.04 84.16,-125.71 89.23,-116.37\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"92.36,-117.93 94.06,-107.47 86.21,-114.59 92.36,-117.93\"/>\n",
       "</g>\n",
       "<!-- xray -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>xray</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"65\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"65\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">xray</text>\n",
       "</g>\n",
       "<!-- lung&#45;&gt;xray -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>lung&#45;&gt;xray</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M35.81,-72.76C40.42,-64.28 46.16,-53.71 51.32,-44.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54.54,-45.61 56.23,-35.15 48.39,-42.27 54.54,-45.61\"/>\n",
       "</g>\n",
       "<!-- bronc&#45;&gt;xray -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>bronc&#45;&gt;xray</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M94.19,-72.76C89.58,-64.28 83.84,-53.71 78.68,-44.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"81.61,-42.27 73.77,-35.15 75.46,-45.61 81.61,-42.27\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f8817e1f5d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz as gr\n",
    "\n",
    "edges = [('smoke', 'lung'),\n",
    "         ('smoke', 'bronc'),\n",
    "         ('lung', 'xray'),\n",
    "         ('bronc', 'xray')]\n",
    "\n",
    "# Create the DAG from the edges\n",
    "DAG = bn.make_DAG(edges)\n",
    "\n",
    "# Plot and make sure the arrows are correct.\n",
    "def plot_from_edges(edges):\n",
    "    # plot\n",
    "    g = gr.Digraph()\n",
    "    \n",
    "    for i in range(0, len(edges)):\n",
    "        g.edge(*edges[i])\n",
    "    return g\n",
    "\n",
    "g = plot_from_edges(edges)\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "At this point we have the data set in our dataframe (df), and we also have the DAG based on our expert knowledge. \n",
    "We can use parameter learning to learn conditional probability distributions (CPDs) of variales in our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bnlearn] >Removing columns from dataframe to make consistent with DAG [['tub_area' 'tub' 'either' 'dysp']]\n",
      "[bnlearn] >Parameter learning> Computing parameters using [bayes]\n",
      "[bnlearn] >CPD of smoke:\n",
      "+----------+----------+\n",
      "| smoke(0) | 0.495333 |\n",
      "+----------+----------+\n",
      "| smoke(1) | 0.504667 |\n",
      "+----------+----------+\n",
      "[bnlearn] >CPD of lung:\n",
      "+---------+---------------------+---------------------+\n",
      "| smoke   | smoke(0)            | smoke(1)            |\n",
      "+---------+---------------------+---------------------+\n",
      "| lung(0) | 0.11449721207460103 | 0.03302509907529723 |\n",
      "+---------+---------------------+---------------------+\n",
      "| lung(1) | 0.8855027879253989  | 0.9669749009247027  |\n",
      "+---------+---------------------+---------------------+\n",
      "[bnlearn] >CPD of bronc:\n",
      "+----------+--------------------+--------------------+\n",
      "| smoke    | smoke(0)           | smoke(1)           |\n",
      "+----------+--------------------+--------------------+\n",
      "| bronc(0) | 0.5919054028071524 | 0.3110020758633704 |\n",
      "+----------+--------------------+--------------------+\n",
      "| bronc(1) | 0.4080945971928475 | 0.6889979241366295 |\n",
      "+----------+--------------------+--------------------+\n",
      "[bnlearn] >CPD of xray:\n",
      "+---------+---------------------+-----+---------------------+\n",
      "| bronc   | bronc(0)            | ... | bronc(1)            |\n",
      "+---------+---------------------+-----+---------------------+\n",
      "| lung    | lung(0)             | ... | lung(1)             |\n",
      "+---------+---------------------+-----+---------------------+\n",
      "| xray(0) | 0.8385598141695703  | ... | 0.06883224440968068 |\n",
      "+---------+---------------------+-----+---------------------+\n",
      "| xray(1) | 0.16144018583042974 | ... | 0.9311677555903193  |\n",
      "+---------+---------------------+-----+---------------------+\n"
     ]
    }
   ],
   "source": [
    "# Learn the parameters from data set.\n",
    "# As input we have the DAG without CPDs.\n",
    "DAG = bn.parameter_learning.fit(DAG, df, methodtype='bayes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPD of smoke:\n",
      "+----------+----------+\n",
      "| smoke(0) | 0.495333 |\n",
      "+----------+----------+\n",
      "| smoke(1) | 0.504667 |\n",
      "+----------+----------+\n",
      "CPD of lung:\n",
      "+---------+---------------------+---------------------+\n",
      "| smoke   | smoke(0)            | smoke(1)            |\n",
      "+---------+---------------------+---------------------+\n",
      "| lung(0) | 0.11449721207460103 | 0.03302509907529723 |\n",
      "+---------+---------------------+---------------------+\n",
      "| lung(1) | 0.8855027879253989  | 0.9669749009247027  |\n",
      "+---------+---------------------+---------------------+\n",
      "CPD of bronc:\n",
      "+----------+--------------------+--------------------+\n",
      "| smoke    | smoke(0)           | smoke(1)           |\n",
      "+----------+--------------------+--------------------+\n",
      "| bronc(0) | 0.5919054028071524 | 0.3110020758633704 |\n",
      "+----------+--------------------+--------------------+\n",
      "| bronc(1) | 0.4080945971928475 | 0.6889979241366295 |\n",
      "+----------+--------------------+--------------------+\n",
      "CPD of xray:\n",
      "+---------+---------------------+-----+---------------------+\n",
      "| bronc   | bronc(0)            | ... | bronc(1)            |\n",
      "+---------+---------------------+-----+---------------------+\n",
      "| lung    | lung(0)             | ... | lung(1)             |\n",
      "+---------+---------------------+-----+---------------------+\n",
      "| xray(0) | 0.8385598141695703  | ... | 0.06883224440968068 |\n",
      "+---------+---------------------+-----+---------------------+\n",
      "| xray(1) | 0.16144018583042974 | ... | 0.9311677555903193  |\n",
      "+---------+---------------------+-----+---------------------+\n",
      "[bnlearn] >Independencies:\n",
      "(lung ⟂ bronc | smoke)\n",
      "(smoke ⟂ xray | lung, bronc)\n",
      "(xray ⟂ smoke | lung, bronc)\n",
      "(bronc ⟂ lung | smoke)\n",
      "[bnlearn] >Nodes: ['smoke', 'lung', 'bronc', 'xray']\n",
      "[bnlearn] >Edges: [('smoke', 'lung'), ('smoke', 'bronc'), ('lung', 'xray'), ('bronc', 'xray')]\n"
     ]
    }
   ],
   "source": [
    "# Print the CPDs\n",
    "bn.print_CPD(DAG)\n",
    "# At this point we have a DAG with the learned CPDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can combined our expert knowledge with a data set! Then we can make inferences which allows us to ask causal questions from the model. Let us demonstrate a few questions...\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Question 1:**\n",
    "What is the probability of lung-cancer, given that we know that patient does smoke?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bnlearn] >Variable Elimination..\n",
      "+----+--------+-----------+\n",
      "|    |   lung |         p |\n",
      "+====+========+===========+\n",
      "|  0 |      0 | 0.0330251 |\n",
      "+----+--------+-----------+\n",
      "|  1 |      1 | 0.966975  |\n",
      "+----+--------+-----------+\n"
     ]
    }
   ],
   "source": [
    "q1 = bn.inference.fit(DAG, variables=['lung'], evidence={'smoke':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "**Question 2:**\n",
    "What is the probability of bronchitis, given that we know patient does smoke?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bnlearn] >Variable Elimination..\n",
      "+----+---------+----------+\n",
      "|    |   bronc |        p |\n",
      "+====+=========+==========+\n",
      "|  0 |       0 | 0.311002 |\n",
      "+----+---------+----------+\n",
      "|  1 |       1 | 0.688998 |\n",
      "+----+---------+----------+\n"
     ]
    }
   ],
   "source": [
    "q2 = bn.inference.fit(DAG, variables=['bronc'], evidence={'smoke':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "**Question 3:** \n",
    "What is the probability of lung-cancer, given that we know that patient does smoke and also has bronchitis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bnlearn] >Variable Elimination..\n",
      "+----+--------+-----------+\n",
      "|    |   lung |         p |\n",
      "+====+========+===========+\n",
      "|  0 |      0 | 0.0330251 |\n",
      "+----+--------+-----------+\n",
      "|  1 |      1 | 0.966975  |\n",
      "+----+--------+-----------+\n"
     ]
    }
   ],
   "source": [
    "q3 = bn.inference.fit(DAG, variables=['lung'], evidence={'smoke':1, 'bronc':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "**Question 4:**\n",
    "Lets specify the question even more. What is the probability of lung-cancer or bronchitis, given that we know that patient does smoke but did not had xray?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bnlearn] >Variable Elimination..\n",
      "+----+---------+--------+-----------+\n",
      "|    |   bronc |   lung |         p |\n",
      "+====+=========+========+===========+\n",
      "|  0 |       0 |      0 | 0.0915345 |\n",
      "+----+---------+--------+-----------+\n",
      "|  1 |       0 |      1 | 0.226912  |\n",
      "+----+---------+--------+-----------+\n",
      "|  2 |       1 |      0 | 0.194173  |\n",
      "+----+---------+--------+-----------+\n",
      "|  3 |       1 |      1 | 0.487381  |\n",
      "+----+---------+--------+-----------+\n"
     ]
    }
   ],
   "source": [
    "q4 = bn.inference.fit(DAG, variables=['bronc','lung'], evidence={'smoke':1, 'xray':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a causal model when we have data and no domain knowledge\n",
    "\n",
    "Suppose that we have the medical records of hundreds or even thousands patients treatment regarding shortness-of-breath (dyspnoea). Our goal is to determine the causality across variables given the data set. We dont have a prior knowledge. e.g. we are a dta scientist whom just start to work with the dataset.\n",
    "\n",
    "We use structure learning to estimate the DAG structure of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bnlearn] >Computing best DAG using [cs]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c069a1dd0247e8b6bdfdb489592651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c18f5c868cb144929507bda2a0d355a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Structure learning on the data set\n",
    "model_estimated = bn.structure_learning.fit(df, methodtype='cs', scoretype='bic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the learned DAG and examine the structure!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.50.0 (0)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"148pt\" height=\"188pt\"\n",
       " viewBox=\"0.00 0.00 148.45 188.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 184)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-184 144.45,-184 144.45,4 -4,4\"/>\n",
       "<!-- lung -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>lung</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"73.45\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"73.45\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">lung</text>\n",
       "</g>\n",
       "<!-- smoke -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>smoke</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"34.45\" cy=\"-90\" rx=\"34.39\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"34.45\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">smoke</text>\n",
       "</g>\n",
       "<!-- lung&#45;&gt;smoke -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>lung&#45;&gt;smoke</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M64.4,-144.76C59.74,-136.4 53.96,-126.02 48.71,-116.61\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"51.64,-114.67 43.71,-107.63 45.52,-118.07 51.64,-114.67\"/>\n",
       "</g>\n",
       "<!-- xray -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>xray</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"113.45\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"113.45\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">xray</text>\n",
       "</g>\n",
       "<!-- lung&#45;&gt;xray -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>lung&#45;&gt;xray</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M82.72,-144.76C87.62,-136.19 93.74,-125.49 99.22,-115.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"102.3,-117.57 104.22,-107.15 96.22,-114.09 102.3,-117.57\"/>\n",
       "</g>\n",
       "<!-- bronc -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>bronc</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"34.45\" cy=\"-18\" rx=\"30.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"34.45\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">bronc</text>\n",
       "</g>\n",
       "<!-- smoke&#45;&gt;bronc -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>smoke&#45;&gt;bronc</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M34.45,-71.7C34.45,-63.98 34.45,-54.71 34.45,-46.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"37.95,-46.1 34.45,-36.1 30.95,-46.1 37.95,-46.1\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f8810af0a10>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_from_model(model):\n",
    "    # plot\n",
    "    g = gr.Digraph()\n",
    "    \n",
    "    for i in range(0, len(model_estimated['model_edges'])):\n",
    "        g.edge(*model_estimated['model_edges'][i])\n",
    "    return g\n",
    "\n",
    "# Plot the DAG\n",
    "plot_from_model(model_estimated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "For this chapter, we used an exmaple from *bnlearn - Library for Bayesian network learning and inference*. The e-book has varius exmaples with nice visualization. Here is the ptython library [bnlearn](https://pypi.org/project/bnlearn/)\n",
    "\n",
    "The Python bnlearn ieself is inspired by the [bnlearn - an R package for Bayesian network learning and inference](https://www.bnlearn.com) book an amazing work by Marco Scutari, IDSIA.\n",
    "\n",
    "For causal struture search methods, we recommned this paper: [Review of Causal Discovery Methods Based on Graphical Models](https://www.frontiersin.org/articles/10.3389/fgene.2019.00524/full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f82454df3ab4669350e470cecfad51160e6fdff8e76eafd19d8880dd92d922a3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
