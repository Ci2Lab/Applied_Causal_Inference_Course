{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Causal Structure Learning from Data\n",
    "\n",
    "In the previous chapter, we build a causal model knowing the equation structure of the problem from our domain knowledge. However, we do not have the privilege of knowing causal structure in the real world.\n",
    "In this chapter, our goal is to create a causal model without knowing the causal structure of the problem. In other words, our problem is **causal structure identification or discovery**.\n",
    "\n",
    "- **With structure learning, we want to determine the structure of the graph that best captures the causal dependencies between the variables in a data set.** \n",
    "- In other words, given a dataset, we derive a causal model that describes it.\n",
    "\n",
    "\n",
    "## How can we estimate the causal structure from a dataset?\n",
    "\n",
    "Unfortunately, there is no standard recipe for that, and that's why the causal inference is generally challenging. Causal discovery is an example of an inverse problem. \n",
    "\n",
    "The usual approach to solving inverse problems is to **make assumptions** about what we are trying to investigate. This narrows down the possible solutions and hopefully makes the problem solvable. \n",
    "\n",
    "There are four common assumptions made across causal discovery algorithms. \n",
    "\n",
    "- **Acyclicity** — causal structure can be represented by a DAG $G$ (mentioned in [Chapter 3](/lectures/CH-3-Graphical-Causal-Models.ipynb))\n",
    "- **Markov Property** — all nodes are independent of their non-descendants when conditioned on their parents (mentioned in [Chapter 3](/lectures/CH-3-Graphical-Causal-Models.ipynb))\n",
    "- **Faithfulness** — all conditional independences in true underlying distribution $p$ are represented in $G$ \n",
    "- **Sufficiency** — any pair of nodes in $G$ has no common external cause\n",
    "\n",
    "\n",
    "A comprehensive discussion of these causal assumptions is availabe  in [Kalainathan et al., 2018]( https://arxiv.org/abs/1803.04929)\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "Although the mentioned assumptions help narrow down the number of possible causal models, they do not guarantee to achieve a causal model. This is where a few tricks for causal discovery are helpful. A list of tricks for causal discovery is given in the table below.\n",
    "\n",
    "\n",
    "| **TRICK**                             | **ALGORITHM**                                               |\n",
    "|-----------------------------------|---------------------------------------------------------|\n",
    "| **Conditional Independence Testing**  | PC <br> Fast Causal Inference (FCI) <br>  Inductive Causation (IC) |\n",
    "| **Greedy Search on DAG Space**        | Gready Equivalent Search (GES) <br>  Gready Interventional Equivalent Search (GIES) <br> Concave penalized Coordinate Descent with reparametrization (CCDr)                                                        |\n",
    "| **Exploiting Asymmetry**              | Linear Non-Gaussian Acyclic Model (LINGAM) <br>  Nonlinear Additive Noise Models <br> Post_nonlinear Causal Model (PNL) <br> Granger Causality                                                      |\n",
    "| **Hybrid**                            | Structural Agnostic Modeling (SAM) <br> Causal Additive Modeling (CAM) <br> Causal Generative Causal Neural Network (CGNN)                                                       |\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "A broad overview of different causal structure search methods is available at:\n",
    "[Review of Causal Discovery Methods Based on Graphical Models](https://www.frontiersin.org/articles/10.3389/fgene.2019.00524/full#:~:text=Causal%20discovery%20aims%20to%20find,process%20or%20the%20sampling%20process)\n",
    "\n",
    "We also recommend you to check Chapter 4, Learning Cause-Effect Models from the [Elements of Causal Inference](https://mitpress.mit.edu/books/elements-causal-inference) book.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example for Causal Structure Learning : Shortness of breath disease\n",
    "\n",
    "We will make use of the **bnlearn** library, which is built on top of the extensive *pgmpy* library. *pgmpy* is a python implementation for Bayesian Networks with various algorithms for Structure Learning, Parameter Estimation, Approximate (Sampling Based), and Causal Inference.\n",
    "\n",
    "In this example we will try to analyse patients treatment regarding shortness-of-breath (dyspnoea). The dataset has few variables and is siumulated by [Lauritzen and Spiegelhalter,1988](https://www.jstor.org/stable/2345762?seq=1). The data is about relationship between lung diseases (tuberculosis, lung cancer or bronchitis) and visits to infection areas for 20000 patintes.\n",
    "\n",
    "**Background:** Shortness-of-breath (dyspnoea) may be due to **tuberculosis, lung cancer, bronchitis**, or none of them, or more than one of them. A recent visit to infectious areas increases the chances of tuberculosis, while smoking is known to be a risk factor for both lung cancer and bronchitis. The results of a single chest X-ray do not discriminate between lung cancer and tuberculosis, as neither does the presence or absence of dyspnoea.\n",
    "\n",
    "![img](img/ch6/dyspnoea.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trick 1: Conditional Independence Testing\n",
    "\n",
    "One of these earliest causal discovery algorithms is the PC algorithm, named after Peter Spirtes and Clark Glymour. This algorithm uses the idea that two statistically independent variables are not causally linked. An outline of the PC algorithm is illustrated in the figure below (image taken from https://towardsdatascience.com/causal-discovery-6858f9af6dcb). \n",
    "\n",
    "![img](img/ch6/Trick1.png)\n",
    "\n",
    "\n",
    "**Step 1:** form a fully connected, undirected graph using every variable in the dataset. \n",
    "\n",
    "**Step 2:**  edges are deleted if the corresponding variables are independent. \n",
    "\n",
    "**Step 3:**  connected edges undergo conditional independence testing, e.g., independence test of bottom and far-right node conditioned on the middle node (see step 2). If conditioning on a variable kills the dependence, that variable is added to the Separation set for those two variables. Depending on the size of the graph, conditional independence testing will continue (i.e. condition on more variables) until there are no more candidates for testing.\n",
    "\n",
    "**Step 4:**  colliders (i.e. $X \\rightarrow Y \\leftarrow Z$) are oriented based on the Separation set of node pairs. \n",
    "\n",
    "**Step 5:**  remaining edges are directed based on two constraints, 1) no new v-structures and 2) no directed cycles can be formed.\n",
    "\n",
    "\n",
    "### Trick 2: Greedy Search of Graph Space\n",
    "\n",
    "A greedy algorithm is an approach for solving a problem by selecting the best option available at the moment. \n",
    "\n",
    "- greedy algorithms doesn't worry whether the current best result will bring the globally optimal result. The algorithm never reverses the earlier decision, even if the choice is wrong. \n",
    "\n",
    "- Usually, greedy algorithms are easier to describe and can perform quite well than other algorithms. However, greedy searches cannot guarantee an optimal solution. \n",
    "\n",
    "- For most problems, the space of possible DAGs is so big that finding a true optimal solution is challenging.\n",
    "\n",
    "- The **Greedy Equivalence Search (GES)** algorithm uses this trick. GES starts with an empty graph and iteratively adds directed edges such that the improvement in a model fitness measure (i.e., score) is maximized.\n",
    "\n",
    "\n",
    "### Trick 3: Exploiting Asymmetries in Cause-Effect Relations\n",
    "\n",
    "A fundamental property of causality is **asymmetry**. $A$ could cause $B$, but $B$ may not cause $A$. Thus, some algorithms leverage this idea to select between causal model candidates concerning time, complexity, and functions.\n",
    "\n",
    "- Time asymmetry is quite natural since causes happen before effects. This is used in the **Granger causality test** too. Granger causality test says that a variable $X$ that evolves over time causes another evolving variable $Y$ if predictions of the value of $Y$ based on its own past values and based on the past values of $X$ are better than predictions of $Y$ based only on $ Y$'s own past values.\n",
    "\n",
    "- Complexity asymmetry follows **Occam's razor principle**, that simpler models are better. In other words, if you have a handful of candidate models to choose from, this idea says to pick the simplest one. One way of quantifying simplicity (or complexity) is the **Kolmogorov Complexity** theory.\n",
    "\n",
    "- Functional asymmetry assumes models that better fit a relationship are better candidates. For example, given two variables $X$ and $Y$, the nonlinear additive noise model (NANM) performs a nonlinear regression between $X$ and $Y$ , e.g. $y = f(x) + n$, where $n$ = noise/residual, in both directions. The model (of causation) is then accepted if the potential cause (e.g. $x$) is independent of the noise term (e.g. $n$).\n",
    "\n",
    "### Trick 4: Hybrid Approaches\n",
    "\n",
    "The last trick includes algorithms that differ from other tricks and exploit different assumptions. e.g., neural networks have been used to explore causal relationships. Following are two examples:\n",
    "\n",
    "- Causal Generative Neural Networks (CGNN), where the algorithm learns functional causal models from observational data based on generative neural networks. [CGNN](https://arxiv.org/pdf/1711.08936.pdf)\n",
    "\n",
    "- Casual Recurrent Neural Networks (CRNN), where we developed a framework to explore causal structure in a multivariate time-series problem. [CRNN](https://ieeexplore.ieee.org/abstract/document/8437162)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bnlearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8325cd728d52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mbnlearn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data/smoke_dataset.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bnlearn'"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "import bnlearn as bn\n",
    "df = bn.load(filepath='data/smoke_dataset.pkl')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a causal model when we have data and domain knowledge\n",
    "\n",
    "As we saw in the lectures, expert knowledge can be included in causal models by using graphs in the form of a Directed Acyclic Graphs. Let's assume that our knowledge about dyspnoea is limited to: smoking is related to lung cancer, smoking is related to bronchitis, and if you have lung or bronchitus we may need an xray examination. \n",
    "\n",
    "Therefore, we create a DAG based on this knowledge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3c22c9011437>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Create the DAG from the edges\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mDAG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_DAG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Plot and make sure the arrows are correct.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bn' is not defined"
     ]
    }
   ],
   "source": [
    "edges = [('smoke', 'lung'),\n",
    "         ('smoke', 'bronc'),\n",
    "         ('lung', 'xray'),\n",
    "         ('bronc', 'xray')]\n",
    "\n",
    "# Create the DAG from the edges\n",
    "DAG = bn.make_DAG(edges)\n",
    "\n",
    "# Plot and make sure the arrows are correct.\n",
    "bn.plot(DAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "At this point we have the data set in our dataframe (df), and we also have the DAG based on our expert knowledge. \n",
    "We can use parameter learning to learn conditional probability distributions (CPDs) of variales in our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bnlearn] >Variable Elimination..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding Elimination Order: : : 0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----------+\n",
      "|    |   lung |         p |\n",
      "+====+========+===========+\n",
      "|  0 |      0 | 0.0330251 |\n",
      "+----+--------+-----------+\n",
      "|  1 |      1 | 0.966975  |\n",
      "+----+--------+-----------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "q1 = bn.inference.fit(DAG, variables=['lung'], evidence={'smoke':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-db02963e8fc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Check the current CPDs in the DAG.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_CPD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDAG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# [bnlearn] >No CPDs to print. Tip: use bn.plot(DAG) to make a plot.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# This is correct, we dit not yet specify any CPD.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bn' is not defined"
     ]
    }
   ],
   "source": [
    "# Check the current CPDs in the DAG.\n",
    "bn.print_CPD(DAG)\n",
    "# [bnlearn] >No CPDs to print. Tip: use bn.plot(DAG) to make a plot.\n",
    "# This is correct, we dit not yet specify any CPD.\n",
    "\n",
    "# Learn the parameters from data set.\n",
    "# As input we have the DAG without CPDs.\n",
    "DAG = bn.parameter_learning.fit(DAG, df, methodtype='bayes')\n",
    "\n",
    "# Print the CPDs\n",
    "bn.print_CPD(DAG)\n",
    "# At this point we have a DAG with the learned CPDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can combined our expert knowledge with a data set! Then we can make inferences which allows us to ask causal questions from the model. Let us demonstrate a few questions...\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "**Question 1:**\n",
    "What is the probability of lung-cancer, given that we know that patient does smoke?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "**Question 2:**\n",
    "What is the probability of bronchitis, given that we know patient does smoke?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bnlearn] >Variable Elimination..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding Elimination Order: : : 0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+----------+\n",
      "|    |   bronc |        p |\n",
      "+====+=========+==========+\n",
      "|  0 |       0 | 0.311002 |\n",
      "+----+---------+----------+\n",
      "|  1 |       1 | 0.688998 |\n",
      "+----+---------+----------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "q2 = bn.inference.fit(DAG, variables=['bronc'], evidence={'smoke':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "**Question 3:** \n",
    "What is the probability of lung-cancer, given that we know that patient does smoke and also has bronchitis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bnlearn] >Variable Elimination..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding Elimination Order: : : 0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----------+\n",
      "|    |   lung |         p |\n",
      "+====+========+===========+\n",
      "|  0 |      0 | 0.0330251 |\n",
      "+----+--------+-----------+\n",
      "|  1 |      1 | 0.966975  |\n",
      "+----+--------+-----------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "q3 = bn.inference.fit(DAG, variables=['lung'], evidence={'smoke':1, 'bronc':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "**Question 4:**\n",
    "Lets specify the question even more. What is the probability of lung-cancer or bronchitis, given that we know that patient does smoke but did not had xray?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bnlearn] >Variable Elimination..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding Elimination Order: : : 0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+--------+-----------+\n",
      "|    |   bronc |   lung |         p |\n",
      "+====+=========+========+===========+\n",
      "|  0 |       0 |      0 | 0.0915345 |\n",
      "+----+---------+--------+-----------+\n",
      "|  1 |       0 |      1 | 0.226912  |\n",
      "+----+---------+--------+-----------+\n",
      "|  2 |       1 |      0 | 0.194173  |\n",
      "+----+---------+--------+-----------+\n",
      "|  3 |       1 |      1 | 0.487381  |\n",
      "+----+---------+--------+-----------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "q4 = bn.inference.fit(DAG, variables=['bronc','lung'], evidence={'smoke':1, 'xray':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a causal model when we have data and no domain knowledge\n",
    "\n",
    "Suppose that we have the medical records of hundreds or even thousands patients treatment regarding shortness-of-breath (dyspnoea). Our goal is to determine the causality across variables given the data set. We dont have a prior knowledge. e.g. we are a dta scientist whom just start to work with the dataset.\n",
    "\n",
    "We use structure learning to estimate the DAG structure of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f5f9c0f807b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data/smoke_dataset.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# Structure learning on the data set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructure_learning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethodtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoretype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bn' is not defined"
     ]
    }
   ],
   "source": [
    "df = bn.load(filepath='data/smoke_dataset.pkl')\n",
    "# Structure learning on the data set\n",
    "model = bn.structure_learning.fit(df, methodtype='hc', scoretype='bic')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the learned DAG and examine the structure!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f11323474cb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructure_learning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethodtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoretype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Compute edge strength with the chi_square test statistic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# model = bn.independence_test(model, df, test='chi_square')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bn' is not defined"
     ]
    }
   ],
   "source": [
    "model = bn.structure_learning.fit(df, methodtype='cs', scoretype='bic')\n",
    "\n",
    "# Compute edge strength with the chi_square test statistic\n",
    "# model = bn.independence_test(model, df, test='chi_square')\n",
    "\n",
    "# Plot the DAG\n",
    "bn.plot(model, interactive=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "For this chapter, we used an exmaple from *bnlearn - Library for Bayesian network learning and inference*. The e-book has varius exmaples with nice visualization. Here is the ptython library [bnlearn](https://pypi.org/project/bnlearn/)\n",
    "\n",
    "The Python bnlearn ieself is inspired by the [bnlearn - an R package for Bayesian network learning and inference](https://www.bnlearn.com) book an amazing work by Marco Scutari, IDSIA.\n",
    "\n",
    "For causal struture search methods, we recommned this paper: [Review of Causal Discovery Methods Based on Graphical Models](https://www.frontiersin.org/articles/10.3389/fgene.2019.00524/full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f82454df3ab4669350e470cecfad51160e6fdff8e76eafd19d8880dd92d922a3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
