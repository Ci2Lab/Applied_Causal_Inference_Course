{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "fa524251-c07c-44c7-89a2-0fbaa27f6617",
    "deepnote_cell_height": 6039.234375,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# Chapter 1 - Introduction to Causality\n",
    "\n",
    "\n",
    "## Anything wrong with data science?\n",
    "\n",
    "Let's start with a simple image (Courtesy of Markus Elsholz). \n",
    "What is the object in the image below? A chair, right?\n",
    "\n",
    "![img](img/ch1/Beuchet_chair_a.png)\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "What if we look at the object from a different angle? \n",
    "\n",
    "![img](img/ch1/Beuchet_chair_b.png)\n",
    "\n",
    "\n",
    "The object is not a chair! We just had an illusion of a chair if the parts are viewed from a single and specific angle. This is the **Beuchet chair experiment** on changing perception in observations.\n",
    "\n",
    "Do you have any other examples?\n",
    "\n",
    "<br/><br/>\n",
    "Unfortunately, most of our works in statistics, data science, and machine learning are based on **observations**! However, we can not just rely on observations to model and understand our world. \n",
    "\n",
    "![img](img/ch1/Math_Learning.jpeg)     \n",
    "\n",
    "\n",
    "Saying that, let's see what Causality is or is NOT.\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "## What is NOT Causality?\n",
    "\n",
    "<br/><br/>\n",
    "### Causality is not Algebra\n",
    "\n",
    "Dario is a little boy that feels fever, so his Mom measures his temperature with a thermometer. Then, hopefully, he can skip school today. \n",
    "From an algebra point of view, the height of mercury $X$ in the pipe is related to Dario's body temperature $Y$ with a constant $k$. \n",
    "\n",
    "$Y = k * X$\n",
    "\n",
    "<img src=\"img/ch1/little-sick-boy.jpeg\" width=\"250\"/>\n",
    "\n",
    "For our algebra equation, it does not matter if Dario's body temperature increases the mercury column height or the other way.\n",
    "\n",
    "<br/><br/>\n",
    "### Causality is not Statistics\n",
    "\n",
    "- Most scientific inquiry/data analyses have one of the two goals:\n",
    "\n",
    "    - **Association/prediction**, i.e., determine predictors or variables associated with the outcome of interest.\n",
    "    - **Causality**, i.e., understanding factors that cause or influence the outcome of interest.\n",
    "\n",
    "- Statistical concepts are those expressible in terms of the joint distribution of observed variables.\n",
    "\n",
    "- We are often told that association is not causation. However, we forget about it. Therefore, we see numerous spurious/funny correlations like examples in the [Spurious Correlations collection](https://tylervigen.com/spurious-correlations). \n",
    "\n",
    "![img](img/ch1/Spurious_Correlations_Muzzarella.png)     \n",
    "\n",
    "<br/><br/>\n",
    "Another example is related to cigarette commercials in the USA in the 50th that claim smoking is helpful for coughs treatment and even helps you have a more fit body!\n",
    "\n",
    "![img](img/ch1/Cigarette_Commercials.png)  \n",
    "\n",
    "<br/><br/>\n",
    "### Causality is not Machine Learning\n",
    "\n",
    "We hear about rapid advances in machine learning systems every day, such as deep-learning algorithms in self-driving cars, speech-recognition systems, image processing, and virtual reality. Nevertheless, deep learning has succeeded primarily by performing repeatable tasks to answer specific questions that we thought were difficult. But, those questions are not that difficult. \n",
    "\n",
    "<br/><br/>\n",
    "Machine learning has not addressed the tough questions that prevent us from achieving human-level AI. The public believes that AI machines can think like humans. In reality, computers don't even have animal-like cognitive abilities yet. See [Adnan Darwiche's paper, Human-Level Intelligence or Animal-Like Abilities?](https://arxiv.org/abs/1707.04327). \n",
    "\n",
    "<br/><br/>\n",
    "The field of artificial intelligence is \"bursting with micro discoveries\"—the sort of things that make good press releases—but machines are still disappointingly far from human-like cognition. See [Gary Marcus's book, Rebooting AI: Building Machines We Can Trust](http://garymarcus.com/index.html)\n",
    "\n",
    "![img](img/ch1/ML_Animal_Abilities.png)  \n",
    "\n",
    "<br/><br/>\n",
    "Machine learning systems usually operate in complex environments governed by rich webs of causal relations while having only access to surface representations of those causal relations through observation and measurements. Medicine, economics, education, climatology, and politics are typical examples of such environments. In other words, machine-learning methods today provide us with an efficient way of going from finite sample estimates to probability distributions. However, we still need to move from distributions to cause-effect relations in the real world. Machine learning is trapped in the Plato cave. See [Judea Pearl's book, the Book of Why](http://bayes.cs.ucla.edu/WHY/). \n",
    "\n",
    "![img](img/ch1/Plato_Cave.jpeg)  \n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "Followings are some shortcomings of machine learning when it comes to causal inference. \n",
    "\n",
    "- Machine learning is limited to transferability to new problems and any form of generalization to data with a different distribution. \n",
    "\n",
    "- Machine learning often disregards information that even animals use heavily, e.g., interventions, domain shifts, and temporal structure. \n",
    "\n",
    "- Most current successes of machine learning boil down to large-scale pattern recognition on suitably collected independent and identically distributed (i.i.d.) data which is not the case in reality!\n",
    "\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "## Going Beyond Machine Learning to Answer a Different Kind of Questions\n",
    "\n",
    "Machine Learning is currently very good at answering **prediction kinds of questions**. As the authors put it in the book Prediction Machines, \"the new wave of artificial intelligence does not bring us intelligence but instead a critical component of intelligence - prediction.\" We can do all sorts of fantastic things with machine learning. The only requirement is that we frame our problems as prediction problems. \n",
    "\n",
    "\n",
    "<font color='blue'>*Do we want to translate from English to Italian?*</font>\n",
    "\n",
    "<font color='blue'>*Do we want to recognize human faces?*</font>\n",
    "\n",
    "\n",
    "An ML algorithm can wonder under very strict boundaries, and it fails miserably if our used data deviates a little from what the model has been trained before. ML is notoriously poor at this inverse causality type of problem that requires us to answer **what if** questions or **counterfactuals**. \n",
    "\n",
    "<font color='blue'>*What would happen if I do a low-sugar one instead of this low-fat diet I'm in?*</font>\n",
    "\n",
    "<font color='blue'>*What would happen if I used another price instead of this price I'm currently asking for my merchandise?*</font>\n",
    "\n",
    "At the heart of these questions, there is a causal inquiry we wish to know the answer to. **Causal Questions** permeate everyday problems, like figuring out how to make sales go up. Still, they also play an important role in very personal and dear dilemmas: \n",
    "\n",
    "<font color='blue'>*Do I have to go to an expensive school to be successful in life (does education cause earnings)?*</font>\n",
    "\n",
    "<font color='blue'>*Does the public healthcare system increase life expectancy?*</font>\n",
    "\n",
    "\n",
    "Unfortunately for Machine Learning, we can't rely on correlation-type predictions to answer causal questions. Answering this kind of question is more challenging than most people appreciate. Your teachers have probably repeated that \"association is not causation\" and \"association is not causation.\" This is what this course is all about. \n",
    "\n",
    "![img](img/ch1/Courtroom.png)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "681a415d-d513-41b0-bd5a-9eea5b588413",
    "deepnote_cell_height": 2079.828125,
    "deepnote_cell_type": "markdown",
    "owner_user_id": "a91a2c6a-0c99-4421-a773-926bd3919ed9",
    "tags": []
   },
   "source": [
    "<br/><br/>\n",
    "## What is Causality?\n",
    "\n",
    "<br/><br/>\n",
    "### Traditional Statistical Inference Paradigm\n",
    "\n",
    "To explain causality, we go back to statistics fundamentals. Statistics summarize a population/set/observation into a distribution based on samples drawn from that population. Remember that we cannot derive causal claims from observational data alone.\n",
    "\n",
    "Causal inference is the scientific process in which cause-and-effect relationships are inferred from observational data, but only after assuming a **causal model** that drives the relationships between random variables. \n",
    "\n",
    "We used an analogy proposed initially by [Judea Pearl, 2016](http://bayes.cs.ucla.edu/jsm-august2016-bw.pdf) and later used by [Camilo Hurtado, 2017](https://repositorio.unal.edu.co/handle/unal/59495) to better explain the causal inference. We assume an unknown, invariant, and true data generating process, $M$, generates a set of observed random variables (data), $D$, and associated multivariate probability distribution, $P$. \n",
    "The target of scientific inquiry in traditional statistical analysis is a probabilistic quantity, $Q(P)$, which summarizes some attribute of $D$ that is of our interest. $Q(P)$ can be estimated from $P$ and $D$ alone. \n",
    "\n",
    "![img](img/ch1/Stat_Paradigm.png)\n",
    "\n",
    "However, causal analysis is different from statistical analysis. Causal inference is interested in an external intervention (treatment) effect on the causal system $M$ when experimental conditions change. This **intervention** acts as a specific modification to the data-generating model $M$, leading to an **unobserved (counterfactual) set of data $D'$ and a distribution $P'$**. This change is known as the **causal effect of an intervention**. In other words, it is the changes in the data generating process $M$ that generate hypothetical (unobserved) $D'$ and $P'$. Then, a causal target parameter $Q(P')$ is computed, which summarizes the causal effect of the given intervention (or treatment). \n",
    "\n",
    "![img](img/ch1/Causal_Paradigm.png)\n",
    "\n",
    "The problem is that we only have access to $D$ and therefore $P$ in observational studies, while $D'$ and $P'$ remain unknown. Therefore, $D$ or $P$ alone cannot answer the causal quantity of interest. That is why we use a set of (un)testable causal assumptions to estimate $Q(P')$ from $D$ and $P$. With these assumptions at hand; we can mathematically express $Q(P')$ in terms of both $D$ and $P$, leaving $D'$ and $P'$ out.\n",
    "\n",
    "<br/><br/>\n",
    "### Causality is Beyond Statistics\n",
    "\n",
    "- Causal inference requires extra information. There is nothing in the distribution of the data alone that tells us how it should change when conditions change.\n",
    "- To make causal inferences we must make **assumptions** about the processes that generated the data. These are not statistical assumptions.\n",
    "- **Causal assumptions** come from the expertise and previous experience of the researcher.\n",
    "- **Causal questions** are questions about what happens when we change the way data are generated.\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "To summarize, we have two schools of thought for inference:\n",
    "\n",
    "- **Associational Inference:** it includes any relationship that can be defined in terms of a joint distribution of observed variables\n",
    "    - Correlation, conditional independence, dependence, likelihood, confidence level…\n",
    "    - Testable in principle\n",
    "\n",
    "- **Causation Inference:** it includes any relationship that cannot be defined in terms of joint distribution alone\n",
    "    - Randomization, confounding, mediation, attribution, effect, …\n",
    "    - Not testable in principle (without experimental control)\n",
    "    - we can only test them if we can intervene on the system and see what happens. Even if we can intervene, there are some complications. But there are some tricks…\n",
    "    - Capturing the patterns in the data we capture the principle patterns of the process that generates the data.\n",
    "    \n",
    "![img](img/ch1/DataGeneration.gif)\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "### Causality Ladder\n",
    "\n",
    "In the [Book of Why](http://bayes.cs.ucla.edu/WHY/), Judea Pearl suggested the **Ladder of Causation**, which represents three levels of causality with different organisms at each level. \n",
    "* Most animals and present-day learning machines are on the first level, learning from the association. \n",
    "* Tool users, such as early humans, are on the second level if they act by planning and not merely by imitation. We can also use experiments to learn the effects of interventions, and presumably, this is how babies acquire much of their causal knowledge. \n",
    "* Finally, on the top level, counterfactual learners can imagine worlds that do not exist and infer reasons for observed phenomena. \n",
    "\n",
    "![img](img/ch1/Causal_Ladder.png)\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "<h3 style=\"color: green;\">Ask ChatGPT</h3>\n",
    "\n",
    "<font color='green'>**Prompt: What is the difference of causal inference and machine learning?**<font>\n",
    "\n",
    "\n",
    "Causal inference and machine learning are two distinct but related fields with different goals and approaches, although there is some overlap between them. Here are the key differences:\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "Causal Inference: The primary goal of causal inference is to understand cause-and-effect relationships and make causal claims. It focuses on identifying the causal effects of specific factors or interventions on outcomes of interest.\n",
    "Machine Learning: The primary goal of machine learning is to develop algorithms and models that can learn patterns and make predictions or classifications based on data. It focuses on optimizing predictive accuracy and generalization to new data.\n",
    "\n",
    "**Focus:**\n",
    "\n",
    "Causal Inference: Causal inference is concerned with understanding the underlying mechanisms and factors that lead to a particular outcome. It seeks to answer questions like \"What is the effect of X on Y?\" or \"What would have happened if X had not occurred?\"\n",
    "Machine Learning: Machine learning focuses on building models that can accurately predict outcomes based on input variables. It emphasizes finding patterns and relationships in the data that allow for accurate predictions without necessarily aiming to establish causal relationships.\n",
    "\n",
    "**Data Requirements:**\n",
    "\n",
    "Causal Inference: Causal inference often requires specific types of data, such as randomized controlled trial (RCT) data or carefully designed observational studies with controls for confounding factors. It typically involves collecting data that allow for the identification of causal effects.\n",
    "Machine Learning: Machine learning can work with various types of data, including observational data, as long as it provides enough information to train a model and make accurate predictions. It can handle complex and large-scale datasets without necessarily requiring specific experimental designs.\n",
    "Methods and Approaches:\n",
    "\n",
    "Causal Inference: Causal inference relies on a range of statistical and econometric techniques, such as propensity score matching, instrumental variables, difference-in-differences, or structural equation modeling. These methods are designed to control for confounding and estimate causal effects.\n",
    "Machine Learning: Machine learning encompasses a broad set of algorithms and techniques, such as decision trees, neural networks, support vector machines, and ensemble methods. These methods are primarily focused on learning patterns and relationships in data to optimize predictive performance.\n",
    "It's important to note that while causal inference and machine learning have distinct goals, they can also be complementary. Causal inference can benefit from machine learning techniques for data analysis and prediction, while machine learning can benefit from causal inference to improve interpretability and make more informed decisions based on identified causal relationships.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00009-1b357484-1425-412d-abc8-5a53506c4d3d",
    "deepnote_cell_height": 189.5625,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## The Danger of Oversimplification\n",
    "\n",
    "* Be aware of **our temptation to oversimplify**.\n",
    "* Simple modes are sweeter for human intution. But, intuition is not enough to help us make sense of complex data- that is why we have statistics.\n",
    "* Before using simpler models, let's have our or expert's knowledge justify the model first.\n",
    "* **Oversimplification leads to misinformation**.\n",
    "\n",
    "![img](img/ch1/misinformation.png)  \n",
    "\n",
    "Here is the a model vs. a simplified model:\n",
    "\n",
    "![img](img/ch1/Oversimplified-model.png)  \n",
    "![img](img/ch1/normal-model.png) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "768600bfea1a4cbb81ed3db277469651",
    "deepnote_cell_height": 301.734375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## References\n",
    "\n",
    "Most of the ideas in this chapter are taken from Judea Pearl Books. \n",
    "\n",
    "* [Causality, 2nd Edition](http://bayes.cs.ucla.edu/BOOK-2K/)\n",
    "* [The Book of Why](http://bayes.cs.ucla.edu/WHY/)\n",
    "\n",
    "We also like to reference the open-source book on causality by Matheus Facure Alves. He did a great job in explaining causal concepts with examples and fuuny memes.\n",
    "\n",
    "* [Causal Inference for The Brave and True](https://matheusfacure.github.io/python-causality-handbook/landing-page.html)\n",
    "\n",
    "[Ilya Shpitser](https://www.cs.jhu.edu/~ilyas/) from JHU also did a great job with his causal inference course. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "ca97ebbe-c21a-477a-b9d8-e5669467789a",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
