{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Chapter 4. Structural Causal Models\n\n**Structural equation modeling (SEM)** is a family of various methods scientists use in experimental and observational research. SEM is a model in which different aspects of a phenomenon are related to one another with a structure. This structure is a system of equations that implies statistical and often causal relationships between variables. \n\nSEMs also called **Structural Causal Models (SCM)** are important tools to relate causal and probabilistic statements.\n\n## Graphs vs. Structural Equations\n\nWe saw in Chapter 3 that: \n- **G+** Graph is an excellent tool for communicating with subject matter experts. \n- **G+** Graph can be a helpful way to translate assumptions into a formal model. \n- **G+** Graphs are also useful to see what restrictions (if any) our model puts on the joint distribution of the observed data.\n\n- **E+** As the model becomes more complicated, the equations get a lot more friendly to work with.\n- **E+** Equations may help you resist the urge to oversimplify.\n\n\n## Structural Causal/Equation Model\n\nAn **Structural Causal Model (SCM)** or **Structure Equation Model (SEM)** consists of:\n\n1. *Endogenous variables* $X = {X_1 , ..., X_J }$ \n    - Affected by other variables in the model\n    - May or may not be observed\n\n2. *Background (exogenous) variables* $U = {U_1 , ..., U_J }$ \n    - Not affected by other factors in the model\n    - Not observed\n    - $U_j$ is noise or error variables and $U$ is a joint distribution over noise variables.\n    - Each endogenous variable $X_j$ has an error $U_j$.\n\n3. *Functions* $F = \\lbrace f_{X_1} , ... , f_{X_J} \\rbrace $\n    - The functions $F$ define a set of $J$ **structural equations** for each of the endogenous variables:\n    $X_j = f_{X_j}(Pa(X_j),U_{X_j}), j = 1,...,J$\n\n    $Pa(X_j) \\subseteq X \\backslash Xj$\n    \nwhere $Pa(X_j)$ called **parents** of $X_j$.\n\n- We sometimes call the elements of $PA_j$ not only parents but also **direct causes** of $X_j$, \n- We call $X_j$ a **direct effect** of each of its **direct causes** $PA_j$. \n\n\n\n<br/><br/>\n\n**W-A-Y Example, Make a Graph for SEM:** \n\nThe graph $G$ of an SCM is obtained by creating one node for each $X_j$ and drawing directed edges from each parent in $PA_j$ to $X_j$. \n\n![img](img/ch4/Graph-SEM.png)\n\n- Connect parents to children with a directed link.\n- Each endogenous variable $X_j$ has an error $U_j$.\n- Potential dependence between errors $U_j$ encoded in dashed lines/double headed errors.\n\n- We assume this graph $G$ is acyclic, without directed cycles/feedback loops.\n    - Instead feedback loops, we can use temporal ordering.\n    - In other words, extend graph (and corresponding structural equations) over time\n\n- We will work with recursive SCM. There is ordering between $X={X1,...,XJ}$ such that each $X_j$ is a function of a subset $Pa(Xj)$ of its **predecessors**.\n    - Causes always precede their effects\n    - A natural source of ordering is *time*\n\n    $X_j = f_{X_j}(Pa(X_j),U_{X_j}), j = 1,...,J $ \n\n    $Pa(X_j) \\subseteq {X_1, ..., X_{j-1}}$\n\n<br/><br/>\n\n\n**W-A-Y Example, Causal Exclusion:** \n\nExclusion restrictions on the graph encoded through absence of arrows between variables.\n- Absence of arrow means no direct effect\n\n![img](img/ch4/Graph-SEM-Excluded.png)\n\n<br/><br/>\n\n\n**W-A-Y Example, Independence Assumptions:** \n- Absence of double headed arrows between background (exogenous) variables or errors $U$ means those two errors are **independent**.\n    - It is an assumption on distribution $P_U$.\n    - There is no unmesaured common cause between those two $U_j$\n\n![img](img/ch4/Graph-SEM-Independence.png)\n\n\n<div class=\"alert alert-block alert-info\">\n\n**Proposition 4.1. (SCM Entailed Distributions):** An SCM $C$ defines a unique distribution\nover the variables $X = (X_1,...,X_J)$ such that $X_j = f_{X_j}(Pa(X_j),U_{X_j})$ for\n$j = 1,...,J$. We refer to it as the entailed distribution $P^C_X$ and sometimes write $P_X$.\n\n</div>\n\n<br/><br/>\n\n### SEM Overview\n\n- SEM or SCM represented as a set of non-parametric structural equations or as a graph.\n- SEM or SCM represent what we know AND what we do not know.\n- We represented our knowledge by:\n    1. Exclusion restrictions (missing arrows)\n    2. Independence assumptions (independence of background factors, or no unmeasured common cause)\n    3. We often need to make additional assumptions in order to improve our knowledge",
   "metadata": {
    "cell_id": "333030af5f4a4fa6a16bb18625a6edfc",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 3363.34375
   }
  },
  {
   "cell_type": "markdown",
   "source": "<br/><br/>\n## Interventions on a SCM\n\n- When we intervene on variable $X_j$, say, and set it to a specific outcome. We expect that this intervention changes the distribution of the system compared to its earlier behavior without intervention. \n\n    - Even if variable $X_j$ was causally influenced by other variables before, it is now influenced by nothing else. \n    - $X_j$ has no more causal parents. \n\n- The autonomy of structural equations means that we can make a targeted modification to the set of equations in order to represent our intervention of interest.\n\n\n<div class=\"alert alert-block alert-info\">\n\n**Proposition 4.2. (Intervention Distribution):**[source](https://mitpress.mit.edu/books/elements-causal-inference) Consider an SCM $\\mathfrak{C}$ and its entailed distribution $P^C_X$. We replace one (or several) of the structural assignments to obtain a new SCM $\\tilde{\\mathfrak{C}}$. Assume that we replace the assignment for $X_j$ by:\n\n$X_j = \\tilde{f}_{X_j}(\\tilde{Pa}(X_j),U_{X_j})$\n\nWe then call the entailed distribution of the new SCM an **intervention distribution** and say that the variables whose structural assignment we have replaced have been **intervened** on. We denote the new distribution by:\n\n$P^{\\tilde{\\mathfrak{C}}}_X = P^{\\mathfrak{C} ; do(\\tilde{f}_{X_j}(\\tilde{Pa}(X_j),U_{X_j}))}_X$\n\nThe set of noise variables in $\\tilde{\\mathfrak{C}}$ now contains both some “new” $\\tilde{U}$̃ ’s and some “old” $U$'s, all of which are required to be jointly independent.\n\n</div>\n\n<br/><br/>\n\n**W-A-Y Example, Intervention on A:** \n- We intervene on the system to set $A=1$ and we replace $f_A$ with constant function $A=1$.\n\n![img](img/ch4/Grpah-SEM-Intervene.png)\n\n- $Y_a(u)$ is defined as the solution to the equation $f_Y$ under an intervention on the system of equations to set $A=a$ (with input $U=u$).\n- We can think of $u$ as a particular realization of (values for) the background factors\n- $P_U$ and $F$ induce a probability distribution on $Y_a$ just as they do on $Y$.\n- $Y_a$ is the **post-intervention** or **counterfactual** random variable.\n\n\n<br/><br/>\n\n**Simple Prediction Example, Intervention Targets:** \n\nThis example considers prediction. It shows that even though some variables may be good predictors for a target variable $Y$ , intervening on them may leave the target variable unaffected. Consider the following SCM $\\mathfrak{C}$:\n\n![img](img/ch4/Exp_Predictors_Intervention_Targets.png)\n\n$X_1 = N_{X_1}$\\\n$Y = X_1 + N_Y$\\\n$X_2 = N_{X_2}$\n\nwith following distrbution being jointly independent:\n\n$\n\\begin{cases}\nN_{X_1} \\stackrel{iid}{\\sim} \\mathcal{N}(0,1) \\\\\nN_{X_2} \\stackrel{iid}{\\sim} \\mathcal{N}(0,0.1) \\\\\nN_{Y} \\stackrel{iid}{\\sim} \\mathcal{N}(0,1) \n\\end{cases}\n$\n\n<br/>\n\n**Case 1: Intervene on $X_2$:** We are interested in predicting $Y$ from $X_1$ and $X_2$. Clearly, $X_2$ is a better predictor for $Y$ than $X_1$ is. However, if we want to change $Y$  interventions on $X_2$ are useless. In other words, no matter how strongly we intervene on $X_2$, the distribution of $Y$ remains unaffected\n\n$P^{\\mathfrak{C} ; do(X_2 = \\stackrel{\\sim}{N})}_Y = P^{\\tilde{\\mathfrak{C}}}_Y$\n\n<br/>\n\n**Case 2: Intervene on $X_1$:** An intervention on $X_1$, however, does change the distribution $Y$.\n\n$P^{\\mathfrak{C} ; do(X_1 = \\stackrel{\\sim}{N})}_Y = \\mathcal{N}(E(N_Y) + E(\\stackrel{\\sim}{N}), var (N_Y) + var(\\stackrel{\\sim}{N}))$ \n\n\n$P^{\\mathfrak{C} ; do(X_1 = \\stackrel{\\sim}{N})}_Y \\neq P^{\\tilde{\\mathfrak{C}}}_Y$",
   "metadata": {
    "cell_id": "7d4b15c6e44f4895808d2875a27c5e72",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 2134.328125
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Causal Models and Counterfactuals\n\nHumans often think in **counterfactuals**: *“I should have taken the train.”* or *“We should have invested in Bitcoin in January 2019!”* are only a few examples. Assume someone offers you $10, 000 if you predict the result of a coin flip, you guess “heads” and lose. Some people may then think, “Why did I not say ‘tails’?” even though there was no way they could have possibly known the outcome. \n\nPeople have been thinking about counterfactuals for a long time. For example, Titus Livius discusses in 25 BC what would have happened if Alexander the Great had not died in Asia and had attacked Rome. Livy argues that Rome and Carthage would have joined forces to crush the Macedonian army. [Geradin and Girgenson, 2011](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1970917). \n\n![img](img/ch4/Alexander_mosaic.jpeg)\n\n\n<br/><br/>\n\n**Example: Crop Planning**  \n\nThis example is from [Jerzy Neyman, 1923](https://en.wikipedia.org/wiki/Jerzy_Neyman#:~:text=After%20his%20return%20to%20Poland,studied%20randomized%20experiments%20in%201923.) Consider $m$ plots of land and $v$ varieties of crop. Denote $U_ij$ the crop yield that would be observed if variety $i = 1, . . . , v$ were planted in plot $j = 1,...,m$.\n\nFor each plot $j$, we can only experimentally determine one $U_{ij}$ in each growing season. The others crop yields are called **counterfactuals**.\n\n\n<div class=\"alert alert-block alert-info\">\n\n**Definition 4.3. (Counterfactuals):** Counterfactual corresponds to updating the noise distributions of an SCM (by conditioning) and then performing an intervention.\nConsider an SCM $\\mathfrak{C} = (S,P_N)$ over nodes $X$. Given some observations $x$, we define a *counterfactual SCM* by replacing the distribution of noise variables:\n\n$$\n\\mathfrak{C}_{\\mathbf{X}=\\mathbf{x}}:=\\left(\\mathbf{S}, P_{\\mathbf{N}}^{\\mathfrak{C} \\mid \\mathbf{X}=\\mathbf{x}}\\right)\n$$\n\nwhere \n$$\nP_{\\mathbf{N}}^{\\mathfrak{C} \\mid \\mathbf{X}=\\mathbf{x}} = P_{\\mathbf{N} \\mid \\mathbf{X}=\\mathbf{x}}^{}\n$$\n\nThe new set of noise variables need not be jointly independent anymore. *Counterfactual* statements can now be seen as *do-statements* in the new counterfactual SCM.\n\n</div>\n\n<br/><br/>\n\n**Example: Three Integers, Computing Counterfactuals:** \n\nThis example is from [Elements of Causal Inference Book, Chapter 6](https://mitpress.mit.edu/books/elements-causal-inference). \nConsider the following SCM $\\mathfrak{C}$:\n\n$\n\\begin{cases}\nX = N_X \\\\\nY = X^{2} + N_Y\\\\\nZ = 2Y + X + N_Z\n\\end{cases}\n$\n\nwith uniformly distributed noise values on the integers between −5 and 5:\n\n$\nN_{X}, N_{Y}, N_{Z} \\stackrel{\\text { iid }}{\\sim} \\mathrm{U}(\\{-5,-4, \\ldots, 4, 5\\})\n$\n\n<br/>\n\n**Case 1: Observation:** We assume that we observe $(X,Y,Z) = (1,2,4)$.\n\nThen $P_{\\mathbf{N}}^{\\mathfrak{C} \\mid \\mathbf{X}=\\mathbf{x}}$ puts a point mass on $(N_X , N_Y , N_Z) = (1, 1, -1)$ because here all noise terms can be uniquely reconstructed from the observations.\n\n<br/>\n\n**Case 2: Counterfactual:** In the context of $(X,Y,Z) = (1,2,4)$ observation, we have a counterfactual statement: “$Z$ would have been 11 if we had $X$ been set to 2.” \n\nMathematically, this means that $P_{\\mathbf{Z}}^{\\mathfrak{C} \\mid \\mathbf{X}=\\mathbf{x};do(X=2)} = 11$ or it has $Z$ a point mass on 11. \n\nIf we have a counterfactual statement that says: $Y$ would have been 5 if we had $X$ been set to 2.\n\nMathematically, this is $P_{\\mathbf{Y}}^{\\mathfrak{C} \\mid \\mathbf{X}=\\mathbf{x};do(X=2)} = 5$. \n\nCounterfactuals notation may looks quite complicated.The following image provides further clarification:\n\n![img](img/ch4/Counterfactuals_notation.png)\n",
   "metadata": {
    "cell_id": "c3ab26dcd97a4d69a239c13402c708c1",
    "tags": [],
    "owner_user_id": "a91a2c6a-0c99-4421-a773-926bd3919ed9",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 2154.171875
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Linking Observations to SCMs using Graphs\n\n\n### <font color='blue'> **What causal structures can lead to dependence between two observed variables?**</font>\n\n\n**1- Direct and Indirect Effects**\n\nAn effect of $A$ on $Y$ can result in an **association**.\n\n![img](img/ch4/SEM-Observe-Direct-Effects.png)\n\n\n**2- Shared Common Cause**\n\nCommon cause (measured or unmeasured) of $A$ and $Y$ can result in an association. When the common cause is not included in $X$, it is represented through the dependence it induces between errors $U$.\n\n![img](img/ch4/SEM-Observe-Share-Cause.png)\n\n**3- Non of conditions 1 and 2**\n\nIf neither of these sources of dependence are present, $A$ and $Y$ will be **independent** in every probability distribution $P_0$ compatible with the SCM. \nIn other words, any data generating experiment that is compatible with the SCM will give rise to an observed data distribution in which $A$ and $Y$ are independent regardless of functional form, strength of associations, etc.\n\n![img](img/ch4/SEM-Observe-Independent.png)\n\n**4- Conditioning on a Collider**\n\nCollider is an “inverted fork”. Conditioning on a common effect (descendent) of $A$ and $Y$ can result in an association between $A$ and $Y$. It also called **Berkson’s bias or selection bias**.\n\n![img](img/ch4/SEM-Observe-Colider.png)\n\n\n### <font color='blue'> **What causal structures can remove a source of dependence between variables?**</font>\n\n**Conditioning on a shared common cause** \n\nConditioning on a causal intermediate or shared common cause between $A$ and $Y$ will remove that source of dependence.\n\n![img](img/ch4/SEM-Observe-Confounder.png)\n\n\n### <font color='blue'> **When does a SCM imply that variables are independent?**</font>\n\n**$A$ is independent of $Y$** if there is no path between $A$ and $Y$.\n\n![img](img/ch4/SEM-Observe-Independ-nopath.png)\n\n**$A$ is independent of $Y$** if all paths between $A$ and $Y$ are “blocked” by a collider.\n\n![img](img/ch4/SEM-Observe-Independ-blocked.png)\n\n**$A$ is independent of $Y$ given $W$** if $W$ blocks all unblocked paths and doesn’t create any new unblocked paths. In onther words, conditioning on a non-collider blocks a path.\n\n![img](img/ch4/SEM-Observe-Independ-condition.png)\n\n**$A$ is independent of $Y$ given $W$** if conditioning on a collider (or a descendent of a collider) opens a path.\n\n![img](img/ch4/SEM-Observe-Independ-condition-collider.png)",
   "metadata": {
    "cell_id": "d941b7da8a114925bd1b35e62c40e7d2",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 3130.359375
   }
  },
  {
   "cell_type": "markdown",
   "source": "## References\n\nThis chapter contents are highly inspired by the [Elements of Causal Inference (Open Access) book](https://mitpress.mit.edu/books/elements-causal-inference) by By Jonas Peters, Dominik Janzing and [Bernhard Schölkopf](https://www.is.mpg.de/~bs).\n\nWe also used examples fom the [Introduction to Causal Inference course](https://www.ucbbiostat.com) by Maya L. Petersen & Laura B. Balzer, UC Berkeley.\n\nBruno Gonçalves has a helpful [blog](https://medium.data4sci.com/causal-inference-part-iv-structural-causal-models-df10a83be580) on SEM too.",
   "metadata": {
    "cell_id": "caa1137eda4c4ea4bbf611fff7e1fb61",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 225.953125
   }
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=e9f789e8-0475-4790-a085-15ec7f70f6c3' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "ca1c0dae-cfcc-4ea4-b356-eeedce5aaafd",
  "deepnote_execution_queue": []
 }
}