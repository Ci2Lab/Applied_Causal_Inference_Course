{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "333030af5f4a4fa6a16bb18625a6edfc",
    "deepnote_cell_height": 3363.34375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "# Chapter 4. Structural Causal Models\n",
    "\n",
    "**Structural equation modeling (SEM)** is a family of various methods scientists use in experimental and observational research. SEM is a model in which different aspects of a phenomenon are related to one another with a structure. This structure is a system of equations that implies statistical and often causal relationships between variables. \n",
    "\n",
    "SEMs also called **Structural Causal Models (SCM)** are important tools to relate causal and probabilistic statements.\n",
    "\n",
    "## Graphs vs. Structural Equations\n",
    "\n",
    "We saw in Chapter 3 that: \n",
    "- **G+** Graph is an excellent tool for communicating with subject matter experts. \n",
    "- **G+** Graph can be a helpful way to translate assumptions into a formal model. \n",
    "- **G+** Graphs are also useful to see what restrictions (if any) our model puts on the joint distribution of the observed data.\n",
    "\n",
    "- **E+** As the model becomes more complicated, the equations get a lot more friendly to work with.\n",
    "- **E+** Equations may help you resist the urge to oversimplify.\n",
    "\n",
    "\n",
    "## Structural Causal/Equation Model\n",
    "\n",
    "An **Structural Causal Model (SCM)** or **Structure Equation Model (SEM)** consists of:\n",
    "\n",
    "1. *Endogenous variables* $X = {X_1 , ..., X_J }$ \n",
    "    - Affected by other variables in the model\n",
    "    - May or may not be observed\n",
    "\n",
    "2. *Background (exogenous) variables* $U = {U_1 , ..., U_J }$ \n",
    "    - Not affected by other factors in the model\n",
    "    - Not observed\n",
    "    - $U_j$ is noise or error variables and $U$ is a joint distribution over noise variables.\n",
    "    - Each endogenous variable $X_j$ has an error $U_j$.\n",
    "\n",
    "3. *Functions* $F = \\lbrace f_{X_1} , ... , f_{X_J} \\rbrace $\n",
    "    - The functions $F$ define a set of $J$ **structural equations** for each of the endogenous variables:\n",
    "    $X_j = f_{X_j}(Pa(X_j),U_{X_j}), j = 1,...,J$\n",
    "\n",
    "    $Pa(X_j) \\subseteq X \\backslash Xj$\n",
    "    \n",
    "where $Pa(X_j)$ called **parents** of $X_j$.\n",
    "\n",
    "- We sometimes call the elements of $PA_j$ not only parents but also **direct causes** of $X_j$, \n",
    "- We call $X_j$ a **direct effect** of each of its **direct causes** $PA_j$. \n",
    "\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "**W-A-Y Example, Make a Graph for SCM:** \n",
    "\n",
    "The graph $G$ of an SCM $\\mathfrak{C}$ is obtained by creating one node for each $X_j$ and drawing directed edges from each parent in $PA_j$ to $X_j$. \n",
    "\n",
    "![img](img/ch4/Graph-SEM.png)\n",
    "\n",
    "- Connect parents to children with a directed link.\n",
    "- Each endogenous variable $X_j$ has an error $U_j$.\n",
    "- Potential dependence between errors $U_j$ encoded in dashed lines/double headed errors.\n",
    "\n",
    "- We assume this graph $G$ is acyclic, without directed cycles/feedback loops.\n",
    "    - Instead feedback loops, we can use temporal ordering.\n",
    "    - In other words, extend graph (and corresponding structural equations) over time\n",
    "\n",
    "- We will work with recursive SCM. There is ordering between $X={X1,...,XJ}$ such that each $X_j$ is a function of a subset $Pa(Xj)$ of its **predecessors**.\n",
    "    - Causes always precede their effects\n",
    "    - A natural source of ordering is *time*\n",
    "\n",
    "    $X_j = f_{X_j}(Pa(X_j),U_{X_j}), j = 1,...,J $ \n",
    "\n",
    "    $Pa(X_j) \\subseteq {X_1, ..., X_{j-1}}$\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "**W-A-Y Example, Causal Exclusion:** \n",
    "\n",
    "Exclusion restrictions on the graph encoded through absence of arrows between variables.\n",
    "- Absence of arrow means no direct effect\n",
    "\n",
    "![img](img/ch4/Graph-SEM-Excluded.png)\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "**W-A-Y Example, Independence Assumptions:** \n",
    "- Absence of double headed arrows between background (exogenous) variables or errors $U$ means those two errors are **independent**.\n",
    "    - It is an assumption on distribution $P_U$.\n",
    "    - There is no unmesaured common cause between those two $U_j$\n",
    "\n",
    "![img](img/ch4/Graph-SEM-Independence.png)\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Proposition 4.1. (SCM Entailed Distributions):** An SCM $\\mathfrak{C}$ defines a unique distribution over the variables $X = (X_1,...,X_J)$ such that $X_j = f_{X_j}(Pa(X_j),U_{X_j})$ for $j = 1,...,J$. \n",
    "We refer to it as the entailed distribution $P^C_X$ and sometimes write $P_X$.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "7d4b15c6e44f4895808d2875a27c5e72",
    "deepnote_cell_height": 2134.328125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<br/><br/>\n",
    "\n",
    "## Interventions on a SCM\n",
    "\n",
    "- When we intervene on variable $X_j$, say, and set it to a specific outcome. We expect that this intervention changes the distribution of the system compared to its earlier behavior without intervention. \n",
    "\n",
    "    - Even if variable $X_j$ was causally influenced by other variables before, it is now influenced by nothing else. \n",
    "    - $X_j$ has no more causal parents. \n",
    "\n",
    "- The autonomy of structural equations means that we can make a targeted modification to the set of equations in order to represent our intervention of interest.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Proposition 4.2. (Intervention Distribution):** [source](https://mitpress.mit.edu/books/elements-causal-inference) \n",
    "\n",
    "Consider an SCM $\\mathfrak{C}$ and its entailed distribution $P^\\mathfrak{C}_X$. We replace one (or several) of the structural assignments to obtain a new SCM $\\tilde{\\mathfrak{C}}$. \n",
    "\n",
    "<br/>\n",
    "\n",
    "Assume that we replace the assignment for $X_j$ by:\n",
    "\n",
    "$X_j = \\tilde{f}_{X_j}(\\tilde{Pa}(X_j),U_{X_j})$.\n",
    "\n",
    "<br/>\n",
    "\n",
    "We then call the entailed distribution of the new SCM $\\tilde{\\mathfrak{C}}$ an **intervention distribution** and say that the variables whose structural assignment we have replaced have been **intervened** on. We denote the new distribution $P^{\\tilde{\\mathfrak{C}}}_X$ by:\n",
    "\n",
    "$P^{\\tilde{\\mathfrak{C}}}_X = P^{\\mathfrak{C} ; do(\\tilde{f}_{X_j}(\\tilde{Pa}(X_j),U_{X_j}))}_X$.\n",
    "\n",
    "<br/>\n",
    "\n",
    "The set of noise (error) variables in $\\tilde{\\mathfrak{C}}$ now contains both some “new” $\\tilde{U}$ 's and some \"old\" $U$'s, all of which are required to be jointly independent.\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "**W-A-Y Example, Intervention on A:** \n",
    "- We intervene on the system to set $A=1$ and we replace $f_A$ with constant function $A=1$.\n",
    "\n",
    "![img](img/ch4/Grpah-SEM-Intervene.png)\n",
    "\n",
    "- $Y_a(u)$ is defined as the solution to the equation $f_Y$ under an intervention on the system of equations to set $A=a$ (with input $U_Y=u$).\n",
    "- We can think of $u$ as a particular realization of (values for) the background factors\n",
    "- $P_{U_Y}$ and $F$ induce a probability distribution on $Y_a$ just as they do on $Y$.\n",
    "- $Y_a$ is a **post-intervention** or **counterfactual** random variable.\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "**Simple Prediction Example, Intervention Targets:** \n",
    "\n",
    "This example considers prediction. It shows that even though some variables may be good predictors for a target variable $Y$ , intervening on them may leave the target variable unaffected. Consider the following SCM $\\mathfrak{C}$:\n",
    "\n",
    "![img](img/ch4/Exp_Predictors_Intervention_Targets.png)\n",
    "\n",
    "$\n",
    "\\begin{cases}\n",
    " X_1 = U_{X_1}\\\\\n",
    " Y = X_1 + U_Y\\\\\n",
    " X_2 = Y + U_{X_2}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "with following distribution being jointly independent:\n",
    "\n",
    "$\n",
    "\\begin{cases}\n",
    "U_{X_1} \\stackrel{iid}{\\sim} \\mathcal{N}(0,1) \\\\\n",
    "U_{X_2} \\stackrel{iid}{\\sim} \\mathcal{N}(0,0.1) \\\\\n",
    "U_{Y} \\stackrel{iid}{\\sim} \\mathcal{N}(0,1) \n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Case 1: Intervene on $X_2$:** We are interested in predicting $Y$ from $X_1$ and $X_2$. Clearly, $X_2$ is a better predictor for $Y$ than $X_1$ is. Clearly, X2 is a better predic- tor for Y than X1 is. For example, a linear model without $X_2$ leads to a (significantly) larger mean squared error than a linear model without $X_1$ would.\n",
    "\n",
    "\n",
    "However, if we want to study $Y$, intervention on $X_2$ is useless. In other words, no matter how strongly we intervene on $X_2$, the distribution of $Y$ remains unaffected\n",
    "\n",
    "$P^{\\mathfrak{C} ; do(X_2 = \\stackrel{\\sim}{U})}_Y = P^{\\mathfrak{C}}_Y$\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Case 2: Intervene on $X_1$:** An intervention on $X_1$, however, does change the distribution $Y$.\n",
    "\n",
    "$P^{\\mathfrak{C} ; do(X_1 = \\stackrel{\\sim}{U})}_Y = \\mathcal{N}(E(U_Y) + E(\\stackrel{\\sim}{U}), var (U_Y) + var(\\stackrel{\\sim}{U}))$ \n",
    "\n",
    "\n",
    "$P^{\\mathfrak{C} ; do(X_1 = \\stackrel{\\sim}{U})}_Y \\neq P^{\\mathfrak{C}}_Y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "## Calculating Intervention Distributions\n",
    "\n",
    "We use a trivial but powerful invariance statement for calculating intervention distributions. SCM $\\mathfrak{C}$, and writing $p a(j):=\\mathbf{P A}_{j}^{\\mathcal{G}}$, we have:\n",
    "\n",
    "$$\n",
    "p^{\\tilde{\\mathfrak{C}}}\\left(x_{j} \\mid x_{p a(j)}\\right)=p^{\\mathfrak{C}}\\left(x_{j} \\mid x_{p a(j)}\\right)\n",
    "$$ \n",
    "<p style='text-align: center;'> (Eq.1) </p>\n",
    "\n",
    "For any SCM $\\tilde{\\mathfrak{C}}$ that is constructed from $\\mathfrak{C}$ by **intervening** on (some) $X_k$ but not on $X_j$.\n",
    "\n",
    "- Equation above shows that causal relationships are **autonomous** under interventions. This property is therefore sometimes called **autonomy**. It means if we intervene on a variable, then the other mechanisms remain invariant.\n",
    "\n",
    "\n",
    "The autonomy formula is the base for the **G-computation formula** (Robins, 1986), **truncated factorization** (Pearl, 1993), or **manipulation theorem** (Spirtes, 2000) that we have seen in Chapter 3. To refresh our minds, we repeat those formulas.  \n",
    "\n",
    "We start with an SCM $\\mathfrak{C}$ with structural assignments with density $p^{\\mathfrak{C}}$:\n",
    "\n",
    "$$\n",
    "X_{j}:=f_{j}\\left(X_{pa(j)}, N_{j}\\right), \\quad j=1, \\ldots, d\n",
    "$$\n",
    "<p style='text-align: center;'> (Eq.2) </p>\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "Using Markov property, we have:\n",
    "\n",
    "$$\n",
    "p^{\\mathfrak{C}}\\left(x_{1}, \\ldots, x_{d}\\right)=\\prod_{j=1}^{d} p^{\\mathfrak{C}}\\left(x_{j} \\mid x_{p a(j)}\\right)\n",
    "$$\n",
    "<p style='text-align: center;'> (Eq.3) </p>\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "We do an **intervention**, so the SCM $\\mathfrak{\\tilde{C}}$ evolves from $\\mathfrak{C}$ after $do\\left(X_{k}:=\\tilde{N}_{k}\\right)$, where $\\tilde{N}_{k}$ density changes to $\\tilde{p}$. Again, we use the Markov assumption:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p^{\\mathfrak{C} ; d o\\left(X_{k}:=\\tilde{N}_{k}\\right)}\\left(x_{1}, \\ldots, x_{d}\\right) &=\\prod_{j \\neq k} p^{\\mathfrak{C} ; d o\\left(X_{k}:=\\tilde{N}_{k}\\right)}\\left(x_{j} \\mid x_{p a(j)}\\right) \\cdot p^{\\mathfrak{C} ; d o\\left(X_{k}:=\\tilde{N}_{k}\\right)}\\left(x_{k}\\right) \\\\\n",
    "&=\\prod_{j \\neq k} p^{\\mathfrak{C}}\\left(x_{j} \\mid x_{p a(j)}\\right) \\tilde{p}\\left(x_{k}\\right) .\n",
    "\\end{aligned}\n",
    "$$\n",
    "<p style='text-align: center;'> (Eq.4) </p>\n",
    "\n",
    "\n",
    "**The *(Eq.4)* allows us to compute an interventional statement (left-hand side) from observational quantities (right-hand side).**\n",
    "\n",
    "\n",
    "We can rewrite interventional statement in *(Eq.4)* as following too:\n",
    "\n",
    "$$\n",
    "p^{\\mathfrak{C} ; d o\\left(X_{k}:=a\\right)}\\left(x_{1}, \\ldots, x_{d}\\right)=\\left\\{\\begin{array}{cl}\n",
    "\\prod_{j \\neq k} p^{\\mathfrak{C}}\\left(x_{j} \\mid x_{p a(j)}\\right) & \\text { if } x_{k}=a \\\\\n",
    "0 & \\text { otherwise }\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "<p style='text-align: center;'> (Eq.5) </p>\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "We have seen in Chapter 2 that conditioning and intervening ( with $do()$ ) are usually different operations. However, these operations become identical for variables with no parents (source variables or exogenous variables). Let us assume that $X_1$ is such a source node without loss of generality. Then, we have:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p^{\\mathfrak{C}}\\left(x_{2}, \\ldots, x_{d} \\mid x_{1}=a\\right) &=\\frac{p\\left(x_{1}=a\\right) \\prod_{j=2}^{d} p^{\\mathfrak{C}}\\left(x_{j} \\mid x_{p a(j)}\\right)}{p\\left(x_{1}=a\\right)} \\\\\n",
    "&=p^{\\mathfrak{C} ; d o\\left(X_{1}:=a\\right)}\\left(x_{2}, \\ldots, x_{d}\\right) .\n",
    "\\end{aligned}\n",
    "$$\n",
    "<p style='text-align: center;'> (Eq.6) </p>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kidney Stones Example, Calculating Intervention \n",
    "\n",
    "This example is from a famous data set for the success rates of two treatments for kidney stones. It is a classic example of Simpson’s paradox and used in [Bottou et al., 2013](https://jmlr.org/papers/v14/bottou13a.html), [Charig et al., 1986](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1339981/). \n",
    "\n",
    "![img](img/ch4/Kidney-stones-xray.png)\n",
    "\n",
    "Out of 700 patients, one-half were treated with open surgery (treatment $T = A$, 78% recovery rate) and the other half with percutaneous nephrolithotomy ($T = B$, 83% recovery rate), a surgical procedure to remove kidney stones by a small puncture wound. \n",
    "\n",
    "If we do not know anything other than the overall recovery rates and neglect side effects, many people would prefer treatment $b$ if they had to decide.\n",
    "\n",
    "The table below shows the data.\n",
    "\n",
    "![img](img/ch4/Kidney-stones.png)\n",
    "\n",
    "### Solution Part 1: Avoiding Simpson Paradox with Intervention\n",
    "\n",
    "Observing the data in more detail, we can categorize kidney stones into small and large stones. We realize that open surgery performs better in both categories. **How do we deal with this inversion of conclusion?**\n",
    "\n",
    "\n",
    "**First Viewpoint:** \n",
    "\n",
    "- Larger stones are more severe than small stones, and treatment had to deal with many more difficult cases (even though the total number of patients assigned to $A$ and $B$ are equal). This is why treatment $A$ can look worse than $B$ on the entire population but better in both subgroups. \n",
    " - 263 out of 350 patients in treatment $A$ have large stones.\n",
    " - just 80 patients out of 350 in treatment $B$ have large stones.\n",
    "\n",
    "<br/>\n",
    "\n",
    "- The imbalance in assignment could, for example, arise if the medical doctors expect treatment $A$ to be better than treatment $B$ and therefore assign the difficult cases with large stones to treatment $A$ with a higher probability. \n",
    "\n",
    "**Second Viewpoint:** \n",
    "\n",
    "- We propose to use the language of interventions to formulate the precise question we are interested in. \n",
    "\n",
    "<br/>\n",
    "\n",
    "- Our goal is not whether treatment $T = A$ or treatment $T = B$ was more successful in this particular study but how the treatments outcomes compare when:\n",
    "\n",
    " - $P^{\\mathfrak{C} ; d o(T:=A)}$, we force all patients to take treatment $A$. \n",
    " - $P^{\\mathfrak{C} ; d o(T:=B)}$, we force all patients to take treatment $B$. \n",
    " - $P^{\\mathfrak{C} ; d o\\left(T:=\\tilde{N}_{T}\\right)}$, each patient is assigned randomly to one of the treatments.\n",
    "\n",
    "<br/>\n",
    "\n",
    "- These three situations concern an intervention distribution that is different from the observational distribution $P_\\mathbf{X}$. \n",
    "\n",
    "\n",
    "### Solution Part 2: Compute Intervention Distributions $P^{\\mathfrak{C} ; d o(T:=A)}, P^{\\mathfrak{C} ; d o(T:=B)}, \\text { or } P^{\\mathfrak{C} ; d o\\left(T:=\\tilde{N}_{T}\\right)}$\n",
    "\n",
    "We assume the true underlying SCM allows for the following graph where $Z$ is the size of the stone, $T$ the treatment, and $R$ the recovery (all binary).\n",
    "\n",
    "![img](img/ch4/Kidney-stones-graph.png)\n",
    "\n",
    "Some points from the graph:\n",
    "\n",
    "- The recovery is influenced by the treatment and the size of the stone. \n",
    "- The treatment itself depends on the size. \n",
    "- A large proportion of difficult cases was assigned to treatment $a$,open surgery. \n",
    "\n",
    "<br/>\n",
    "\n",
    "\n",
    "We consider two diffrent variation of SCM $\\mathfrak{C}$ based on the type of the treatment:\n",
    "\n",
    "- SCM $\\mathfrak{C}_A$ obtained after replacing the structural assignment for $T$ with $T := A$ with corresponding probability distributions $P^{\\mathfrak{C}_A}$\n",
    "\n",
    "- SCM $\\mathfrak{C}_B$ obtained after replacing the structural assignment for $T$ with $T := B$ with corresponding probability distributions $P^{\\mathfrak{C}_B}$\n",
    "\n",
    "Given that a patient is diagnosed with a kidney stone without knowing its size, we should base our choice of treatment on a comparison between:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "&\\mathbb{E}^{\\mathfrak{C}_{A}} R=P^{\\mathfrak{C}_{A}}(R=1)=P^{\\mathfrak{C} ; d o(T:=A)}(R=1) \\\\\n",
    "&\\mathbb{E}^{\\mathfrak{C}_{B}} R=P^{\\mathfrak{C}_{B}}(R=1)=P^{\\mathfrak{C} ; d o(T:=B)}(R=1)\n",
    "\\end{cases}\n",
    "$$\n",
    "<p style='text-align: center;'> (Eq.7) </p>\n",
    "\n",
    "Given that we have observed data from $\\mathfrak{C}$, **how can we estimate these quantities?**\n",
    "Here is the calculation of $P^{\\mathfrak{C}_{A}}(R=1)$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P^{\\mathfrak{C}_{A}}(R=1) &=\\sum_{z=0}^{1} P^{\\mathfrak{C}_{A}}(R=1, T=A, Z=z) \\\\\n",
    "&=\\sum_{z=0}^{1} P^{\\mathfrak{C}_{A}}(R=1 \\mid T=A, Z=z) P^{\\mathfrak{C}_{A}}(T=A, Z=z) \\\\\n",
    "&=\\sum_{z=0}^{1} P^{\\mathfrak{C}_{A}}(R=1 \\mid T=A, Z=z) P^{\\mathfrak{C}_{A}}(Z=z) \\\\\n",
    "& \\stackrel{(Eq.1)}{=} \\sum_{z=0}^{1} P^{\\mathfrak{C}}(R=1 \\mid T=A, Z=z) P^{\\mathfrak{C}}(Z=z) .\n",
    "\\end{aligned}\n",
    "$$\n",
    "<p style='text-align: center;'> (Eq.8) </p>\n",
    "\n",
    "<br/>\n",
    "\n",
    "The last step of *Eq.8* achieved from the invariance equation (Eq.1). Using empirical data in the table above, we can estimate $P^{\\mathfrak{C}_{A}}(R=1)$. In words, it means the probability of recovery if we force all patients to use treatment $A$ including those ones who has small and big stones.\n",
    "\n",
    "$$\n",
    "P^{\\mathfrak{C} ; d o(T:=A)}(R=1) = P^{\\mathfrak{C}_{A}}(R=1) \\approx 0.93 \\cdot \\frac{357}{700}+0.73 \\cdot \\frac{343}{700}=0.832\n",
    "$$\n",
    "<p style='text-align: center;'> (Eq.9) </p>\n",
    "\n",
    "<br/>\n",
    "\n",
    "Similarly, we can calculate:\n",
    "\n",
    "$$\n",
    "P^{\\mathfrak{C} ; d o(T:=B)}(R=1) = P^{\\mathfrak{C}_{B}}(R=1) \\approx 0.87 \\cdot \\frac{357}{700}+0.69 \\cdot \\frac{343}{700} \\approx 0.782\n",
    "$$\n",
    "<p style='text-align: center;'> (Eq.10) </p>\n",
    "\n",
    "\n",
    "**Therefore, we conclude that we would rather go for treatment $A$. What do we see in the table?!**\n",
    "\n",
    "We also see that the **average causal effect (ACE)** for this binary treatments is positive.\n",
    "\n",
    "$$\n",
    "P^{\\mathfrak{C}_{A}}(R=1)-P^{\\mathfrak{C}_{B}}(R=1) \\approx 0.832-0.782\n",
    "$$\n",
    "<p style='text-align: center;'> (Eq.11) </p>\n",
    "\n",
    "<br/>\n",
    "\n",
    "Notice that intervention is different from simple conditioning (the problem with Simpson paradox). In this case conditioning has even the opposite sign of the ACE.\n",
    "\n",
    "$$\n",
    "P^{\\mathfrak{C}}(R=1 \\mid T=A)-P^{\\mathfrak{C}}(R=1 \\mid T=B)=0.78-0.83\n",
    "$$\n",
    "<p style='text-align: center;'> (Eq.12) </p>\n",
    "\n",
    "<br/>\n",
    "\n",
    "This three-node example nicely highlights the difference between **intervening** and **conditioning**. In terms of densities, it reads:\n",
    "\n",
    "$$\n",
    "p^{\\mathfrak{C} ; d o(T:=t)}(r)=\\sum_{z} p^{\\mathfrak{C}}(r \\mid z, t) p^{\\mathfrak{C}}(z) \\neq \\sum_{z} p^{\\mathfrak{C}}(r \\mid z, t) p^{\\mathfrak{C}}(z \\mid t)=p^{\\mathfrak{C}}(r \\mid t) .\n",
    "$$\n",
    "<p style='text-align: center;'> (Eq.13) </p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do-Calculus\n",
    "\n",
    "So far, we have talked about causality in terms of interventions. In simple words, we say that $A$ causes $Y$ if an intervention in $X$ changes $Y$, while intervention in $Y$ does not necessarily result in a change in $X$. \n",
    "\n",
    "The do-operator is a mathematical representation of physical intervention. For example, in our $W → A → Y$ model, we can simulate an intervention in $A$ by deleting all the incoming arrows to $A$, and manually setting $A$ to some value $a_0$.\n",
    "\n",
    "![img](img/ch4/do-operator.png)\n",
    "\n",
    "For example, suppose we want to ask, will increasing the marketing budget boost Smoked Salmon sales? If we have a causal model that includes marketing spending and sales, we can simulate what would happen if we were to increase the marketing budget and assess whether the change in Smoked Salmon sales is worth it. In other words, we can *evaluate the causal effect* of marketing on sales. \n",
    "\n",
    "- The importance of do-operator is that it allows us to simulate experiments, given we know the causal model. \n",
    "\n",
    "Following is a complete set of three rules that outline how to use the **do-operator** suggested by [Judea Pearl, 2009](http://bayes.cs.ucla.edu/BOOK-2K/). Notice that **do-calculus** can translate **interventional distributions** (i.e., probabilities with the do-operator) into **observational distributions** (i.e., probabilities without the do-operator). This can be seen by rules 2 and 3. Given a graph $\\mathcal{G}$ and disjoint subsets $\\mathbf{X, Y, Z, W}$, we have:\n",
    "\n",
    "\n",
    "### Rule 1. Insertion/deletion of observations:\n",
    "\n",
    "$$\n",
    "p^{\\mathfrak{C} ; d o(\\mathbf{X}:=\\mathbf{x})}(\\mathbf{y} \\mid \\mathbf{z}, \\mathbf{w})=p^{\\mathfrak{C} ; d o(\\mathbf{X}:=\\mathbf{x})}(\\mathbf{y} \\mid \\mathbf{w})\n",
    "$$\n",
    "\n",
    "if $\\mathbf{Y}$ and $\\mathbf{Z}$ are d-separated by $\\mathbf{X,W}$ in a graph where incoming edges in $\\mathbf{X}$ have been removed. \n",
    "\n",
    "In simple words, $\\mathbf{Z}$ is irrelevant to $\\mathbf{Y}$.\n",
    "\n",
    "<br/>\n",
    "\n",
    "### Rule 2. Action/observation exchange:\n",
    "\n",
    "$$\n",
    "p^{\\mathfrak{C} ; d o(\\mathbf{X}:=\\mathbf{x}, \\mathbf{Z}=\\mathbf{z})}(\\mathbf{y} \\mid \\mathbf{w})=p^{\\mathfrak{C} ; d o(\\mathbf{X}:=\\mathbf{x})}(\\mathbf{y} \\mid \\mathbf{z}, \\mathbf{w})\n",
    "$$\n",
    "\n",
    "if $\\mathbf{Y}$ and $\\mathbf{Z}$ are d-separated by $\\mathbf{X,W}$ in a graph where incoming edges in $\\mathbf{X}$ and outgoing edges from $\\mathbf{Z}$ have been removed. \n",
    "\n",
    "In simple words, $\\mathbf{X \\cup W}$ blocks all back-door paths from $\\mathbf{Z}$ to $\\mathbf{Y}$.\n",
    "\n",
    "<br/>\n",
    "\n",
    "### Rule 3. Insertion/deletion of actions:\n",
    "\n",
    "$$\n",
    "p^{\\mathfrak{C} ; d o(\\mathbf{X}:=\\mathbf{x}, \\mathbf{Z}=\\mathbf{z})}(\\mathbf{y} \\mid \\mathbf{w})=p^{\\mathfrak{C} ; d o(\\mathbf{X}:=\\mathbf{x})}(\\mathbf{y} \\mid \\mathbf{w})\n",
    "$$\n",
    "\n",
    "if $\\mathbf{Y}$ and $\\mathbf{Z}$ are d-separated by $\\mathbf{X,W}$ in a graph where incoming edges in $\\mathbf{X}$ and $\\mathbf{Z(W)}$ have been removed. Here $\\mathbf{Z(W)}$ is the subset of nodes in $\\mathbf{Z}$ that are not ancestors of any node in $\\mathbf{W}$ in a graph that is obtained from $\\mathcal{G}$ after removing all edges into $\\mathbf{X}$.\n",
    "\n",
    "In simple words, there is no causal path from $\\mathbf{Z}$ to $\\mathbf{Y}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c3ab26dcd97a4d69a239c13402c708c1",
    "deepnote_cell_height": 2154.171875,
    "deepnote_cell_type": "markdown",
    "owner_user_id": "a91a2c6a-0c99-4421-a773-926bd3919ed9",
    "tags": []
   },
   "source": [
    "## Causal Models and Counterfactuals\n",
    "\n",
    "Humans often think in the form of **counterfactuals**: *“I should have taken the train.”* or *“We should have invested in Bitcoin in January 2019!”* are only a few examples. \n",
    "\n",
    "Assume someone offers you $10000 if you predict the result of a coin flip. You guess “heads” and lose. Some people may then think, “Why did I not say ‘tails’?” even though there was no way they could have possibly known the outcome. \n",
    "\n",
    "Counterfactuals have a long history ad the humankind. For example, Titus Livius discusses in 25 BC what would have happened if Alexander the Great had not died in his way back from Persia and had attacked Rome. Livy argues that Rome and Carthage would have joined forces to crush the Macedonian army. [Geradin and Girgenson, 2011](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1970917). \n",
    "\n",
    "![img](img/ch4/Alexander_mosaic.jpeg)\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "**Example: Crop Planning**  \n",
    "\n",
    "This example is from [Jerzy Neyman, 1923](https://en.wikipedia.org/wiki/Jerzy_Neyman#:~:text=After%20his%20return%20to%20Poland,studied%20randomized%20experiments%20in%201923.) Consider $m$ plots of land and $v$ varieties of crop. Denote $U_{ij}$ the crop yield that would be observed if variety $i = 1, . . . , v$ were planted in plot $j = 1,...,m$.\n",
    "\n",
    "For each plot $j$, we can only experimentally determine one $U_{ij}$ in each growing season. The others crop yields are called **counterfactuals**.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Definition 4.3. (Counterfactuals):** Counterfactual corresponds to updating the noise distributions of an SCM (by conditioning) and then performing an intervention.\n",
    "Consider an SCM $\\mathfrak{C} = (S,P_U)$ over nodes $X$. Given some observations $x$, we define a *counterfactual SCM* by replacing the distribution of noise variables:\n",
    "\n",
    "$$\n",
    "\\mathfrak{C}_{\\mathbf{X}=\\mathbf{x}}:=\\left(\\mathbf{S}, P_{\\mathbf{U}}^{\\mathfrak{C} \\mid \\mathbf{X}=\\mathbf{x}}\\right)\n",
    "$$\n",
    "\n",
    "where \n",
    "$$\n",
    "P_{\\mathbf{U}}^{\\mathfrak{C} \\mid \\mathbf{X}=\\mathbf{x}} = P_{\\mathbf{U} \\mid \\mathbf{X}=\\mathbf{x}}^{}\n",
    "$$\n",
    "\n",
    "The new set of noise variables need not be jointly independent anymore. *Counterfactual* statements can now be seen as *do-statements* in the new counterfactual SCM.\n",
    "\n",
    "</div>\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "**Example: Three Integers, Computing Counterfactuals:** \n",
    "\n",
    "This example is from [Elements of Causal Inference Book, Chapter 6](https://mitpress.mit.edu/books/elements-causal-inference). \n",
    "Consider the following SCM $\\mathfrak{C}$:\n",
    "\n",
    "$\n",
    "\\begin{cases}\n",
    "X = U_X \\\\\n",
    "Y = X^{2} + U_Y\\\\\n",
    "Z = 2Y + X + U_Z\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "with uniformly distributed noise values on the integers between −5 and 5:\n",
    "\n",
    "$\n",
    "U_{X}, U_{Y}, U_{Z} \\stackrel{\\text { iid }}{\\sim} \\mathrm{U}(\\{-5,-4, \\ldots, 4, 5\\})\n",
    "$\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Case 1: Observation:** We assume that we observe $(X,Y,Z) = (1,2,4)$.\n",
    "\n",
    "Then $P_{\\mathbf{U}}^{\\mathfrak{C} \\mid \\mathbf{X}=\\mathbf{x}}$ puts a point mass on $(U_X , U_Y , U_Z) = (1, 1, -1)$ because here all noise terms can be uniquely reconstructed from the observations.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Case 2: Counterfactual:** In the context of $(X,Y,Z) = (1,2,4)$ observation, we have a counterfactual statement: “$Z$ would have been 11 if we had $X$ been set to 2.” \n",
    "\n",
    "Mathematically, this means that $P_{\\mathbf{Z}}^{\\mathfrak{C} \\mid \\mathbf{X}=\\mathbf{x};do(X=2)} = 11$ or it has $Z$ a point mass on 11. \n",
    "\n",
    "If we have a counterfactual statement that says: $Y$ would have been 5 if we had $X$ been set to 2.\n",
    "\n",
    "Mathematically, this is $P_{\\mathbf{Y}}^{\\mathfrak{C} \\mid \\mathbf{X}=\\mathbf{x};do(X=2)} = 5$. \n",
    "\n",
    "Counterfactuals notation may looks quite complicated.The following image provides further clarification:\n",
    "\n",
    "![img](img/ch4/Counterfactuals_notation.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "## Total Causal Effect\n",
    "\n",
    "We define the existence of a total causal effect as follows based on [Judea Pearl, 2009](https://ftp.cs.ucla.edu/pub/stat_ser/r350.pdf) and Chapter 6 of [Elements of Causal Inference](https://mitpress.mit.edu/books/elements-causal-inference).\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Definition 4.4. ((Total Causal Effect):** Given an SCM $\\mathfrak{C}$ over nodes $X$, there is a total causal effect from $X$ to $Y$ for some random variable $\\tilde{U}_{X}$ if and only if:\n",
    "\n",
    "$$X \\not\\!\\perp\\!\\!\\!\\perp Y \\quad in  P_{\\mathbf{X}}^{\\mathfrak{C} ; do\\left(X:=\\tilde{U}_{X}\\right)}$$\n",
    "\n",
    "</div>\n",
    "\n",
    "<br/>\n",
    "\n",
    "The existence of a total causal effect is also related to the existence of a directed path in the corresponding graph $\\mathfrak{G}$. The correspondence, however, is not one-to-one. While a directed path is necessary for a total causal effect, it is not sufficient.\n",
    "\n",
    "<br/>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Proposition 4.5 (Graphical criteria for total causal effects):** Assume we are given an SCM $\\mathfrak{C}$ with a corresponding graph $\\mathfrak{G}$.\n",
    "\n",
    "- (i) If there is no directed path from $X$ to $Y$ , then there is no total causal effect.\n",
    "- (ii) Sometimes there is a directed path but no total causal effect.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "caa1137eda4c4ea4bbf611fff7e1fb61",
    "deepnote_cell_height": 225.953125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## References\n",
    "\n",
    "This chapter contents are highly inspired by the [Elements of Causal Inference (Open Access) book](https://mitpress.mit.edu/books/elements-causal-inference) by By Jonas Peters, Dominik Janzing and [Bernhard Schölkopf](https://www.is.mpg.de/~bs).\n",
    "\n",
    "We also used examples fom the [Introduction to Causal Inference course](https://www.ucbbiostat.com) by Maya L. Petersen & Laura B. Balzer, UC Berkeley.\n",
    "\n",
    "Bruno Gonçalves has a helpful [blog](https://medium.data4sci.com/causal-inference-part-iv-structural-causal-models-df10a83be580) on SEM too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "ca1c0dae-cfcc-4ea4-b356-eeedce5aaafd",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
