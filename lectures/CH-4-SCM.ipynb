{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "333030af5f4a4fa6a16bb18625a6edfc",
    "deepnote_cell_height": 3363.34375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "# Chapter 4. Structural Causal Models\n",
    "\n",
    "**Structural equation modeling (SEM)** is a family of various methods scientists use in experimental and observational research. SEM is a model in which different aspects of a phenomenon are related to one another with a structure. This structure is a system of equations that implies statistical and often causal relationships between variables. \n",
    "\n",
    "SEMs also called **Structural Causal Models (SCM)** are important tools to relate causal and probabilistic statements.\n",
    "\n",
    "## Graphs vs. Structural Equations\n",
    "\n",
    "We saw in Chapter 3 that: \n",
    "- **G+** Graph is an excellent tool for communicating with subject matter experts. \n",
    "- **G+** Graph can be a helpful way to translate assumptions into a formal model. \n",
    "- **G+** Graphs are also useful to see what restrictions (if any) our model puts on the joint distribution of the observed data.\n",
    "\n",
    "- **E+** As the model becomes more complicated, the equations get a lot more friendly to work with.\n",
    "- **E+** Equations may help you resist the urge to oversimplify.\n",
    "\n",
    "\n",
    "## Structural Causal/Equation Model\n",
    "\n",
    "An **Structural Causal Model (SCM)** or **Structure Equation Model (SEM)** consists of:\n",
    "\n",
    "1. *Endogenous variables* $X = {X_1 , ..., X_J }$ \n",
    "    - Affected by other variables in the model\n",
    "    - May or may not be observed\n",
    "\n",
    "2. *Background (exogenous) variables* $U = {U_1 , ..., U_J }$ \n",
    "    - Not affected by other factors in the model\n",
    "    - Not observed\n",
    "    - $U_j$ is noise or error variables and $U$ is a joint distribution over noise variables.\n",
    "    - Each endogenous variable $X_j$ has an error $U_j$.\n",
    "\n",
    "3. *Functions* $F = \\lbrace f_{X_1} , ... , f_{X_J} \\rbrace $\n",
    "    - The functions $F$ define a set of $J$ **structural equations** for each of the endogenous variables:\n",
    "    $X_j = f_{X_j}(Pa(X_j),U_{X_j}), j = 1,...,J$\n",
    "\n",
    "    $Pa(X_j) \\subseteq X \\backslash Xj$\n",
    "    \n",
    "where $Pa(X_j)$ called **parents** of $X_j$.\n",
    "\n",
    "- We sometimes call the elements of $PA_j$ not only parents but also **direct causes** of $X_j$, \n",
    "- We call $X_j$ a **direct effect** of each of its **direct causes** $PA_j$. \n",
    "\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "**W-A-Y Example, Make a Graph for SCM:** \n",
    "\n",
    "The graph $G$ of an SCM $\\mathfrak{C}$ is obtained by creating one node for each $X_j$ and drawing directed edges from each parent in $PA_j$ to $X_j$. \n",
    "\n",
    "![img](img/ch4/Graph-SEM.png)\n",
    "\n",
    "- Connect parents to children with a directed link.\n",
    "- Each endogenous variable $X_j$ has an error $U_j$.\n",
    "- Potential dependence between errors $U_j$ encoded in dashed lines/double headed errors.\n",
    "\n",
    "- We assume this graph $G$ is acyclic, without directed cycles/feedback loops.\n",
    "    - Instead feedback loops, we can use temporal ordering.\n",
    "    - In other words, extend graph (and corresponding structural equations) over time\n",
    "\n",
    "- We will work with recursive SCM. There is ordering between $X={X1,...,XJ}$ such that each $X_j$ is a function of a subset $Pa(Xj)$ of its **predecessors**.\n",
    "    - Causes always precede their effects\n",
    "    - A natural source of ordering is *time*\n",
    "\n",
    "    $X_j = f_{X_j}(Pa(X_j),U_{X_j}), j = 1,...,J $ \n",
    "\n",
    "    $Pa(X_j) \\subseteq {X_1, ..., X_{j-1}}$\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "**W-A-Y Example, Causal Exclusion:** \n",
    "\n",
    "Exclusion restrictions on the graph encoded through absence of arrows between variables.\n",
    "- Absence of arrow means no direct effect\n",
    "\n",
    "![img](img/ch4/Graph-SEM-Excluded.png)\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "**W-A-Y Example, Independence Assumptions:** \n",
    "- Absence of double headed arrows between background (exogenous) variables or errors $U$ means those two errors are **independent**.\n",
    "    - It is an assumption on distribution $P_U$.\n",
    "    - There is no unmesaured common cause between those two $U_j$\n",
    "\n",
    "![img](img/ch4/Graph-SEM-Independence.png)\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Proposition 4.1. (SCM Entailed Distributions):** An SCM $\\mathfrak{C}$ defines a unique distribution over the variables $X = (X_1,...,X_J)$ such that $X_j = f_{X_j}(Pa(X_j),U_{X_j})$ for $j = 1,...,J$. \n",
    "We refer to it as the entailed distribution $P^C_X$ and sometimes write $P_X$.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "7d4b15c6e44f4895808d2875a27c5e72",
    "deepnote_cell_height": 2134.328125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<br/><br/>\n",
    "## Interventions on a SCM\n",
    "\n",
    "- When we intervene on variable $X_j$, say, and set it to a specific outcome. We expect that this intervention changes the distribution of the system compared to its earlier behavior without intervention. \n",
    "\n",
    "    - Even if variable $X_j$ was causally influenced by other variables before, it is now influenced by nothing else. \n",
    "    - $X_j$ has no more causal parents. \n",
    "\n",
    "- The autonomy of structural equations means that we can make a targeted modification to the set of equations in order to represent our intervention of interest.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Proposition 4.2. (Intervention Distribution):** [source](https://mitpress.mit.edu/books/elements-causal-inference) \n",
    "\n",
    "Consider an SCM $\\mathfrak{C}$ and its entailed distribution $P^\\mathfrak{C}_X$. We replace one (or several) of the structural assignments to obtain a new SCM $\\tilde{\\mathfrak{C}}$. \n",
    "\n",
    "<br/>\n",
    "\n",
    "Assume that we replace the assignment for $X_j$ by:\n",
    "\n",
    "$X_j = \\tilde{f}_{X_j}(\\tilde{Pa}(X_j),U_{X_j})$.\n",
    "\n",
    "<br/>\n",
    "\n",
    "We then call the entailed distribution of the new SCM $\\tilde{\\mathfrak{C}}$ an **intervention distribution** and say that the variables whose structural assignment we have replaced have been **intervened** on. We denote the new distribution $P^{\\tilde{\\mathfrak{C}}}_X$ by:\n",
    "\n",
    "$P^{\\tilde{\\mathfrak{C}}}_X = P^{\\mathfrak{C} ; do(\\tilde{f}_{X_j}(\\tilde{Pa}(X_j),U_{X_j}))}_X$.\n",
    "\n",
    "<br/>\n",
    "\n",
    "The set of noise (error) variables in $\\tilde{\\mathfrak{C}}$ now contains both some “new” $\\tilde{U}$ 's and some \"old\" $U$'s, all of which are required to be jointly independent.\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "**W-A-Y Example, Intervention on A:** \n",
    "- We intervene on the system to set $A=1$ and we replace $f_A$ with constant function $A=1$.\n",
    "\n",
    "![img](img/ch4/Grpah-SEM-Intervene.png)\n",
    "\n",
    "- $Y_a(u)$ is defined as the solution to the equation $f_Y$ under an intervention on the system of equations to set $A=a$ (with input $U_Y=u$).\n",
    "- We can think of $u$ as a particular realization of (values for) the background factors\n",
    "- $P_{U_Y}$ and $F$ induce a probability distribution on $Y_a$ just as they do on $Y$.\n",
    "- $Y_a$ is a **post-intervention** or **counterfactual** random variable.\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "**Simple Prediction Example, Intervention Targets:** \n",
    "\n",
    "This example considers prediction. It shows that even though some variables may be good predictors for a target variable $Y$ , intervening on them may leave the target variable unaffected. Consider the following SCM $\\mathfrak{C}$:\n",
    "\n",
    "![img](img/ch4/Exp_Predictors_Intervention_Targets.png)\n",
    "\n",
    "$\n",
    "\\begin{cases}\n",
    " X_1 = U_{X_1}\\\\\n",
    " Y = X_1 + U_Y\\\\\n",
    " X_2 = Y + U_{X_2}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "with following distribution being jointly independent:\n",
    "\n",
    "$\n",
    "\\begin{cases}\n",
    "U_{X_1} \\stackrel{iid}{\\sim} \\mathcal{N}(0,1) \\\\\n",
    "U_{X_2} \\stackrel{iid}{\\sim} \\mathcal{N}(0,0.1) \\\\\n",
    "U_{Y} \\stackrel{iid}{\\sim} \\mathcal{N}(0,1) \n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Case 1: Intervene on $X_2$:** We are interested in predicting $Y$ from $X_1$ and $X_2$. Clearly, $X_2$ is a better predictor for $Y$ than $X_1$ is. Clearly, X2 is a better predic- tor for Y than X1 is. For example, a linear model without $X_2$ leads to a (significantly) larger mean squared error than a linear model without $X_1$ would.\n",
    "\n",
    "\n",
    "However, if we want to study $Y$, intervention on $X_2$ is useless. In other words, no matter how strongly we intervene on $X_2$, the distribution of $Y$ remains unaffected\n",
    "\n",
    "$P^{\\mathfrak{C} ; do(X_2 = \\stackrel{\\sim}{U})}_Y = P^{\\mathfrak{C}}_Y$\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Case 2: Intervene on $X_1$:** An intervention on $X_1$, however, does change the distribution $Y$.\n",
    "\n",
    "$P^{\\mathfrak{C} ; do(X_1 = \\stackrel{\\sim}{U})}_Y = \\mathcal{N}(E(U_Y) + E(\\stackrel{\\sim}{U}), var (U_Y) + var(\\stackrel{\\sim}{U}))$ \n",
    "\n",
    "\n",
    "$P^{\\mathfrak{C} ; do(X_1 = \\stackrel{\\sim}{U})}_Y \\neq P^{\\mathfrak{C}}_Y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Intervention Distributions\n",
    "\n",
    "We use a trivial but powerful invariance statement for calculating intervention distributions. SCM $\\mathfrak{C}$, and writing $p a(j):=\\mathbf{P A}_{j}^{\\mathcal{G}}$, we have:\n",
    "\n",
    "$$\n",
    "p^{\\tilde{\\mathfrak{C}}}\\left(x_{j} \\mid x_{p a(j)}\\right)=p^{\\mathfrak{C}}\\left(x_{j} \\mid x_{p a(j)}\\right)\n",
    "$$ \n",
    "<p style='text-align: center;'> (Eq.1) </p>\n",
    "\n",
    "For any SCM $\\tilde{\\mathfrak{C}}$ that is constructed from $\\mathfrak{C}$ by **intervening** on (some) $X_k$ but not on $X_j$.\n",
    "\n",
    "- Equation above shows that causal relationships are **autonomous** under interventions. This property is therefore sometimes called **autonomy**. It means if we intervene on a variable, then the other mechanisms remain invariant.\n",
    "\n",
    "\n",
    "The autonomy formula is the base for the **G-computation formula** (Robins, 1986), **truncated factorization** (Pearl, 1993), or **manipulation theorem** (Spirtes, 2000) that we have seen in Chapter 3. To refresh our minds, we repeat those formulas.  \n",
    "\n",
    "We start with an SCM $\\mathfrak{C}$ with structural assignments with density $p^{\\mathfrak{C}}$:\n",
    "\n",
    "$$\n",
    "X_{j}:=f_{j}\\left(X_{pa(j)}, N_{j}\\right), \\quad j=1, \\ldots, d\n",
    "$$\n",
    "<p style='text-align: center;'> (Eq.2) </p>\n",
    "\n",
    "Using Markov property, we have:\n",
    "\n",
    "$$\n",
    "p^{\\mathfrak{C}}\\left(x_{1}, \\ldots, x_{d}\\right)=\\prod_{j=1}^{d} p^{\\mathfrak{C}}\\left(x_{j} \\mid x_{p a(j)}\\right)\n",
    "$$\n",
    "<p style='text-align: center;'> (Eq.3) </p>\n",
    "\n",
    "We do an **intervention**, so the SCM $\\mathfrak{\\tilde{C}}$ evolves from $\\mathfrak{C}$ after $do\\left(X_{k}:=\\tilde{N}_{k}\\right)$, where $\\tilde{N}_{k}$. Similarly, density changes to $\\tilde{p}$. Again, we use the Markov assumption:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p^{\\mathfrak{C} ; d o\\left(X_{k}:=\\tilde{N}_{k}\\right)}\\left(x_{1}, \\ldots, x_{d}\\right) &=\\prod_{j \\neq k} p^{\\mathfrak{C} ; d o\\left(X_{k}:=\\tilde{N}_{k}\\right)}\\left(x_{j} \\mid x_{p a(j)}\\right) \\cdot p^{\\mathfrak{C} ; d o\\left(X_{k}:=\\tilde{N}_{k}\\right)}\\left(x_{k}\\right) \\\\\n",
    "&=\\prod_{j \\neq k} p^{\\mathfrak{C}}\\left(x_{j} \\mid x_{p a(j)}\\right) \\tilde{p}\\left(x_{k}\\right) .\n",
    "\\end{aligned}\n",
    "$$\n",
    "<p style='text-align: center;'> (Eq.4) </p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c3ab26dcd97a4d69a239c13402c708c1",
    "deepnote_cell_height": 2154.171875,
    "deepnote_cell_type": "markdown",
    "owner_user_id": "a91a2c6a-0c99-4421-a773-926bd3919ed9",
    "tags": []
   },
   "source": [
    "## Causal Models and Counterfactuals\n",
    "\n",
    "Humans often think in the form of **counterfactuals**: *“I should have taken the train.”* or *“We should have invested in Bitcoin in January 2019!”* are only a few examples. \n",
    "\n",
    "Assume someone offers you $10000 if you predict the result of a coin flip. You guess “heads” and lose. Some people may then think, “Why did I not say ‘tails’?” even though there was no way they could have possibly known the outcome. \n",
    "\n",
    "Counterfactuals have a long history ad the humankind. For example, Titus Livius discusses in 25 BC what would have happened if Alexander the Great had not died in his way back from Persia and had attacked Rome. Livy argues that Rome and Carthage would have joined forces to crush the Macedonian army. [Geradin and Girgenson, 2011](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1970917). \n",
    "\n",
    "![img](img/ch4/Alexander_mosaic.jpeg)\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "**Example: Crop Planning**  \n",
    "\n",
    "This example is from [Jerzy Neyman, 1923](https://en.wikipedia.org/wiki/Jerzy_Neyman#:~:text=After%20his%20return%20to%20Poland,studied%20randomized%20experiments%20in%201923.) Consider $m$ plots of land and $v$ varieties of crop. Denote $U_{ij}$ the crop yield that would be observed if variety $i = 1, . . . , v$ were planted in plot $j = 1,...,m$.\n",
    "\n",
    "For each plot $j$, we can only experimentally determine one $U_{ij}$ in each growing season. The others crop yields are called **counterfactuals**.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Definition 4.3. (Counterfactuals):** Counterfactual corresponds to updating the noise distributions of an SCM (by conditioning) and then performing an intervention.\n",
    "Consider an SCM $\\mathfrak{C} = (S,P_U)$ over nodes $X$. Given some observations $x$, we define a *counterfactual SCM* by replacing the distribution of noise variables:\n",
    "\n",
    "$$\n",
    "\\mathfrak{C}_{\\mathbf{X}=\\mathbf{x}}:=\\left(\\mathbf{S}, P_{\\mathbf{U}}^{\\mathfrak{C} \\mid \\mathbf{X}=\\mathbf{x}}\\right)\n",
    "$$\n",
    "\n",
    "where \n",
    "$$\n",
    "P_{\\mathbf{U}}^{\\mathfrak{C} \\mid \\mathbf{X}=\\mathbf{x}} = P_{\\mathbf{U} \\mid \\mathbf{X}=\\mathbf{x}}^{}\n",
    "$$\n",
    "\n",
    "The new set of noise variables need not be jointly independent anymore. *Counterfactual* statements can now be seen as *do-statements* in the new counterfactual SCM.\n",
    "\n",
    "</div>\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "**Example: Three Integers, Computing Counterfactuals:** \n",
    "\n",
    "This example is from [Elements of Causal Inference Book, Chapter 6](https://mitpress.mit.edu/books/elements-causal-inference). \n",
    "Consider the following SCM $\\mathfrak{C}$:\n",
    "\n",
    "$\n",
    "\\begin{cases}\n",
    "X = U_X \\\\\n",
    "Y = X^{2} + U_Y\\\\\n",
    "Z = 2Y + X + U_Z\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "with uniformly distributed noise values on the integers between −5 and 5:\n",
    "\n",
    "$\n",
    "U_{X}, U_{Y}, U_{Z} \\stackrel{\\text { iid }}{\\sim} \\mathrm{U}(\\{-5,-4, \\ldots, 4, 5\\})\n",
    "$\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Case 1: Observation:** We assume that we observe $(X,Y,Z) = (1,2,4)$.\n",
    "\n",
    "Then $P_{\\mathbf{U}}^{\\mathfrak{C} \\mid \\mathbf{X}=\\mathbf{x}}$ puts a point mass on $(U_X , U_Y , U_Z) = (1, 1, -1)$ because here all noise terms can be uniquely reconstructed from the observations.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Case 2: Counterfactual:** In the context of $(X,Y,Z) = (1,2,4)$ observation, we have a counterfactual statement: “$Z$ would have been 11 if we had $X$ been set to 2.” \n",
    "\n",
    "Mathematically, this means that $P_{\\mathbf{Z}}^{\\mathfrak{C} \\mid \\mathbf{X}=\\mathbf{x};do(X=2)} = 11$ or it has $Z$ a point mass on 11. \n",
    "\n",
    "If we have a counterfactual statement that says: $Y$ would have been 5 if we had $X$ been set to 2.\n",
    "\n",
    "Mathematically, this is $P_{\\mathbf{Y}}^{\\mathfrak{C} \\mid \\mathbf{X}=\\mathbf{x};do(X=2)} = 5$. \n",
    "\n",
    "Counterfactuals notation may looks quite complicated.The following image provides further clarification:\n",
    "\n",
    "![img](img/ch4/Counterfactuals_notation.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d941b7da8a114925bd1b35e62c40e7d2",
    "deepnote_cell_height": 3130.359375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Linking Observations to SCMs using Graphs\n",
    "\n",
    "\n",
    "### <font color='blue'> **What causal structures can lead to dependence between two observed variables?**</font>\n",
    "\n",
    "\n",
    "**1- Direct and Indirect Effects**\n",
    "\n",
    "An effect of $A$ on $Y$ can result in an **association**.\n",
    "\n",
    "![img](img/ch4/SEM-Observe-Direct-Effects.png)\n",
    "\n",
    "\n",
    "**2- Shared Common Cause**\n",
    "\n",
    "Common cause (measured or unmeasured) of $A$ and $Y$ can result in an association. When the common cause is not included in $X$, it is represented through the dependence it induces between errors $U$.\n",
    "\n",
    "![img](img/ch4/SEM-Observe-Share-Cause.png)\n",
    "\n",
    "**3- Non of conditions 1 and 2**\n",
    "\n",
    "If neither of these sources of dependence are present, $A$ and $Y$ will be **independent** in every probability distribution $P_0$ compatible with the SCM. \n",
    "In other words, any data generating experiment that is compatible with the SCM will give rise to an observed data distribution in which $A$ and $Y$ are independent regardless of functional form, strength of associations, etc.\n",
    "\n",
    "![img](img/ch4/SEM-Observe-Independent.png)\n",
    "\n",
    "**4- Conditioning on a Collider**\n",
    "\n",
    "Collider is an “inverted fork”. Conditioning on a common effect (descendent) of $A$ and $Y$ can result in an association between $A$ and $Y$. It also called **Berkson’s bias or selection bias**.\n",
    "\n",
    "![img](img/ch4/SEM-Observe-Colider.png)\n",
    "\n",
    "\n",
    "### <font color='blue'> **What causal structures can remove a source of dependence between variables?**</font>\n",
    "\n",
    "**Conditioning on a shared common cause** \n",
    "\n",
    "Conditioning on a causal intermediate or shared common cause between $A$ and $Y$ will remove that source of dependence.\n",
    "\n",
    "![img](img/ch4/SEM-Observe-Confounder.png)\n",
    "\n",
    "\n",
    "### <font color='blue'> **When does a SCM imply that variables are independent?**</font>\n",
    "\n",
    "**$A$ is independent of $Y$** if there is no path between $A$ and $Y$.\n",
    "\n",
    "![img](img/ch4/SEM-Observe-Independ-nopath.png)\n",
    "\n",
    "**$A$ is independent of $Y$** if all paths between $A$ and $Y$ are “blocked” by a collider.\n",
    "\n",
    "![img](img/ch4/SEM-Observe-Independ-blocked.png)\n",
    "\n",
    "**$A$ is independent of $Y$ given $W$** if $W$ blocks all unblocked paths and doesn’t create any new unblocked paths. In onther words, conditioning on a non-collider blocks a path.\n",
    "\n",
    "![img](img/ch4/SEM-Observe-Independ-condition.png)\n",
    "\n",
    "**$A$ is independent of $Y$ given $W$** if conditioning on a collider (or a descendent of a collider) opens a path.\n",
    "\n",
    "![img](img/ch4/SEM-Observe-Independ-condition-collider.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "## Total Causal Effect\n",
    "\n",
    "We define the existence of a total causal effect as follows based on [Judea Pearl, 2009](https://ftp.cs.ucla.edu/pub/stat_ser/r350.pdf) and Chapter 6 of [Elements of Causal Inference](https://mitpress.mit.edu/books/elements-causal-inference).\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Definition 4.4. ((Total Causal Effect):** Given an SCM $\\mathfrak{C}$ over nodes $X$, there is a total causal effect from $X$ to $Y$ for some random variable $\\tilde{U}_{X}$ if and only if:\n",
    "\n",
    "$$X \\not\\!\\perp\\!\\!\\!\\perp Y \\quad in  P_{\\mathbf{X}}^{\\mathfrak{C} ; do\\left(X:=\\tilde{U}_{X}\\right)}$$\n",
    "\n",
    "</div>\n",
    "\n",
    "<br/>\n",
    "\n",
    "The existence of a total causal effect is also related to the existence of a directed path in the corresponding graph $\\mathfrak{G}$. The correspondence, however, is not one-to-one. While a directed path is necessary for a total causal effect, it is not sufficient.\n",
    "\n",
    "<br/>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Proposition 4.5 (Graphical criteria for total causal effects):** Assume we are given an SCM $\\mathfrak{C}$ with a corresponding graph $\\mathfrak{G}$.\n",
    "\n",
    "- (i) If there is no directed path from $X$ to $Y$ , then there is no total causal effect.\n",
    "- (ii) Sometimes there is a directed path but no total causal effect.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "caa1137eda4c4ea4bbf611fff7e1fb61",
    "deepnote_cell_height": 225.953125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## References\n",
    "\n",
    "This chapter contents are highly inspired by the [Elements of Causal Inference (Open Access) book](https://mitpress.mit.edu/books/elements-causal-inference) by By Jonas Peters, Dominik Janzing and [Bernhard Schölkopf](https://www.is.mpg.de/~bs).\n",
    "\n",
    "We also used examples fom the [Introduction to Causal Inference course](https://www.ucbbiostat.com) by Maya L. Petersen & Laura B. Balzer, UC Berkeley.\n",
    "\n",
    "Bruno Gonçalves has a helpful [blog](https://medium.data4sci.com/causal-inference-part-iv-structural-causal-models-df10a83be580) on SEM too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "ca1c0dae-cfcc-4ea4-b356-eeedce5aaafd",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
